{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks for H.P Lovecraft text generation\n",
    "\n",
    "\"The color out of space\" is one of my favorite tales from Lovecraft, i will use it(as well as others as the call of cthulhu) to create a recurrent neural network in tensorflow that learns his style and generates new text in his style\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn) and an example from \"Deep Learning Nanodegree\" on udacity. Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. \n",
    "\n",
    "## General architecture using \"Long short term memory\" units in the recurrent layers\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime ,localtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only the  first time nltk is used to download language\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define conf variables and hyper parameteters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = \"words\" #characters or words\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 100       # Sequences per batch\n",
    "num_steps = 50         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.0001    # Learning rate\n",
    "keep_prob = 0.15       # Dropout keep probability\n",
    "\n",
    "resume_from_checkpoint = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base text\n",
    "Once trained ,the network can take base text and a sequence size and generate new text using base text as first characters in the sequence. For every element in base text wi will create a list that will store generated text as training goes, to be able to compare results between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_to_try = [\"In the first place\",\"the night before\",\"horror\",\"creature\",\"night\",\"dream\",\"thing\",\"That night\",\"mountain\",\"Ammi\",\"Cthulhu\",\"raven\",\"bird\",\"nevermore\",\"dead\",\"The bird\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that separates text into tokens(for whitespace characters, only new line is implemented, missing tabs and others="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " ' ',\n",
       " 'my',\n",
       " ' ',\n",
       " 'name',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'Luis',\n",
       " ' ',\n",
       " 'Leal',\n",
       " '!',\n",
       " 'new_line_token',\n",
       " 'new_line_token',\n",
       " 'From',\n",
       " ' ',\n",
       " 'Guatemala',\n",
       " ' ']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_by_words(text):\n",
    "    text = text.replace(\"\\n\",\" new_line_token \")\n",
    "    tokens = []\n",
    "    splitted =[[word_tokenize(w),' ']for w in text.split()]\n",
    "    splitted = list(itertools.chain(*list(itertools.chain(*splitted))))\n",
    "    \n",
    "    token_list = []\n",
    "    i = 0\n",
    "    while i < len(splitted):\n",
    "        if splitted[i] == \"new_line_token\":\n",
    "            if   token_list[-1]==\" \":\n",
    "                token_list[-1] = splitted[i]\n",
    "            else:\n",
    "                token_list.append(splitted[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            token_list.append(splitted[i])\n",
    "        i+=1\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "def tokenize_by_characters(text):\n",
    "    return list(text)\n",
    "\n",
    "def tokenize_text(text,mode=\"characters\"):\n",
    "    if mode == \"characters\":\n",
    "        return tokenize_by_characters(text)\n",
    "    elif mode == \"words\":\n",
    "        return tokenize_by_words(text)\n",
    "    \n",
    "tokenize_text(\"Hello, my name is Luis Leal!\\n\\nFrom Guatemala\",mode)\n",
    "#tokenize_text(\"Hello, my name is Luis Leal!\\n\\nFrom Guatemala\",\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(tokenize_text(text,mode))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a little portion of text for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenize_text(text,mode)\n",
    "encoded_dataset = np.array([vocab_to_int[c] for c in tokenized_text if c in vocab_to_int], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = batch_size * num_steps #create a single baty\n",
    "validation_start_index = len(encoded_dataset) - validation_size\n",
    "\n",
    "encoded = encoded_dataset[:validation_start_index]\n",
    "encoded_val = encoded_dataset[validation_start_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " occurred to our philosophers that a laudable curiosity might be gratified, and, at the same time, the interests of science much advanced, by living this natural term in installments. In the case of history, indeed, experience demonstrated that something of this kind was indispensable. An historian\n"
     ]
    }
   ],
   "source": [
    "def encoded_to_text(encoded):\n",
    "    return \"\".join([int_to_vocab[number] for number in encoded])\n",
    "\n",
    "print(encoded_to_text(encoded_val[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_text =encoded_to_text(encoded_val)\n",
    "text = encoded_to_text(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters of train and validation, make sure everything is peachy.  line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE COLOUR OUT OF SPACEnew_line_tokennew_line_tokenWest of Arkham the hills rise wild, and there are'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' occurred to our philosophers that a laudable curiosity might be gratified, and, at the same time, t'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integersin both train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18807,  8219, 17188,  8219,  3006,  8219,  5226,  8219, 19968,\n",
       "       28186, 28186,  8666,  8219,  8382,  8219,  2882,  8219, 26062,\n",
       "        8219, 12184,  8219, 12880,  8219,  4196, 21748,  8219, 24822,\n",
       "        8219, 26937,  8219,  1636,  8219,  2632,  8219, 10345,  8219,\n",
       "        6091,  8219, 28230,  8219,  4213,  8219,  3825,  8219, 16131,\n",
       "        8219,  3841,  8219, 25736,  8219, 21112,  8264,  8219, 14789,\n",
       "        8219,  1636,  8219, 15592,  8219, 18817,  8219, 11463,  8219,\n",
       "        3742,  8219, 26062,  8219, 17376,  8219, 18569,  8219,  1104,\n",
       "       21748,  8219, 24822,  8219,  3742,  8219, 22212,  8219,   567,\n",
       "        8219, 25013,  8219,  8603,  8219, 25736,  8219, 18195,  8219,\n",
       "        4700,  8219, 26062,  8219, 18698,  8219,  8382,  8219, 17245,  8264], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8219, 18674,  8219, 22564,  8219, 21369,  8219, 25473,  8219,\n",
       "        4213,  8219,  4904,  8219, 19184,  8219,  1780,  8219,  1785,\n",
       "        8219, 26000,  8219,  9762, 21748,  8219, 24822, 21748,  8219,\n",
       "        1169,  8219, 26062,  8219, 21614,  8219,  1817, 21748,  8219,\n",
       "       26062,  8219,  3076,  8219,  8382,  8219, 22185,  8219,  6147,\n",
       "        8219,  7445, 21748,  8219, 11413,  8219,  3572,  8219, 24884,\n",
       "        8219, 11713,  8219, 14685,  8219, 20084,  8219, 13790,  8264,\n",
       "        8219, 23094,  8219, 26062,  8219, 19523,  8219,  8382,  8219,\n",
       "       17279, 21748,  8219,  4644, 21748,  8219,  5490,  8219,   785,\n",
       "        8219,  4213,  8219,  9398,  8219,  8382,  8219, 24884,  8219,\n",
       "       28320,  8219,  7822,  8219,  8472,  8264,  8219, 28229,  8219,  5745], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_val[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual english tokens, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28404"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps \n",
    "    n_batches =  len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr =  arr[:n_batches*batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:] ,x[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[18807  8219 17188  8219  3006  8219  5226  8219 19968 28186]\n",
      " [28167  8219 14071  8264 28186 28186  3060  8219  7822  8219]\n",
      " [ 8219  8226  8219   529  8219 13889  8219  2830  8219  8382]\n",
      " [ 4904  8219 20883  8219 10345  8219 23259 21748  8219  8151]\n",
      " [ 8219 26062  8219 25434  8219 20125  8219 23576  8219 19823]\n",
      " [ 8219 22593  8219 17160  8219 18530  8219 18875 21748  8219]\n",
      " [27909  8219  2872  8264  8219  8555  8219 12044  8219 24306]\n",
      " [ 8264  8219 21500  8219  3387  8219 26062  8219  8176  8219]\n",
      " [ 8382  8219 19823 21748  8219 24822  8219 18002  8219  4213]\n",
      " [ 8219 26000  8219 27946  8219 19284  8219 13483  8219 23576]]\n",
      "\n",
      "y\n",
      " [[  8219.  17188.   8219.   3006.   8219.   5226.   8219.  19968.  28186.\n",
      "   28186.]\n",
      " [  8219.  14071.   8264.  28186.  28186.   3060.   8219.   7822.   8219.\n",
      "   23659.]\n",
      " [  8226.   8219.    529.   8219.  13889.   8219.   2830.   8219.   8382.\n",
      "    8219.]\n",
      " [  8219.  20883.   8219.  10345.   8219.  23259.  21748.   8219.   8151.\n",
      "    8219.]\n",
      " [ 26062.   8219.  25434.   8219.  20125.   8219.  23576.   8219.  19823.\n",
      "    8219.]\n",
      " [ 22593.   8219.  17160.   8219.  18530.   8219.  18875.  21748.   8219.\n",
      "   26062.]\n",
      " [  8219.   2872.   8264.   8219.   8555.   8219.  12044.   8219.  24306.\n",
      "    8219.]\n",
      " [  8219.  21500.   8219.   3387.   8219.  26062.   8219.   8176.   8219.\n",
      "    4213.]\n",
      " [  8219.  19823.  21748.   8219.  24822.   8219.  18002.   8219.   4213.\n",
      "    8219.]\n",
      " [ 26000.   8219.  27946.   8219.  19284.   8219.  13483.   8219.  23576.\n",
      "    8219.]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"targets\")\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. For example,\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow will create different weight matrices for all `cell` objects. Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    # Add dropout to the cell outputs\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.BasicLSTMCell(lstm_size),output_keep_prob = keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output,axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros([out_size]))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits =  tf.add(tf.matmul(x,softmax_w),softmax_b) \n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits,name =\"out\")\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip,global_step):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        global_step: to control the total number of train steps\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars),global_step)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.global_step_tensor = tf.Variable(0,trainable=False,name = \"global_step\")\n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size,num_steps)\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs,lstm_size,num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits,self.targets,lstm_size,num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip,self.global_step_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network:. \n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_of_parameters():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        #print(shape)\n",
    "        #print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        \n",
    "        for dim in shape:\n",
    "            #print(dim)\n",
    "            variable_parameters*=dim.value\n",
    "        #print(variable_parameters)\n",
    "        total_parameters+= variable_parameters\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = {\"train\":[],\"validation\":[]}\n",
    "x_steps = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting at time: 2017-10-15 18:45:12\n",
      "Number of parameters: 75892468 Dataset size: 951327\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i5700_l512.ckpt\n",
      "Epoch: 1/15...  Training Step: 5701...  Training loss: 2.7913...  Val loss: 3.6031...  1.4087 sec/batch\n",
      "Epoch: 1/15...  Training Step: 5801...  Training loss: 2.6679...  Val loss: 3.6421...  1.0793 sec/batch\n",
      "Epoch: 2/15...  Training Step: 5901...  Training loss: 2.7310...  Val loss: 3.6354...  1.0757 sec/batch\n",
      "Epoch: 2/15...  Training Step: 6001...  Training loss: 2.6242...  Val loss: 3.6420...  1.0765 sec/batch\n",
      "Epoch: 3/15...  Training Step: 6101...  Training loss: 2.6680...  Val loss: 3.6202...  1.0842 sec/batch\n",
      "Epoch: 3/15...  Training Step: 6201...  Training loss: 2.6472...  Val loss: 3.6697...  1.0887 sec/batch\n",
      "Epoch: 4/15...  Training Step: 6301...  Training loss: 2.6841...  Val loss: 3.6337...  1.0848 sec/batch\n",
      "Epoch: 4/15...  Training Step: 6401...  Training loss: 2.6483...  Val loss: 3.6663...  1.0821 sec/batch\n",
      "Epoch: 5/15...  Training Step: 6501...  Training loss: 2.6298...  Val loss: 3.6630...  1.0863 sec/batch\n",
      "Epoch: 5/15...  Training Step: 6601...  Training loss: 2.6276...  Val loss: 3.7525...  1.0839 sec/batch\n",
      "Epoch: 6/15...  Training Step: 6701...  Training loss: 2.6046...  Val loss: 3.7207...  1.0799 sec/batch\n",
      "Epoch: 6/15...  Training Step: 6801...  Training loss: 2.6066...  Val loss: 3.6768...  1.0823 sec/batch\n",
      "Epoch: 7/15...  Training Step: 6901...  Training loss: 2.6305...  Val loss: 3.7061...  1.0994 sec/batch\n",
      "Epoch: 7/15...  Training Step: 7001...  Training loss: 2.5702...  Val loss: 3.7111...  1.0866 sec/batch\n",
      "Epoch: 8/15...  Training Step: 7101...  Training loss: 2.5754...  Val loss: 3.7079...  1.0901 sec/batch\n",
      "Epoch: 8/15...  Training Step: 7201...  Training loss: 2.5770...  Val loss: 3.7355...  1.0825 sec/batch\n",
      "Epoch: 9/15...  Training Step: 7301...  Training loss: 2.5863...  Val loss: 3.7501...  1.0839 sec/batch\n",
      "Epoch: 9/15...  Training Step: 7401...  Training loss: 2.6213...  Val loss: 3.7428...  1.0900 sec/batch\n",
      "Epoch: 10/15...  Training Step: 7501...  Training loss: 2.6539...  Val loss: 3.7213...  1.0836 sec/batch\n",
      "Epoch 10/15 time:206.405255317688...  finished at 2017-10-15 19:19:54\n",
      "Epoch: 11/15...  Training Step: 7601...  Training loss: 2.6854...  Val loss: 3.7100...  1.0916 sec/batch\n",
      "Epoch: 11/15...  Training Step: 7701...  Training loss: 2.5794...  Val loss: 3.7354...  1.0873 sec/batch\n",
      "Epoch: 12/15...  Training Step: 7801...  Training loss: 2.6647...  Val loss: 3.7320...  1.0759 sec/batch\n",
      "Epoch: 12/15...  Training Step: 7901...  Training loss: 2.5638...  Val loss: 3.7359...  1.0886 sec/batch\n",
      "Epoch: 13/15...  Training Step: 8001...  Training loss: 2.5954...  Val loss: 3.6959...  1.0809 sec/batch\n",
      "Epoch: 13/15...  Training Step: 8101...  Training loss: 2.5720...  Val loss: 3.7639...  1.0844 sec/batch\n",
      "Epoch: 14/15...  Training Step: 8201...  Training loss: 2.6176...  Val loss: 3.6989...  1.0854 sec/batch\n",
      "Epoch: 14/15...  Training Step: 8301...  Training loss: 2.5783...  Val loss: 3.7520...  1.0810 sec/batch\n",
      "Epoch: 15/15...  Training Step: 8401...  Training loss: 2.5817...  Val loss: 3.7858...  1.0797 sec/batch\n",
      "Epoch: 15/15...  Training Step: 8501...  Training loss: 2.5714...  Val loss: 3.8354...  1.0909 sec/batch\n",
      "Training ending at time: 2017-10-15 19:37:18\n",
      "Trainint total time: 3126.2703969478607\n"
     ]
    }
   ],
   "source": [
    "#epochs = 1\n",
    "# Save every N iterations\n",
    "save_every_n = 500\n",
    "print_loss_every_n = 100\n",
    "sample_every = 500\n",
    "print_epoch_time_every = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "    #print(\"after model\")\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "print(\"Training starting at time:\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "train_start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Number of parameters:\",get_number_of_parameters(),\"Dataset size:\",len(encoded))\n",
    "    #print(\"after initializer\")\n",
    "    if resume_from_checkpoint:\n",
    "        latest_checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            \n",
    "            \n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "        \n",
    "            \n",
    "            end = time.time()\n",
    "            if counter%print_loss_every_n == 0:\n",
    "                val_batches = get_batches(encoded_val,int(len(encoded_val)/num_steps),num_steps)\n",
    "                x_val,y_val = next(val_batches)\n",
    "                \n",
    "                val_dict = {model.inputs: x_val,\n",
    "                            model.targets: y_val,\n",
    "                            model.keep_prob: 1,\n",
    "                            model.initial_state: new_state}\n",
    "                \n",
    "                val_loss,prediction = sess.run([model.loss,model.prediction],feed_dict=val_dict)\n",
    "                \n",
    "                losses[\"train\"].append(batch_loss)\n",
    "                losses[\"validation\"].append(val_loss)\n",
    "                \n",
    "                \n",
    "                global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "                x_steps.append(global_step)\n",
    "                \n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(global_step),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      'Val loss: {:.4f}... '.format(val_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "                saver.save(sess, \"checkpoints/m{}_i{}_l{}.ckpt\".format(mode,global_step, lstm_size))\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        \n",
    "        \n",
    "        if ((e+1) % print_epoch_time_every== 0):\n",
    "            print('Epoch {}/{} time:{}...'.format(e+1,epochs,epoch_end-epoch_start),\n",
    "                 \" finished at\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "        \n",
    "            \n",
    "    global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "    saver.save(sess, \"checkpoints/m{}_i{}_l{}.ckpt\".format(mode,global_step, lstm_size))\n",
    "    \n",
    "print(\"Training ending at time:\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print(\"Trainint total time:\",time.time()-train_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH0CAYAAACEkWPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5a34bfa90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_steps,losses[\"train\"],label=\"Train loss\")\n",
    "plt.plot(x_steps,losses[\"validation\"],label=\"Validation loss\")\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoints = tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling final trained model\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \",mode=\"characters\"):\n",
    "    print(mode)\n",
    "    samples = tokenize_text(prime,mode)\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in tokenize_text(prime,mode):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples).replace(\"new_line_token\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/mwords_i8550_l512.ckpt'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new text from \"base\" text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "\n",
    "\n",
    "samples = list()\n",
    "for text in text_to_try:\n",
    "    #print(\"------------------------\",text)\n",
    "    samples.append( sample(checkpoint, 500, lstm_size, len(vocab), prime=text,mode=mode))\n",
    "    #print(samp)\n",
    "    #print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ In the first place\n",
      "In the first place of the other manner, as we was to be a very little manner of the same manner in the other manner to the same door. In my own time, we had found a few of my own,\n",
      "a very little time to be been for the whole\n",
      "portion of the whole door --and that a few minutes had not seen a\n",
      "mile to the sea. This latter was a large portion\n",
      "of the vessel, as we was to be so more in his\n",
      "way in our hand. He now found the mate in the water for the\n",
      "time, and I had been a large species of the savages was\n",
      "taken to a great manner.\n",
      "\n",
      "It was very very more than a very very little difficulty for the cabin\n",
      "of the most degree of the cabin. We made us a large manner of water. It was no time in this moment, and I could not be been to be to go in the\n",
      "vessel, I had been a large and very\n",
      "very long; and we had been now found to\n",
      "take a few minutes\n",
      "in the other manner. The brig would have\n",
      "been to a\n",
      "most difficulty of despair, the brig had seen\n",
      "the brig in\n",
      "the sea of the water in a most distance of the\n",
      "cabin in our head in the savages; and, with a most of the sea was\n",
      "been as in our own position, we were not \n",
      "------------------------\n",
      "------------------------ the night before\n",
      "the night before the great world, but I could have been to be the kind of earths core. He was very very long, but he was now in my own way, for that I could not see the same man of the house, and I had no little way to make the kind of horror. But the same night and one of the whole thing of the world. They could have seen my eyes in the old man's old house. The first day was a more. He had been not to make the way in my mind that they could be been. He would have been to be to have seen a great man. It was a few or a few and a few man in all which we had ever. It was not to be to see a kind of old and that they had never been been for the place of the world. But it was a great thing that the man had been to be in a great place and a few thing that he would have been to be to see the place of the town. They was no very very more than the time.\n",
      "It was not a few and the mans place, and he was not a few of the world that the Great Ones had seen to see his house. There was a great man in a great city of, but they had seen that \n",
      "------------------------\n",
      "------------------------ horror\n",
      "horror and the great room of a most  . We had\n",
      "been not a little of the other and the most distance to the water.\n",
      "\n",
      "The first was no longer of the same manner of the same manner in which the vessel would be done for the vessel of his own situation of our water.\n",
      "\n",
      "We could not be no more than it. It was no doubt the most of the whole door, and the most of a\n",
      "most time of the most manner which was not in his own condition. The brig could not be\n",
      "able to be a more than the most of\n",
      "my head, and I could be to be a large degree of\n",
      "latitude -- or longitude 15 degrees degrees''\n",
      "\n",
      "The head of the brig had been been, and\n",
      "I saw to the most time of the\n",
      "water to the water, and in the most portion of\n",
      "our\n",
      "deck, in the sea, we had been in some\n",
      "degree of water, which had been a large and\n",
      "the water of the most\n",
      "of the most distance and the water\n",
      "in the water, we could not be so\n",
      "a little of our way in the water as\n",
      "a little portion of the brig, and the brig was to the\n",
      "vessel with our most species of sea and water\n",
      "to the southward, and the most\n",
      "portion of the schooner, with the water, and the\n",
      "most large and a small\n",
      "portion \n",
      "------------------------\n",
      "------------------------ creature\n",
      "creature to the great, and the most  . It was not a very very very more to be been in my own, and I could not be to make the same time, and the most of this moment we had found a great degree of water. I had not a more than a few, and the\n",
      "whole of the whole manner of our own manner as we\n",
      "should have done a way of the most manner of the\n",
      "water and the most portion of the brig and a\n",
      "great manner. In some minutes the most manner was to be\n",
      "to be to be in the vessel of the cabin. In a large manner, and the most species of\n",
      "thirst we saw that we had been taken from\n",
      "his body in my mind; but I had now found\n",
      "it to make a great difficulty of the vessel\n",
      "to the water of the water, as it would be\n",
      "to be been to have been so a very more to be.\n",
      "\n",
      "We had seen a very very large and one of his own body in the water and the vessel was to be\n",
      "been in the most portion of the cabin,\n",
      "and\n",
      "we could not have been much to the brig of the\n",
      "water\n",
      "of the water in the other portion of the water in\n",
      "the sea, and the vessel was in\n",
      "the most portion of the\n",
      "brig as to be a large condition, and in a\n",
      "------------------------\n",
      "------------------------ night\n",
      "night but  the first were a great degree of the most, and the other time I could have come up to the way, but it is a long more than the whole of which he saw the first of the most thing was so to the place. He did not not be to be been in the way to the sea. The first were the great, in which the most of this latter, however, was no longer.\n",
      "The first of the day had been no longer to make the whole degree of the whole of the most degree, and I had been not to be so.\n",
      "\n",
      "The most of the day, the same thing we were not so to be so in the whole of the water, I had not been to be in the most portion of\n",
      "my own condition. The next day, however, was to be\n",
      "in the most of the water, the whole portion of the\n",
      "cabin of my eyes. It was not no more than\n",
      "the same portion of which he had been at a\n",
      "time to make us a few of the cabin as\n",
      "I could be to be in the same portion of the vessel)\n",
      "and we could not be done, but the most species was to\n",
      "be to be so more than a few\n",
      "of the most degree of the vessel. The whole day was no\n",
      "large or a little degree, and the\n",
      "\n",
      "------------------------\n",
      "------------------------ dream\n",
      "dream of a little. It was no\n",
      "more than the way of the same room. In my own mind I could not be been for a moment. It was a great time to the great door of the sea, but the whole portion of the water had to be a very very very very more than the most of the sea.\n",
      "\n",
      "``You will do you have not to be the word of the matter --the most of the most idea of my own hand -- --the whole words of his own eyes, and it would be a few of my friend, the man was in the whole of the room of his own hand, but it was no more than the most of the same, or the time to the most portion of the\n",
      "cabin and the most degree of terror, which we had not seen the vessel of\n",
      "the same time and in the sea, and, however, I could\n",
      "not make me a kind of water, for my own manner, and I\n",
      "should be to be so to be to be in the most time. The whole portion of the brig was to be to be been to be so\n",
      "to be to take the most manner of the water, and the whole\n",
      "time of my mind was so more than the whole of my own hand, and I\n",
      "could have been a few minutes to the first time as\n",
      "------------------------\n",
      "------------------------ thing\n",
      "thing and  and I had no more in the same man, and the man were; for a few of the great world was a more than that I could not make him a little place of the most degree of earth, and in the most degree of the sea, and a very little man of the most of my mind. We could not make the way of the same place in the most of a same time.\n",
      "\n",
      "The next day, I had been in the same time that he could not be been at length to the other and, the first of the same period. The third were now of a most very few and which he had been made to the whole degree, in the most portion of the\n",
      "brig and the whole day was, in a small manner of the most, and a large islands were thrown up from the cabin of the southward. The most portion of the\n",
      "vessel was in a few minutes, and he was to the\n",
      "way of my own way, as I saw that the mate were to be so to any means of\n",
      "the\n",
      "sea, and the whole of the sea, I had not been\n",
      "been to take a few and the vessel of a most\n",
      "distance of my head. We found a small manner of\n",
      "the brig, in our head, which were a very little portion of the\n",
      "vessel of a\n",
      "great\n",
      "------------------------\n",
      "------------------------ That night\n",
      "That night in the other of my hand. I could not be to be no way to make them more as; for it had no longer to be seen in the other of the most degree of water, and of the whole portion of the cabin which I felt a little of a very few of the most of the most portion of the brig, and the vessel is in our own\n",
      "means, and the whole of the sea, and he had been\n",
      "to be so a very very more than a little difficulty in a\n",
      "great portion of the savages in the\n",
      "cabin.\n",
      "\n",
      "I have not now had no longer to be to\n",
      "be so a very more than more than the whole side,\n",
      "we had not been to be in his hand, and a very\n",
      "little moment to be a little of the vessel in the savages in\n",
      "my\n",
      "hand, the whole manner of the most degree of the vessel.\n",
      "\n",
      "The brig was now to be so a large of the\n",
      "most of my own nature of the cabin in the water, and\n",
      "we had been to be so to be a very large,\n",
      "the most of a vessel of his hand; the mate was the small portion which had to be\n",
      "thrown up by the water to\n",
      "the water; and the other time\n",
      "to,\n",
      "I could not be been in his\n",
      "own condition, and I found a very large portion \n",
      "------------------------\n",
      "------------------------ mountain\n",
      "mountain a  and I had been to say in the whole time in the whole manner in the whole of the same man.\n",
      "\n",
      "The most of the same man was the few of the same day.\n",
      "\n",
      "The body of the old day had been seen in my hand, and the whole of a great thing was so to a little time. We now had been to have no little more than a more than one of the most distance, and in a few manner of the same manner of the other nature. The first was no little, and he was a very very little more than a kind of wine, which there was the most measure of a trap as I could not be to be a long and one of the most of the vessel,\n",
      "I saw the most portion of the brig in the water,\n",
      "and a great portion of a large condition of our hands; as I could not have been\n",
      "to be to be a few of the sea to be\n",
      "in the other of the vessel.\n",
      "\n",
      "It was not a few of the brig\n",
      "of the most degree of a\n",
      "water to the most of the vessel of a\n",
      "vessel as we had been to be, as I\n",
      "could be been to be in the\n",
      "other time and had been to make the\n",
      "hold of the cabin and the most\n",
      "degree of water.\n",
      "\n",
      "``It is no more\n",
      "------------------------\n",
      "------------------------ Ammi\n",
      "Ammi had been a very long and a few of the same world, but that the most of the whole man was so a few or a great state, which he had seen to be to be so to the most of the other, and\n",
      "we found that it were no very very more than the whole degree. We could see the same time,\n",
      "the most of the whole of the sea was so more to be\n",
      "more than the same man of the other manner. We could not\n",
      "have done the same time in the most portion of the\n",
      "schooner. The weather were in two hundred feet in a small\n",
      "distance on a small islands, which had been so\n",
      "to the vessel, the vessel was so very more. I could\n",
      "not have seen a few of the most of the vessel in\n",
      "the sea, which was a large appearance of the brig in the\n",
      "vessel. The brig was a very difficulty of a few\n",
      "of the water, and we had not no more than\n",
      "the brig to the southward; and the\n",
      "first of the sea had been so to the brig. I had found a large manner in a most portion of the\n",
      "vessel, as the whole of the sea was in the water as the whole vessel was the\n",
      "vessel of the sea,\n",
      "we were now as much of the vessel, and in\n",
      "the vessel of the brig had been so\n",
      "------------------------\n",
      "------------------------ Cthulhu\n",
      "Cthulhu of this the same time I found the most portion of the vessel and a large nature of the vessel.\n",
      "\n",
      "The whole of a great portion of the brig were to the water, and the vessel would be to be, and we was\n",
      "to make a little deal of a very little manner, and we had\n",
      "been been to be.\n",
      "\n",
      "``You will you know it, as I am no longer; and you can not be to do the word --you --the matter you will be to see a kind of your eyes --you will be no longer -- I, and you say it?'' The fact, I have now not to be sure, that I cannot say, as I have not to be sure, as he had to say, the thing in my hand; and I have not a few of a few of the most thing in the house --the whole man was a kind of wine. He had been so in a few minutes of the time in the same period. I had not been to be so much to the first of the time. He was no doubt, for the time of his own voice --the man of my own friend.\n",
      "\n",
      "It is not to say, but, as he was to be to be the word in my head. It is a few minutes that the most portion of the brig),\n",
      "------------------------\n",
      "------------------------ raven\n",
      "raven and the first of this day I had seen to the most of the same man. The third was no more, or now could not have seen a great degree, while it was very more as well in the same.\n",
      "\n",
      "``And a more than a little idea of this man are the matter, and it is a few of the way of the same man, and I could not say the first moment of the room of my own own voice. I have been a little of the way of the whole time in this time --the first man, it was no longer a few in a time, as the same time in my hand. In the whole moment it would have been to make. He now had been to make a few of my mind as he could do. But I had not to be to have no way of the house. The most time of this time we was to take them by the way of my own mind and the man was not to be to be to be in the same\n",
      "manner in the other world. I had no more than a little difficulty of my\n",
      "friend, and it had been been to take the same way in the sea, and we had been no little more than the vessel to have been to be in our attention to the sea.\n",
      "\n",
      "We had been, and I\n",
      "------------------------\n",
      "------------------------ bird\n",
      "bird the   of the most time which I have seen the way of a other world; for the time of the other man was the very very few and one had been to make the way from my mind. The whole day were the great door in the sea's  , the great line of the ground, which was a great distance of their black rock.\n",
      "\n",
      "``And it is that you is a little man of your eyes --the matter of a very little matter? But, that they had not been to do? It was the thing to do that it, but the old thing had been to have been a very few of the old man, for a little man was not so to be the matter of the world of my own mind. The first man was to be in the most of the same thing. The first of the same time he had been so to be a few of his own hand, and it was a very more than that the house of the room of the world was so more than the man of a most thing to be been at a great distance to the great abyss.\n",
      "\n",
      "It had been no long, and he had been to be in the most distance; for this time the most time was not to be seen. It was no longer, but was the most of my own \n",
      "------------------------\n",
      "------------------------ nevermore\n",
      "nevermore The  , the most of the water were in the sea. We could scarcely be done with a little distance, in the most of the cabin, as the same period of the sea. The whole of this time, however, were a small appearance of the whole door, and we could not be seen to\n",
      "be the few of the most of the most vessel, and we had been to be a\n",
      "large position, that we saw a large nature which could be seen in\n",
      "a large condition, and that we had been\n",
      "no difficulty in his eyes. In a time he had been\n",
      "so more than that it was no very longer. The mate had been in my\n",
      "attention, in one of the water, and in the brig.\n",
      "\n",
      "It are now, however, that I could not be\n",
      "to be to have been so, in the vessel of, the first manner\n",
      "in the water of my own condition.\n",
      "\n",
      "The whole portion of the whole of the\n",
      "brig were to the first the whole of the savages, and the whole manner had been,\n",
      "for that the whole portion of the whole of the vessel was to\n",
      "be\n",
      "to be to be to be been at once.\n",
      "\n",
      "In these time, however, I could have been a few portion of\n",
      "my head at a most portion of the brig in the\n",
      "sea of the water.\n",
      "\n",
      "The body of the \n",
      "------------------------\n",
      "------------------------ dead\n",
      "dead and a little time in my way for the other of the most of the same room, but the whole of the whole day of the old city had been seen in the same place and other feet and a great abyss. It were no longer to say it, but it was not to be so more than the man of the old world of the old man. It was not the few of his dreams and that the old city was a very kind of stone or to the world. It was not a very very very long, but the thing would be been for that the world was more than the old side of the city in the town and had ever ever found. There was, and a little, the first had been, and that he was the very very little thing of his house as the old Old Ones had been known to the world; and the first   , the old man had been to be seen. It was very great of the old city of the great town and the black stone city in the Great Ones of the earth; and that the Old Ones had a more than a great stone of a other and of the Great Ones the earths camp and the great world in earths earths mountains which. There was not the monstrous stone and the ancient peaks of the city. \n",
      "------------------------\n",
      "------------------------ The bird\n",
      "The bird of the most  and a very few of the whole day had been been as to make a great distance. The whole of the whole day we saw the first time of the whole manner and a most thing was so more than a kind of\n",
      "London, and this latter had been found a large feet with the cabin and the\n",
      "same time the most degree of earth. The whole portion of my eyes had taken, but I could not make the most moment, and it was not a long manner to be to be more, and the first of the time was a few degree of a large position in the vessel and a large islands of the cabin in the sea, and a few of the vessel were\n",
      "a great distance; and the small number of the vessel was the vessel in\n",
      "our head in the sea of the sea.\n",
      "\n",
      "The brig were now, in a large manner, with a large manner in the\n",
      "water, in the other time. I had been no\n",
      "doubt, and the first day of a most manner of the\n",
      "cabin. We now had been been at a little portion of my hand.\n",
      "\n",
      "It was no longer a very few. We had been no longer\n",
      "to make me the time.\n",
      "\n",
      "The first day was the very very great portion of the cabin with the brig, and the mate was been\n",
      "upon the way.\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text_to_try)):\n",
    "    text = text_to_try[i]\n",
    "    generated = samples[i]\n",
    "    print(\"------------------------\",text)\n",
    "    print(generated)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/mwords_i8550_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mwords_i6701_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mwords_i7201_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mwords_i7701_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mwords_i8201_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mwords_i8550_l512.ckpt\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mwords_i8550_l512.ckpt\n",
      "The thing that should not be to be to make the same man; but it is a little more than the time of the most portion of the water; but in his hand, the same period he would not be to be a few of the most of the sea, and the first of the same time, we had been been at length, but to the most of the whole of our hand -- and a most of my head was to be to be to the vessel.\n",
      "\n",
      "We had not not been to say the most time to be to\n",
      "have to be to take my head with the most of the cabin and the\n",
      "brig had ever been been at once in a great portion\n",
      "of the water) and a few of a other\n",
      "manner of the sea and most of the vessel\n",
      "as the vessel had been to be been at the sea of the vessel and the sea\n",
      "were so much and the first of the water and a other portion\n",
      "of the sea, and in a great portion of the brig (with our\n",
      "way with the most portion of\n",
      "my eyes; and a little manner, in the same manner\n",
      "of a large manner and the water, and was so much in a\n",
      "great distance. We now made a few of the cabin\n",
      "in the cabin and the brig had been taken from the savages\n",
      "in the cabin which I could not say, as it was a\n",
      "few of the water; but the most of the\n",
      "most of the brig, and the whole manner of\n",
      "his head were at the most time\n",
      "of the most portion of the cabin and a few of the vessel\n",
      "of the vessel in the sea, we found the whole degree of deck. The whole manner\n",
      "were found with a great manner, and the whole day, and a\n",
      "most very large manner which made us no doubt to the vessel. In a time\n",
      "a great portion of a vessel had been so to any\n",
      "the vessel, we were to be been to be\n",
      "so more than that I was, and a few of our most\n",
      "of the most manner of my companions was the mate\n",
      "before.\n",
      "\n",
      "I have made a few degree of a great position; for a\n",
      "most time in my head, as it would have, in the most manner\n",
      "of a great vessel of the sea, which we could\n",
      "not be to be to be\n",
      "to make them to make the vessel to\n",
      "the vessel of the water\n",
      "in the water, but in the other\n",
      "manner, and I had found a large appearance, the whole and. The whole\n",
      "day had been been so as I could not be been to\n",
      "make us a large degree, and the whole of his brig, as\n",
      "I could not make us be to be so in the most portion of\n",
      "the cabin, in my\n",
      "hand, I had been to be a little difficulty in his\n",
      "head in a vessel in a most manner. We\n",
      "had been to the most degree of deck, and\n",
      "the vessel was in a few of our head\n",
      "from the savages in the savages of the vessel, the most of the brig of the\n",
      "sea, and a large islands was a few of the water. We was not the\n",
      "great difficulty which could not be able to be so, and we could\n",
      "see the whole of the water in the vessel\n",
      "of the cabin as I should be been at least, as he, and I\n",
      "could be able to get up that a few minutes. I\n",
      "had found, I was no more than the whole of a large condition\n",
      "of the water; as the most of the water,\n",
      "we had no more than a very little, as we had\n",
      "found the whole portion of the brig (with the cabin or\n",
      "to the water. We had been in\n",
      "my hand as the brig were to the sea. We found the brig\n",
      "at the whole period, the wind of our body was so much in the vessel\n",
      "to the cabin, as we could not say a few of our\n",
      "way to his mind in the cabin, and I\n",
      "found the most degree of thirst, the whole canoes\n",
      "in the cabin of the\n",
      "cabin of our body. The whole manner is been as as\n",
      "the most manner of my own hand, I had found the most\n",
      "large and the water was to be a more than a most\n",
      "of my own condition, and he had now been to take him\n",
      "by my own.\n",
      "\n",
      "``It will be sure,'' replied this, as I could not be done. I was no little very less than a very little of the water, I had no longer no reason for the same manner. He had been in the most portion of the whole door -- and it would have been to be so more than the way on my hand. It was not a few and the most difficulty, and the first\n",
      "of the first moment the most time of his own position -- the most of a whole manner, and the most portion of the brig.\n",
      "\n",
      "The whole manner had made a little manner, as he was a large manner to the\n",
      "water, and that the time was the most difficulty and\n",
      "discovered his own situation and to be been to make\n",
      "a large manner of the sea. It was very very little more,\n",
      "and a few of the brig were to the water, and the mate\n",
      "was not so more than that the brig, and it was no\n",
      "way to the sea of our most degree, and it. The brig\n",
      "were no longer to be been for the vessel of the\n",
      "vessel, but a large manner of our head on the sea,\n",
      "which was a small portion of the vessel as\n",
      "a great vessel, which was not been in the vessel to the\n",
      "cabin of my own way. We could not say that it had\n",
      "been been to be to make a great distance. We now now no\n",
      "little\n",
      "of a great degree of water. I was no more than a\n",
      "large position of the cabin, and I could not help a large manner.\n",
      "\n",
      "In a most portion of the whole of the whole portion of his\n",
      "own body in the most manner of the sea.\n",
      "\n",
      "The most time was not to be to be been;\n",
      "but I had not been at\n",
      "a moment, and he could not have been the\n",
      "vessel and a most portion of the savages\n",
      "at the manner. The whole time is a very large, and\n",
      "we could not be,\n",
      "I saw, I was to have no means to\n",
      "be in the most degree of the\n",
      "deck of a great extent, a large vessel and\n",
      "two of the cabin and\n",
      "be taken from a small degree of a most\n",
      "distance of the sea. The vessel was to\n",
      "be no more than the water,\n",
      "and to be to have been to make the way\n",
      "to a great portion, which had been\n",
      "found in the most degree of water.\n",
      "\n",
      "In the most portion of the whole of this\n",
      "day, we was to be a small difficulty, and\n",
      "now immediately in the sea to the vessel of the\n",
      "savages to the cabin. The whole portion of a whole portion\n",
      "was in our hands, as we had been a very few of our body. We were a little portion of the\n",
      "brig in the sea, and\n",
      "the most time\n",
      "I could be able to have been the rope to our\n",
      "hold.\n",
      "\n",
      "The brig was a very little very large in the sea in the water.\n",
      "\n",
      "The brig was a very very little more than a few of our water, but a few and the\n",
      "whole manner were in the sea, and I had been to\n",
      "be to make a large state of the\n",
      "sea; and the mate was to be\n",
      "so very far to be so much than a large portion of the\n",
      "savages in our head; the brig, and\n",
      "the whole time of the savages, and we found, the most of the\n",
      "vessel was to the vessel of the\n",
      "cabin in our own hand, and the brig\n",
      "had been to be in a\n",
      "few of the sea.\n",
      "\n",
      "``The word, --the whole time is a little very very long of the same time I should see the vessel of the whole\n",
      "door in a great manner of the whole, and I was not\n",
      "to be so more than it, for a time I should have been to be a\n",
      "very few degree of. I was a little more than the whole manner in a\n",
      "vessel. The most moment had been been to the vessel of the water,\n",
      "which was no longer so more than a little portion of the\n",
      "brig, and it, in the\n",
      "other time I could not help myself\n",
      "at the manner in the vessel, the mate had\n",
      "been to take the cabin\n",
      "with a most difficulty of despair, the mate\n",
      "had been to make a large position, the brig in our\n",
      "savages, I could scarcely be to go from the vessel\n",
      "and the water of my own;\n",
      "and he were no very more\n",
      "than the vessel in the water, in the vessel in our deck, as\n",
      "the vessel was now to take the most portion of\n",
      "the savages\n",
      "in the cabin, which he had been made from a most\n",
      "of a large condition, as the brig had been to\n",
      "be in a large degree of the vessel in the water.\n",
      "\n",
      "In the whole day he could not have been to be to be\n",
      "in the most degree of latitude)\n",
      "\n",
      "((the longitude -- -- , upon the brig\n",
      "of the brig, with the vessel of the\n",
      "water, which was the most very little than the\n",
      "vessel of the water, the\n",
      "whole of the brig had been to\n",
      "make a great manner, and I found a great manner from the cabin and\n",
      "no great degree of water, which would be to be\n",
      "so to the sea. I could scarcely say\n",
      "that the time were not been so to be in his\n",
      "mind in our way for my head of the water\n",
      "of my mind in the vessel to the most degree of water. The\n",
      "whole of this latter had been been at least in\n",
      "the sea, and we found the first\n",
      "time to the water as a\n",
      "few of his hand. In a\n",
      "most moment the vessel had been\n",
      "found that we could be seen, and the first of this period\n",
      "of the most time. I saw a very few of the vessel we had been been.\n",
      "\n",
      "It was a very very very few of the vessel, and a large\n",
      "manner of my eyes, a small manner in our water.\n",
      "\n",
      "We was no little than my mind to make us to be a\n",
      "large manner with the cabin, and the brig were to the vessel in\n",
      "the vessel of the vessel, and in the\n",
      "most time in the sea in the vessel, I had\n",
      "been so, as he could never have been to the\n",
      "most portion of the savages, with\n",
      "his way, the most portion of the savages\n",
      "in the cabin, the mate were to be so more than it in the\n",
      "vessel, and he were not a few of a most\n",
      "difficulty and immediately in the water, as\n",
      "the first of the\n",
      "sea, we were now to be a few\n",
      "of the most manner of a\n",
      "large position, a small islands of our head in\n",
      "the savages.\n",
      "\n",
      "In this manner the whole manner were in the sea, and\n",
      "I had been to the first of a few degree of\n",
      "a great extent of the water as the vessel of our water.\n",
      "\n",
      "In the whole manner the brig, however, we found it in the same\n",
      "moment in this manner to be the vessel, but the whole portion\n",
      "of the savages, in a small manner, which he was no more\n",
      "to be more than the water. It was the most\n",
      "portion of my head in the sea, and a most difficulty of thirst\n",
      "and nearly up which we could see a vessel; and the most difficulty\n",
      "was in a large position, the vessel\n",
      "was a few of my own hand. It is\n",
      "not to be a little portion, and it was,\n",
      "to the first manner that of the most of our\n",
      "head was so\n",
      "far to be so far at the most manner to\n",
      "be to make a vessel\n",
      "in the water to the most\n",
      "portion of the vessel. This being now was\n",
      "a large manner in the cabin, and it, the whole\n",
      "manner was so very much and more than the whole time in the\n",
      "most portion of\n",
      "the brig, and at the same time, the mate\n",
      "had been taken by our hand.\n",
      "\n",
      "We found a large manner, and a great difficulty\n",
      "and discovered a few breeze from the cabin\n",
      "and the most portion of our head, as\n",
      "the brig was no\n",
      "longer and possible. It had been very long in the water, and had\n",
      "to be to be so more than the manner, but it was to make a few minutes in a\n",
      "large\n",
      "portion of his companions, as if I saw the vessel to\n",
      "the sea,\n",
      "and the most of a large condition was the vessel, and we had\n",
      "been now to have been a\n",
      "large degree, as he had taken a large\n",
      "degree, and a few minutes in the vessel, we\n",
      "had been\n",
      "to be to be so in my hand.\n",
      "\n",
      "``And it, as I am no good -- in a kind'''' I replied, ``as I had been at a time to the most time --I am no longer to be been at the matter. It was to be so very long, or to say. ``It is a little of a little idea; but I am to do it ----I am not not to be to say, and, in the word, the man, I am sure the skull is that the thing of the matter I can?''\n",
      "\n",
      "I have been to be a few of the most of the vessel in the whole manner, the most manner of \n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 5000, lstm_size, len(vocab), prime=\"The thing that should not be\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
