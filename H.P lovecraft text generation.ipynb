{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks for H.P Lovecraft text generation\n",
    "\n",
    "\"The color out of space\" is one of my favorite tales from Lovecraft, i will use it(as well as others as the call of cthulhu) to create a recurrent neural network in tensorflow that learns his style and generates new text in his style\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('colour_out_of_space.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE COLOUR OUT OF SPACE\\n\\nWest of Arkham the hills rise wild, and there are valleys with deep woods that no axe has ever cut. There are dark narrow glens where the trees slope fantastically, and where thin brooklets trickle without ever having caught the glint of sunlight. On the gentle slopes there are farms, ancient and rocky, with squat, moss-coated cottages brooding eternally over old New England secrets in the lee of great ledges; but these are all vacant now, the wide chimneys crumbling and the shingled sides bulging perilously beneath low gambrel roofs.\\n\\nThe old folk have gone away, and foreigners do not like to live there. French-Canadians have tried it, Italians have tried it, and the Poles have come and departed. It is not because of anything that can be seen or heard or handled, but because of something that is imagined. The place is not good for imagination, and does not bring restful dreams at night. It must be this which keeps the foreigners away, for old Ammi Pierce has never told them of anything he recalls from the strange days. Ammi, whose head has been a little queer for years, is the only one who still remains, or who ever talks of the strange days; and he dares to do this because his house is so near the open fields and the travelled roads around Arkham.\\n\\nThere was once a road over the hills and through the valleys, that ran straight where the blasted heath is now; but people ceased to use it and a new road was laid curving far toward the south. Traces of the old one can still be found amidst the weeds of a returning wilderness, and some of them will doubtless linger even when half the hollows are flooded for the new reservoir. Then the dark woods will be cut down and the blasted heath will slumber far below blue waters whose surface will mirror the sky and ripple in the sun. And the secrets of the strange days will be one with the deep\\'s secrets; one with the hidden lore of old ocean, and all the mystery of primal earth.\\n\\nWhen I went into the hills and vales to survey for the new reservoir they told me the place was evil. They told me this in Arkham, and because that is a very old town full of witch legends I thought the evil must be something which grandams had whispered to children through centuries. The name \"blasted heath\" seemed to me very odd and theatrical, and I wondered how it had come into the folklore of a Puritan people. Then I saw that dark westward tangle of glens and slopes for myself, and ceased to wonder at anything beside its own elder mystery. It was morning when I saw it, but shadow lurked always there. The trees grew too thickly, and their trunks were too big for any healthy New England wood. There was too much silence in the dim alleys between them, and the floor was too soft with the dank moss and mattings of infinite years of decay.\\n\\nIn the open spaces, mostly along the line of the old road, there were little hillside farms; sometimes with all the buildings standing, sometimes with only one or two, and sometimes with only a lone chimney or fast-filling cellar. Weeds and briers reigned, and furtive wild things rustled in the undergrowth. Upon everything was a haze of restlessness and oppression; a touch of the unreal and the grotesque, as if some vital element of perspective or chiaroscuro were awry. I did not wonder that the foreigners would not stay, for this was no region to sleep in. It was too much like a landscape of Salvator Rosa; too much like some forbidden woodcut in a tale of terror.\\n\\nBut even all this was not so bad as the blasted heath. I knew it the moment I came upon it at the bottom of a spacious valley; for no other name could fit such a thing, or any other thing fit such a name. It was as if the poet had coined the phrase from having seen this one particular region. It must, I thought as I viewed it, be the outcome of a fire; but why had nothing new ever grown over these five acres of grey desolation that sprawled open to the sky like a great spot eaten by acid in the woods and fields? It lay largely to the north of the ancient road line, but encroached a little on the other side. I felt an odd reluctance about approaching, and did so at last only because my business took me through and past it. There was no vegetation of any kind on that broad expanse, but only a fine grey dust or ash which no wind seemed ever to blow about. The trees near it were sickly and stunted, and many dead trunks stood or lay rotting at the rim. As I walked hurriedly by I saw the tumbled bricks and stones of an old chimney and cellar on my right, and the yawning black maw of an abandoned well whose stagnant vapours played strange tricks with the hues of the sunlight. Even the long, dark woodland climb beyond seemed welcome in contrast, and I marvelled no more at the frightened whispers of Arkham people. There had been no house or ruin near; even in the old days the place must have been lonely and remote. And at twilight, dreading to repass that ominous spot, I walked circuitously back to the town by the curious road on the south. I vaguely wished some clouds would gather, for an odd timidity about the deep skyey voids above had crept into my soul.\\n\\nIn the evening I asked old people in Arkham about the blasted heath, and what was meant by that phrase \"strange days\" which so many evasively muttered. I could not, however, get any good answers except that all the mystery was much more recent than I had dreamed. It was not a matter of old legendry at all, but something within the lifetime of those who spoke. It had happened in the \\'eighties, and a family had disappeared or was killed. Speakers would not be exact; and because they all told me to pay no attention to old Ammi Pierce\\'s crazy tales, I sought him out the next morning, having heard that he lived alone in the ancient tottering cottage where the trees first begin to get very thick. It was a fearsomely ancient place, and had begun to exude the faint miasmal odour which clings about houses that have stood too long. Only with persistent knocking could I rouse the aged man, and when he shuffled timidly to the door I could tell he was not glad to see me. He was not so feeble as I had expected; but his eyes drooped in a curious way, and his unkempt clothing and white beard made him seem very worn and dismal.\\n\\nNot knowing just how he could best be launched on his tales, I feigned a matter of business; told him of my surveying, and asked vague questions about the district. He was far brighter and more educated than I had been led to think, and before I knew it had grasped quite as much of the subject as any man I had talked with in Arkham. He was not like other rustics I had known in the sections where reservoirs were to be. From him there were no protests at the miles of old wood and farmland to be blotted out, though perhaps there would have been had not his home lain outside the bounds of the future lake. Relief was all that he showed; relief at the doom of the dark ancient valleys through which he had roamed all his life. They were better under water now--better under water since the strange days. And with this opening his husky voice sank low, while his body leaned forward and his right forefinger began to point shakily and impressively.\\n\\nIt was then that I heard the story, and as the rambling voice scraped and whispered on I shivered again and again spite the summer day. Often I had to recall the speaker from ramblings, piece out scientific points which he knew only by a fading parrot memory of professors\\' talk, or bridge over gaps, where his sense of logic and continuity broke down. When he was done I did not wonder that his mind had snapped a trifle, or that the folk of Arkham would not speak much of the blasted heath. I hurried back before sunset to my hotel, unwilling to have the stars come out above me in the open; and the next day returned to Boston to give up my position. I could not go into that dim chaos of old forest and slope again, or face another time that grey blasted heath where the black well yawned deep beside the tumbled bricks and stones. The reservoir will soon be built now, and all those elder secrets will be safe forever under watery fathoms. But even then I do not believe I would like to visit that country by night--at least not when the sinister stars are out; and nothing could bribe me to drink the new city water of Arkham.\\n\\nIt all began, old Ammi said, with the meteorite. Before that time there had been no wild legends at all since the witch trials, and even then these western woods were not feared half so much as the small island in the Miskatonic where the devil held court beside a curious lone altar older than the Indians. These were not haunted woods, and their fantastic dusk was never terrible till the strange days. Then there had come that white noontide cloud, that string of explosions in the air, and that pillar of smoke from the valley far in the wood. And by night all Arkham had heard of the great rock that fell out of the sky and bedded itself in the ground beside the well at the Nahum Gardner place. That was the house which had stood where the blasted heath was to come--the trim white Nahum Gardner house amidst its fertile gardens and orchards.\\n\\nNahum had come to town to tell people about the stone, and dropped in at Ammi Pierce\\'s on the way. Ammi was forty then, and all the queer things were fixed very strongly in his mind. He and his wife had gone with the three professors from Miskatonic University who hastened out the next morning to see the weird visitor from unknown stellar space, and had wondered why Nahum had called it so large the day before. It had shrunk, Nahum said as he pointed out the big brownish mound above the ripped earth and charred grass near the archaic well-sweep in his front yard; but the wise men answered that stones do not shrink. Its heat lingered persistently, and Nahum declared it had glowed faintly in the night. The professors tried it with a geologist\\'s hammer and found it was oddly soft. It was, in truth, so soft as to be almost plastic; and they gouged rather than chipped a specimen to take back to the college for testing. They took it in an old pail borrowed from Nahum\\'s kitchen, for even the small piece refused to grow cool. On the trip back they stopped at Ammi\\'s to rest, and seemed thoughtful when Mrs. Pierce remarked that the fragment was growing smaller and burning the bottom of the pail. Truly, it was not large, but perhaps they had taken less than they thought.\\n\\nThe day after that--all this was in June of \\'82--the professors had trooped out again in a great excitement. As they passed Ammi\\'s they told him what queer things the specimen had done, and how it had faded wholly away when they put it in a glass beaker. The beaker had gone, too, and the wise men talked of the strange stone\\'s affinity for silicon. It had acted quite unbelievably in that well-ordered laboratory; doing nothing at all and showing no occluded gases when heated on charcoal, being wholly negative in the borax bead, and soon proving itself absolutely non-volatile at any producible temperature, including that of the oxy-hydrogen blowpipe. On an anvil it appeared highly malleable, and in the dark its luminosity was very marked. Stubbornly refusing to grow cool, it soon had the college in a state of real excitement; and when upon heating before the spectroscope it displayed shining bands unlike any known colours of the normal spectrum there was much breathless talk of new elements, bizarre optical properties, and other things which puzzled men of science are wont to say when faced by the unknown.\\n\\nHot as it was, they tested it in a crucible with all the proper reagents. Water did nothing. Hydrochloric acid was the same. Nitric acid and even aqua regia merely hissed and spattered against its torrid invulnerability. Ammi had difficulty in recalling all these things, but recognized some solvents as I mentioned them in the usual order of use. There were ammonia and caustic soda, alcohol and ether, nauseous carbon disulphide and a dozen others; but although the weight grew steadily less as time passed, and the fragment seemed to be slightly cooling, there was no change in the solvents to show that they had attacked the substance at all. It was a metal, though, beyond a doubt. It was magnetic, for one thing; and after its immersion in the acid solvents there seemed to be faint traces of the Widmanstatten figures found on meteoric iron. When the cooling had grown very considerable, the testing was carried on in glass; and it was in a glass beaker that they left all the chips made of the original fragment during the work. The next morning both chips and beaker were gone without trace, and only a charred spot marked the place on the wooden shelf where they had been.\\n\\nAll this the professors told Ammi as they paused at his door, and once more he went with them to see the stony messenger from the stars, though this time his wife did not accompany him. It had now most certainly shrunk, and even the sober professors could not doubt the truth of what they saw. All around the dwindling brown lump near the well was a vacant space, except where the earth had caved in; and whereas it had been a good seven feet across the day before, it was now scarcely five. It was still hot, and the sages studied its surface curiously as they detached another and larger piece with hammer and chisel. They gouged deeply this time, and as they pried away the smaller mass they saw that the core of the thing was not quite homogeneous.\\n\\nThey had uncovered what seemed to be the side of a large coloured globule embedded in the substance. The colour, which resembled some of the bands in the meteor\\'s strange spectrum, was almost impossible to describe; and it was only by analogy that they called it colour at all. Its texture was glossy, and upon tapping it appeared to promise both brittleness and hollowness. One of the professors gave it a smart blow with a hammer, and it burst with a nervous little pop. Nothing was emitted, and all trace of the thing vanished with the puncturing. It left behind a hollow spherical space about three inches across, and all thought it probable that others would be discovered as the enclosing substance wasted away.\\n\\nConjecture was vain; so after a futile attempt to find additional globules by drilling, the seekers left again with their new specimen which proved, however, as baffling in the laboratory as its predecessor. Aside from being almost plastic, having heat, magnetism, and slight luminosity, cooling slightly in powerful acids, possessing an unknown spectrum, wasting away in air, and attacking silicon compounds with mutual destruction as a result, it presented no identifying features whatsoever; and at the end of the tests the college scientists were forced to own that they could not place it. It was nothing of this earth, but a piece of the great outside; and as such dowered with outside properties and obedient to outside laws.\\n\\nThat night there was a thunderstorm, and when the professors went out to Nahum\\'s the next day they met with a bitter disappointment. The stone, magnetic as it had been, must have had some peculiar electrical property; for it had \"drawn the lightning,\" as Nahum said, with a singular persistence. Six times within an hour the farmer saw the lightning strike the furrow in the front yard, and when the storm was over nothing remained but a ragged pit by the ancient well-sweep, half-choked with a caved-in earth. Digging had borne no fruit, and the scientists verified the fact of the utter vanishment. The failure was total; so that nothing was left to do but go back to the laboratory and test again the disappearing fragment left carefully cased in lead. That fragment lasted a week, at the end of which nothing of value had been learned of it. When it had gone, no residue was left behind, and in time the professors felt scarcely sure they had indeed seen with waking eyes that cryptic vestige of the fathomless gulfs outside; that lone, weird message from other universes and other realms of matter, force, and entity.\\n\\nAs was natural, the Arkham papers made much of the incident with its collegiate sponsoring, and sent reporters to talk with Nahum Gardner and his family. At least one Boston daily also sent a scribe, and Nahum quickly became a kind of local celebrity. He was a lean, genial person of about fifty, living with his wife and three sons on the pleasant farmstead in the valley. He and Ammi exchanged visits frequently, as did their wives; and Ammi had nothing but praise for him after all these years. He seemed slightly proud of the notice his place had attracted, and talked often of the meteorite in the succeeding weeks. That July and August were hot; and Nahum worked hard at his haying in the ten-acre pasture across Chapman\\'s Brook; his rattling wain wearing deep ruts in the shadowy lanes between. The labour tired him more than it had in other years, and he felt that age was beginning to tell on him.\\n\\nThen fell the time of fruit and harvest. The pears and apples slowly ripened, and Nahum vowed that his orchards were prospering as never before. The fruit was growing to phenomenal size and unwonted gloss, and in such abundance that extra barrels were ordered to handle the future crop. But with the ripening came sore disappointment, for of all that gorgeous array of specious lusciousness not one single jot was fit to eat. Into the fine flavour of the pears and apples had crept a stealthy bitterness and sickishness, so that even the smallest bites induced a lasting disgust. It was the same with the melons and tomatoes, and Nahum sadly saw that his entire crop was lost. Quick to connect events, he declared that the meteorite had poisoned the soil, and thanked Heaven that most of the other crops were in the upland lot along the road.\\n\\nWinter came early, and was very cold. Ammi saw Nahum less often than usual, and observed that he had begun to look worried. The rest of his family too, seemed to have grown taciturn; and were far from steady in their church-going or their attendance at the various social events of the countryside. For this reserve or melancholy no cause could be found, though all the household confessed now and then to poorer health and a feeling of vague disquiet. Nahum himself gave the most definite statement of anyone when he said he was disturbed about certain footprints in the snow. They were the usual winter prints of red squirrels, white rabbits, and foxes, but the brooding farmer professed to see something not quite right about their nature and arrangement. He was never specific, but appeared to think that they were not as characteristic of the anatomy and habits of squirrels and rabbits and foxes as they ought to be. Ammi listened without interest to this talk until one night when he drove past Nahum\\'s house in his sleigh on the way back from Clark\\'s Comer. There had been a moon, and a rabbit had run across the road, and the leaps of that rabbit were longer than either Ammi or his horse liked. The latter, indeed, had almost run away when brought up by a firm rein. Thereafter Ammi gave Nahum\\'s tales more respect, and wondered why the Gardner dogs seemed so cowed and quivering every morning. They had, it developed, nearly lost the spirit to bark.\\n\\nIn February the McGregor boys from Meadow Hill were out shooting woodchucks, and not far from the Gardner place bagged a very peculiar specimen. The proportions of its body seemed slightly altered in a queer way impossible to describe, while its face had taken on an expression which no one ever saw in a woodchuck before. The boys were genuinely frightened, and threw the thing away at once, so that only their grotesque tales of it ever reached the people of the countryside. But the shying of horses near Nahum\\'s house had now become an acknowledged thing, and all the basis for a cycle of whispered legend was fast taking form.\\n\\nPeople vowed that the snow melted faster around Nahum\\'s than it did anywhere else, and early in March there was an awed discussion in Potter\\'s general store at Clark\\'s Corners. Stephen Rice had driven past Gardner\\'s in the morning, and had noticed the skunk-cabbages coming up through the mud by the woods across the road. Never were things of such size seen before, and they held strange colours that could not be put into any words. Their shapes were monstrous, and the horse had snorted at an odour which struck Stephen as wholly unprecedented. That afternoon several persons drove past to see the abnormal growth, and all agreed that plants of that kind ought never to sprout in a healthy world. The bad fruit of the fall before was freely mentioned, and it went from mouth to mouth that there was poison in Nahum\\'s ground. Of course it was the meteorite; and remembering how strange the men from the college had found that stone to be, several farmers spoke about the matter to them.\\n\\nOne day they paid Nahum a visit; but having no love of wild tales and folklore were very conservative in what they inferred. The plants were certainly odd, but all skunk-cabbages are more or less odd in shape and hue. Perhaps some mineral element from the stone had entered the soil, but it would soon be washed away. And as for the footprints and frightened horses--of course this was mere country talk which such a phenomenon as the aerolite would be certain to start. There was really nothing for serious men to do in cases of wild gossip, for superstitious rustics will say and believe anything. And so all through the strange days the professors stayed away in contempt. Only one of them, when given two phials of dust for analysis in a police job over a year and half later, recalled that the queer colour of that skunk-cabbage had been very like one of the anomalous bands of light shown by the meteor fragment in the college spectroscope, and like the brittle globule found imbedded in the stone from the abyss. The samples in this analysis case gave the same odd bands at first, though later they lost the property.\\n\\nThe trees budded prematurely around Nahum\\'s, and at night they swayed ominously in the wind. Nahum\\'s second son Thaddeus, a lad of fifteen, swore that they swayed also when there was no wind; but even the gossips would not credit this. Certainly, however, restlessness was in the air. The entire Gardner family developed the habit of stealthy listening, though not for any sound which they could consciously name. The listening was, indeed, rather a product of moments when consciousness seemed half to slip away. Unfortunately such moments increased week by week, till it became common speech that \"something was wrong with all Nahum\\'s folks.\" When the early saxifrage came out it had another strange colour; not quite like that of the skunk-cabbage, but plainly related and equally unknown to anyone who saw it. Nahum took some blossoms to Arkham and showed them to the editor of the Gazette, but that dignitary did no more than write a humorous article about them, in which the dark fears of rustics were held up to polite ridicule. It was a mistake of Nahum\\'s to tell a stolid city man about the way the great, overgrown mourning-cloak butterflies behaved in connection with these saxifrages.\\n\\nApril brought a kind of madness to the country folk, and began that disuse of the road past Nahum\\'s which led to its ultimate abandonment. It was the vegetation. All the orchard trees blossomed forth in strange colours, and through the stony soil of the yard and adjacent pasturage there sprang up a bizarre growth which only a botanist could connect with the proper flora of the region. No sane wholesome colours were anywhere to be seen except in the green grass and leafage; but everywhere were those hectic and prismatic variants of some diseased, underlying primary tone without a place among the known tints of earth. The \"Dutchman\\'s breeches\" became a thing of sinister menace, and the bloodroots grew insolent in their chromatic perversion. Ammi and the Gardners thought that most of the colours had a sort of haunting familiarity, and decided that they reminded one of the brittle globule in the meteor. Nahum ploughed and sowed the ten-acre pasture and the upland lot, but did nothing with the land around the house. He knew it would be of no use, and hoped that the summer\\'s strange growths would draw all the poison from the soil. He was prepared for almost anything now, and had grown used to the sense of something near him waiting to be heard. The shunning of his house by neighbors told on him, of course; but it told on his wife more. The boys were better off, being at school each day; but they could not help being frightened by the gossip. Thaddeus, an especially sensitive youth, suffered the most.\\n\\nIn May the insects came, and Nahum\\'s place became a nightmare of buzzing and crawling. Most of the creatures seemed not quite usual in their aspects and motions, and their nocturnal habits contradicted all former experience. The Gardners took to watching at night--watching in all directions at random for something--they could not tell what. It was then that they owned that Thaddeus had been right about the trees. Mrs. Gardner was the next to see it from the window as she watched the swollen boughs of a maple against a moonlit sky. The boughs surely moved, and there was no wind. It must be the sap. Strangeness had come into everything growing now. Yet it was none of Nahum\\'s family at all who made the next discovery. Familiarity had dulled them, and what they could not see was glimpsed by a timid windmill salesman from Bolton who drove by one night in ignorance of the country legends. What he told in Arkham was given a short paragraph in the Gazette; and it was there that all the farmers, Nahum included, saw it first. The night had been dark and the buggy-lamps faint, but around a farm in the valley which everyone knew from the account must be Nahum\\'s, the darkness had been less thick. A dim though distinct luminosity seemed to inhere in all the vegetation, grass, leaves, and blossoms alike, while at one moment a detached piece of the phosphorescence appeared to stir furtively in the yard near the barn.\\n\\nThe grass had so far seemed untouched, and the cows were freely pastured in the lot near the house, but toward the end of May the milk began to be bad. Then Nahum had the cows driven to the uplands, after which this trouble ceased. Not long after this the change in grass and leaves became apparent to the eye. All the verdure was going grey, and was developing a highly singular quality of brittleness. Ammi was now the only person who ever visited the place, and his visits were becoming fewer and fewer. When school closed the Gardners were virtually cut off from the world, and sometimes let Ammi do their errands in town. They were failing curiously both physically and mentally, and no one was surprised when the news of Mrs. Gardner\\'s madness stole around.\\n\\nIt happened in June, about the anniversary of the meteor\\'s fall, and the poor woman screamed about things in the air which she could not describe. In her raving there was not a single specific noun, but only verbs and pronouns. Things moved and changed and fluttered, and ears tingled to impulses which were not wholly sounds. Something was taken away--she was being drained of something--something was fastening itself on her that ought not to be--someone must make it keep off--nothing was ever still in the night--the walls and windows shifted. Nahum did not send her to the county asylum, but let her wander about the house as long as she was harmless to herself and others. Even when her expression changed he did nothing. But when the boys grew afraid of her, and Thaddeus nearly fainted at the way she made faces at him, he decided to keep her locked in the attic. By July she had ceased to speak and crawled on all fours, and before that month was over Nahum got the mad notion that she was slightly luminous in the dark, as he now clearly saw was the case with the nearby vegetation.\\n\\nIt was a little before this that the horses had stampeded. Something had aroused them in the night, and their neighing and kicking in their stalls had been terrible. There seemed virtually nothing to do to calm them, and when Nahum opened the stable door they all bolted out like frightened woodland deer. It took a week to track all four, and when found they were seen to be quite useless and unmanageable. Something had snapped in their brains, and each one had to be shot for its own good. Nahum borrowed a horse from Ammi for his haying, but found it would not approach the barn. It shied, balked, and whinnied, and in the end he could do nothing but drive it into the yard while the men used their own strength to get the heavy wagon near enough the hayloft for convenient pitching. And all the while the vegetation was turning grey and brittle. Even the flowers whose hues had been so strange were greying now, and the fruit was coming out grey and dwarfed and tasteless. The asters and golden-rod bloomed grey and distorted, and the roses and zinneas and hollyhocks in the front yard were such blasphemous-looking things that Nahum\\'s oldest boy Zenas cut them down. The strangely puffed insects died about that time, even the bees that had left their hives and taken to the woods.\\n\\nBy September all the vegetation was fast crumbling to a greyish powder, and Nahum feared that the trees would die before the poison was out of the soil. His wife now had spells of terrific screaming, and he and the boys were in a constant state of nervous tension. They shunned people now, and when school opened the boys did not go. But it was Ammi, on one of his rare visits, who first realised that the well water was no longer good. It had an evil taste that was not exactly fetid nor exactly salty, and Ammi advised his friend to dig another well on higher ground to use till the soil was good again. Nahum, however, ignored the warning, for he had by that time become calloused to strange and unpleasant things. He and the boys continued to use the tainted supply, drinking it as listlessly and mechanically as they ate their meagre and ill-cooked meals and did their thankless and monotonous chores through the aimless days. There was something of stolid resignation about them all, as if they walked half in another world between lines of nameless guards to a certain and familiar doom.\\n\\nThaddeus went mad in September after a visit to the well. He had gone with a pail and had come back empty-handed, shrieking and waving his arms, and sometimes lapsing into an inane titter or a whisper about \"the moving colours down there.\" Two in one family was pretty bad, but Nahum was very brave about it. He let the boy run about for a week until he began stumbling and hurting himself, and then he shut him in an attic room across the hall from his mother\\'s. The way they screamed at each other from behind their locked doors was very terrible, especially to little Merwin, who fancied they talked in some terrible language that was not of earth. Merwin was getting frightfully imaginative, and his restlessness was worse after the shutting away of the brother who had been his greatest playmate.\\n\\nAlmost at the same time the mortality among the livestock commenced. Poultry turned greyish and died very quickly, their meat being found dry and noisome upon cutting. Hogs grew inordinately fat, then suddenly began to undergo loathsome changes which no one could explain. Their meat was of course useless, and Nahum was at his wit\\'s end. No rural veterinary would approach his place, and the city veterinary from Arkham was openly baffled. The swine began growing grey and brittle and falling to pieces before they died, and their eyes and muzzles developed singular alterations. It was very inexplicable, for they had never been fed from the tainted vegetation. Then something struck the cows. Certain areas or sometimes the whole body would be uncannily shrivelled or compressed, and atrocious collapses or disintegrations were common. In the last stages--and death was always the result--there would be a greying and turning brittle like that which beset the hogs. There could be no question of poison, for all the cases occurred in a locked and undisturbed barn. No bites of prowling things could have brought the virus, for what live beast of earth can pass through solid obstacles? It must be only natural disease--yet what disease could wreak such results was beyond any mind\\'s guessing. When the harvest came there was not an animal surviving on the place, for the stock and poultry were dead and the dogs had run away. These dogs, three in number, had all vanished one night and were never heard of again. The five cats had left some time before, but their going was scarcely noticed since there now seemed to be no mice, and only Mrs. Gardner had made pets of the graceful felines.\\n\\nOn the nineteenth of October Nahum staggered into Ammi\\'s house with hideous news. The death had come to poor Thaddeus in his attic room, and it had come in a way which could not be told. Nahum had dug a grave in the railed family plot behind the farm, and had put therein what he found. There could have been nothing from outside, for the small barred window and locked door were intact; but it was much as it had been in the barn. Ammi and his wife consoled the stricken man as best they could, but shuddered as they did so. Stark terror seemed to cling round the Gardners and all they touched, and the very presence of one in the house was a breath from regions unnamed and unnamable. Ammi accompanied Nahum home with the greatest reluctance, and did what he might to calm the hysterical sobbing of little Merwin. Zenas needed no calming. He had come of late to do nothing but stare into space and obey what his father told him; and Ammi thought that his fate was very merciful. Now and then Merwin\\'s screams were answered faintly from the attic, and in response to an inquiring look Nahum said that his wife was getting very feeble. When night approached, Ammi managed to get away; for not even friendship could make him stay in that spot when the faint glow of the vegetation began and the trees may or may not have swayed without wind. It was really lucky for Ammi that he was not more imaginative. Even as things were, his mind was bent ever so slightly; but had he been able to connect and reflect upon all the portents around him he must inevitably have turned a total maniac. In the twilight he hastened home, the screams of the mad woman and the nervous child ringing horribly in his ears.\\n\\nThree days later Nahum burst into Ammi\\'s kitchen in the early morning, and in the absence of his host stammered out a desperate tale once more, while Mrs. Pierce listened in a clutching fright. It was little Merwin this time. He was gone. He had gone out late at night with a lantern and pail for water, and had never come back. He\\'d been going to pieces for days, and hardly knew what he was about. Screamed at everything. There had been a frantic shriek from the yard then, but before the father could get to the door the boy was gone. There was no glow from the lantern he had taken, and of the child himself no trace. At the time Nahum thought the lantern and pail were gone too; but when dawn came, and the man had plodded back from his all-night search of the woods and fields, he had found some very curious things near the well. There was a crushed and apparently somewhat melted mass of iron which had certainly been the lantern; while a bent handle and twisted iron hoops beside it, both half-fused, seemed to hint at the remnants of the pail. That was all. Nahum was past imagining, Mrs. Pierce was blank, and Ammi, when he had reached home and heard the tale, could give no guess. Merwin was gone, and there would be no use in telling the people around, who shunned all Gardners now. No use, either, in telling the city people at Arkham who laughed at everything. Thad was gone, and now Merwin was gone. Something was creeping and creeping and waiting to be seen and heard. Nahum would go soon, and he wanted Ammi to look after his wife and Zenas if they survived him. It must all be a judgment of some sort; though he could not fancy what for, since he had always walked uprightly in the Lord\\'s ways so far as he knew.\\n\\nFor over two weeks Ammi saw nothing of Nahum; and then, worried about what might have happened, he overcame his fears and paid the Gardner place a visit. There was no smoke from the great chimney, and for a moment the visitor was apprehensive of the worst. The aspect of the whole farm was shocking--greyish withered grass and leaves on the ground, vines falling in brittle wreckage from archaic walls and gables, and great bare trees clawing up at the grey November sky with a studied malevolence which Ammi could not but feel had come from some subtle change in the tilt of the branches. But Nahum was alive, after all. He was weak, and lying on a couch in the low-ceiled kitchen, but perfectly conscious and able to give simple orders to Zenas. The room was deadly cold; and as Ammi visibly shivered, the host shouted huskily to Zenas for more wood. Wood, indeed, was sorely needed; since the cavernous fireplace was unlit and empty, with a cloud of soot blowing about in the chill wind that came down the chimney. Presently Nahum asked him if the extra wood had made him any more comfortable, and then Ammi saw what had happened. The stoutest cord had broken at last, and the hapless farmer\\'s mind was proof against more sorrow.\\n\\nQuestioning tactfully, Ammi could get no clear data at all about the missing Zenas. \"In the well--he lives in the well--\" was all that the clouded father would say. Then there flashed across the visitor\\'s mind a sudden thought of the mad wife, and he changed his line of inquiry. \"Nabby? Why, here she is!\" was the surprised response of poor Nahum, and Ammi soon saw that he must search for himself. Leaving the harmless babbler on the couch, he took the keys from their nail beside the door and climbed the creaking stairs to the attic. It was very close and noisome up there, and no sound could be heard from any direction. Of the four doors in sight, only one was locked, and on this he tried various keys of the ring he had taken. The third key proved the right one, and after some fumbling Ammi threw open the low white door.\\n\\nIt was quite dark inside, for the window was small and half-obscured by the crude wooden bars; and Ammi could see nothing at all on the wide-planked floor. The stench was beyond enduring, and before proceeding further he had to retreat to another room and return with his lungs filled with breathable air. When he did enter he saw something dark in the corner, and upon seeing it more clearly he screamed outright. While he screamed he thought a momentary cloud eclipsed the window, and a second later he felt himself brushed as if by some hateful current of vapour. Strange colours danced before his eyes; and had not a present horror numbed him he would have thought of the globule in the meteor that the geologist\\'s hammer had shattered, and of the morbid vegetation that had sprouted in the spring. As it was he thought only of the blasphemous monstrosity which confronted him, and which all too clearly had shared the nameless fate of young Thaddeus and the livestock. But the terrible thing about the horror was that it very slowly and perceptibly moved as it continued to crumble.\\n\\nAmmi would give me no added particulars of this scene, but the shape in the comer does not reappear in his tale as a moving object. There are things which cannot be mentioned, and what is done in common humanity is sometimes cruelly judged by the law. I gathered that no moving thing was left in that attic room, and that to leave anything capable of motion there would have been a deed so monstrous as to damn any accountable being to eternal torment. Anyone but a stolid farmer would have fainted or gone mad, but Ammi walked conscious through that low doorway and locked the accursed secret behind him. There would be Nahum to deal with now; he must be fed and tended, and removed to some place where he could be cared for.\\n\\nCommencing his descent of the dark stairs. Ammi heard a thud below him. He even thought a scream had been suddenly choked off, and recalled nervously the clammy vapour which had brushed by him in that frightful room above. What presence had his cry and entry started up? Halted by some vague fear, he heard still further sounds below. Indubitably there was a sort of heavy dragging, and a most detestably sticky noise as of some fiendish and unclean species of suction. With an associative sense goaded to feverish heights, he thought unaccountably of what he had seen upstairs. Good God! What eldritch dream-world was this into which he had blundered? He dared move neither backward nor forward, but stood there trembling at the black curve of the boxed-in staircase. Every trifle of the scene burned itself into his brain. The sounds, the sense of dread expectancy, the darkness, the steepness of the narrow step--and merciful Heaven!--the faint but unmistakable luminosity of all the woodwork in sight; steps, sides, exposed laths, and beams alike.\\n\\nThen there burst forth a frantic whinny from Ammi\\'s horse outside, followed at once by a clatter which told of a frenzied runaway. In another moment horse and buggy had gone beyond earshot, leaving the frightened man on the dark stairs to guess what had sent them. But that was not all. There had been another sound out there. A sort of liquid splash--water--it must have been the well. He had left Hero untied near it, and a buggy wheel must have brushed the coping and knocked in a stone. And still the pale phosphorescence glowed in that detestably ancient woodwork. God! how old the house was! Most of it built before 1670, and the gambrel roof no later than 1730.\\n\\nA feeble scratching on the floor downstairs now sounded distinctly, and Ammi\\'s grip tightened on a heavy stick he had picked up in the attic for some purpose. Slowly nerving himself, he finished his descent and walked boldly toward the kitchen. But he did not complete the walk, because what he sought was no longer there. It had come to meet him, and it was still alive after a fashion. Whether it had crawled or whether it had been dragged by any external forces, Ammi could not say; but the death had been at it. Everything had happened in the last half-hour, but collapse, greying, and disintegration were already far advanced. There was a horrible brittleness, and dry fragments were scaling off. Ammi could not touch it, but looked horrifiedly into the distorted parody that had been a face. \"What was it, Nahum--what was it?\" He whispered, and the cleft, bulging lips were just able to crackle out a final answer.\\n\\n\"Nothin\\'...nothin\\'...the colour...it burns...cold an\\' wet, but it burns...it lived in the well...I seen it...a kind of smoke...jest like the flowers last spring...the well shone at night...Thad an\\' Merwin an\\' Zenas...everything alive...suckin\\' the life out of everything...in that stone...it must a\\' come in that stone pizened the whole place...dun\\'t know what it wants...that round thing them men from the college dug outen the stone...they smashed it...it was the same colour...jest the same, like the flowers an\\' plants...must a\\' ben more of \\'em...seeds...seeds...they growed...I seen it the fust time this week...must a\\' got strong on Zenas...he was a big boy, full o\\' life...it beats down your mind an\\' then gets ye...burns ye up...in the well water...you was right about that...evil water...Zenas never come back from the well...can\\'t git away...draws ye...ye know summ\\'at\\'s comin\\' but tain\\'t no use...I seen it time an\\' agin senct Zenas was took...whar\\'s Nabby, Ammi?...my head\\'s no good...dun\\'t know how long sense I fed her...it\\'ll git her ef we ain\\'t keerful...jest a colour...her face is gittin\\' to hev that colour sometimes towards night...an\\' it burns an\\' sucks...it come from some place whar things ain\\'t as they is here...one o\\' them professors said so...he was right...look out, Ammi, it\\'ll do suthin\\' more...sucks the life out...\"\\n\\nBut that was all. That which spoke could speak no more because it had completely caved in. Ammi laid a red checked tablecloth over what was left and reeled out the back door into the fields. He climbed the slope to the ten-acre pasture and stumbled home by the north road and the woods. He could not pass that well from which his horses had run away. He had looked at it through the window, and had seen that no stone was missing from the rim. Then the lurching buggy had not dislodged anything after all--the splash had been something else--something which went into the well after it had done with poor Nahum.\\n\\nWhen Ammi reached his house the horses and buggy had arrived before him and thrown his wife into fits of anxiety. Reassuring her without explanations, he set out at once for Arkham and notified the authorities that the Gardner family was no more. He indulged in no details, but merely told of the deaths of Nahum and Nabby, that of Thaddeus being already known, and mentioned that the cause seemed to be the same strange ailment which had killed the live-stock. He also stated that Merwin and Zenas had disappeared. There was considerable questioning at the police station, and in the end Ammi was compelled to take three officers to the Gardner farm, together with the coroner, the medical examiner, and the veterinary who had treated the diseased animals. He went much against his will, for the afternoon was advancing and he feared the fall of night over that accursed place, but it was some comfort to have so many people with him.\\n\\nThe six men drove out in a democrat-wagon, following Ammi\\'s buggy, and arrived at the pest-ridden farmhouse about four o\\'clock. Used as the officers were to gruesome experiences, not one remained unmoved at what was found in the attic and under the red checked tablecloth on the floor below. The whole aspect of the farm with its grey desolation was terrible enough, but those two crumbling objects were beyond all bounds. No one could look long at them, and even the medical examiner admitted that there was very little to examine. Specimens could be analysed, of course, so he busied himself in obtaining them--and here it develops that a very puzzling aftermath occurred at the college laboratory where the two phials of dust were finally taken. Under the spectroscope both samples gave off an unknown spectrum, in which many of the baffling bands were precisely like those which the strange meteor had yielded in the previous year. The property of emitting this spectrum vanished in a month, the dust thereafter consisting mainly of alkaline phosphates and carbonates.\\n\\nAmmi would not have told the men about the well if he had thought they meant to do anything then and there. It was getting toward sunset, and he was anxious to be away. But he could not help glancing nervously at the stony curb by the great sweep, and when a detective questioned him he admitted that Nahum had feared something down there so much so that he had never even thought of searching it for Merwin or Zenas. After that nothing would do but that they empty and explore the well immediately, so Ammi had to wait trembling while pail after pail of rank water was hauled up and splashed on the soaking ground outside. The men sniffed in disgust at the fluid, and toward the last held their noses against the foetor they were uncovering. It was not so long a job as they had feared it would be, since the water was phenomenally low. There is no need to speak too exactly of what they found. Merwin and Zenas were both there, in part, though the vestiges were mainly skeletal. There were also a small deer and a large dog in about the same state, and a number of bones of small animals. The ooze and slime at the bottom seemed inexplicably porous and bubbling, and a man who descended on hand-holds with a long pole found that he could sink the wooden shaft to any depth in the mud of the floor without meeting any solid obstruction.\\n\\nTwilight had now fallen, and lanterns were brought from the house. Then, when it was seen that nothing further could be gained from the well, everyone went indoors and conferred in the ancient sitting-room while the intermittent light of a spectral half-moon played wanly on the grey desolation outside. The men were frankly nonplussed by the entire case, and could find no convincing common element to link the strange vegetable conditions, the unknown disease of live-stock and humans, and the unaccountable deaths of Merwin and Zenas in the tainted well. They had heard the common country talk, it is true; but could not believe that anything contrary to natural law had occurred. No doubt the meteor had poisoned the soil, but the illness of persons and animals who had eaten nothing grown in that soil was another matter. Was it the well water? Very possibly. It might be a good idea to analyze it. But what peculiar madness could have made both boys jump into the well? Their deeds were so similar-and the fragments showed that they had both suffered from the grey brittle death. Why was everything so grey and brittle?\\n\\nIt was the coroner, seated near a window overlooking the yard, who first noticed the glow about the well. Night had fully set in, and all the abhorrent grounds seemed faintly luminous with more than the fitful moonbeams; but this new glow was something definite and distinct, and appeared to shoot up from the black pit like a softened ray from a searchlight, giving dull reflections in the little ground pools where the water had been emptied. It had a very queer colour, and as all the men clustered round the window Ammi gave a violent start. For this strange beam of ghastly miasma was to him of no unfamiliar hue. He had seen that colour before, and feared to think what it might mean. He had seen it in the nasty brittle globule in that aerolite two summers ago, had seen it in the crazy vegetation of the springtime, and had thought he had seen it for an instant that very morning against the small barred window of that terrible attic room where nameless things had happened. It had flashed there a second, and a clammy and hateful current of vapour had brushed past him--and then poor Nahum had been taken by something of that colour. He had said so at the last--said it was like the globule and the plants. After that had come the runaway in the yard and the splash in the well and now that well was belching forth to the night a pale insidious beam of the same demoniac tint.\\n\\nIt does credit to the alertness of Ammi\\'s mind that he puzzled even at that tense moment over a point which was essentially scientific. He could not but wonder at his gleaning of the same impression from a vapour glimpsed in the daytime, against a window opening on the morning sky, and from a nocturnal exhalation seen as a phosphorescent mist against the black and blasted landscape. It wasn\\'t right--it was against Nature--and he thought of those terrible last words of his stricken friend, \"It come from some place whar things ain\\'t as they is here...one o\\' them professors said so...\"\\n\\nAll three horses outside, tied to a pair of shrivelled saplings by the road, were now neighing and pawing frantically. The wagon driver started for the door to do something, but Ammi laid a shaky hand on his shoulder. \"Dun\\'t go out thar,\" he whispered. \"They\\'s more to this nor what we know. Nahum said somethin\\' lived in the well that sucks your life out. He said it must be some\\'at growed from a round ball like one we all seen in the meteor stone that fell a year ago June. Sucks an\\' burns, he said, an\\' is jest a cloud of colour like that light out thar now, that ye can hardly see an\\' can\\'t tell what it is. Nahum thought it feeds on everything livin\\' an\\' gits stronger all the time. He said he seen it this last week. It must be somethin\\' from away off in the sky like the men from the college last year says the meteor stone was. The way it\\'s made an\\' the way it works ain\\'t like no way o\\' God\\'s world. It\\'s some\\'at from beyond.\"\\n\\nSo the men paused indecisively as the light from the well grew stronger and the hitched horses pawed and whinnied in increasing frenzy. It was truly an awful moment; with terror in that ancient and accursed house itself, four monstrous sets of fragments--two from the house and two from the well--in the woodshed behind, and that shaft of unknown and unholy iridescence from the slimy depths in front. Ammi had restrained the driver on impulse, forgetting how uninjured he himself was after the clammy brushing of that coloured vapour in the attic room, but perhaps it is just as well that he acted as he did. No one will ever know what was abroad that night; and though the blasphemy from beyond had not so far hurt any human of unweakened mind, there is no telling what it might not have done at that last moment, and with its seemingly increased strength and the special signs of purpose it was soon to display beneath the half-clouded moonlit sky.\\n\\nAll at once one of the detectives at the window gave a short, sharp gasp. The others looked at him, and then quickly followed his own gaze upward to the point at which its idle straying had been suddenly arrested. There was no need for words. What had been disputed in country gossip was disputable no longer, and it is because of the thing which every man of that party agreed in whispering later on, that the strange days are never talked about in Arkham. It is necessary to premise that there was no wind at that hour of the evening. One did arise not long afterward, but there was absolutely none then. Even the dry tips of the lingering hedge-mustard, grey and blighted, and the fringe on the roof of the standing democrat-wagon were unstirred. And yet amid that tense godless calm the high bare boughs of all the trees in the yard were moving. They were twitching morbidly and spasmodically, clawing in convulsive and epileptic madness at the moonlit clouds; scratching impotently in the noxious air as if jerked by some allied and bodiless line of linkage with subterrene horrors writhing and struggling below the black roots.\\n\\nNot a man breathed for several seconds. Then a cloud of darker depth passed over the moon, and the silhouette of clutching branches faded out momentarily. At this there was a general cry; muffled with awe, but husky and almost identical from every throat. For the terror had not faded with the silhouette, and in a fearsome instant of deeper darkness the watchers saw wriggling at that tree top height a thousand tiny points of faint and unhallowed radiance, tipping each bough like the fire of St. Elmo or the flames that come down on the apostles\\' heads at Pentecost. It was a monstrous constellation of unnatural light, like a glutted swarm of corpse-fed fireflies dancing hellish sarabands over an accursed marsh, and its colour was that same nameless intrusion which Ammi had come to recognize and dread. All the while the shaft of phosphorescence from the well was getting brighter and brighter, bringing to the minds of the huddled men, a sense of doom and abnormality which far outraced any image their conscious minds could form. It was no longer shining out; it was pouring out; and as the shapeless stream of unplaceable colour left the well it seemed to flow directly into the sky.\\n\\nThe veterinary shivered, and walked to the front door to drop the heavy extra bar across it. Ammi shook no less, and had to tug and point for lack of controllable voice when he wished to draw notice to the growing luminosity of the trees. The neighing and stamping of the horses had become utterly frightful, but not a soul of that group in the old house would have ventured forth for any earthly reward. With the moments the shining of the trees increased, while their restless branches seemed to strain more and more toward verticality. The wood of the well-sweep was shining now, and presently a policeman dumbly pointed to some wooden sheds and bee-hives near the stone wall on the west. They were commencing to shine, too, though the tethered vehicles of the visitors seemed so far unaffected. Then there was a wild commotion and clopping in the road, and as Ammi quenched the lamp for better seeing they realized that the span of frantic greys had broken their sapling and run off with the democrat-wagon.\\n\\nThe shock served to loosen several tongues, and embarrassed whispers were exchanged. \"It spreads on everything organic that\\'s been around here,\" muttered the medical examiner. No one replied, but the man who had been in the well gave a hint that his long pole must have stirred up something intangible. \"It was awful,\" he added. \"There was no bottom at all. Just ooze and bubbles and the feeling of something lurking under there.\" Ammi\\'s horse still pawed and screamed deafeningly in the road outside, and nearly drowned its owner\\'s faint quaver as he mumbled his formless reflections. \"It come from that stone--it growed down thar--it got everything livin\\'--it fed itself on \\'em, mind and body--Thad an\\' Merwin, Zenas an\\' Nabby--Nahum was the last--they all drunk the water--it got strong on \\'em--it come from beyond, whar things ain\\'t like they be here--now it\\'s goin\\' home--\"\\n\\nAt this point, as the column of unknown colour flared suddenly stronger and began to weave itself into fantastic suggestions of shape which each spectator described differently, there came from poor tethered Hero such a sound as no man before or since ever heard from a horse. Every person in that low-pitched sitting room stopped his ears, and Ammi turned away from the window in horror and nausea. Words could not convey it--when Ammi looked out again the hapless beast lay huddled inert on the moonlit ground between the splintered shafts of the buggy. That was the last of Hero till they buried him next day. But the present was no time to mourn, for almost at this instant a detective silently called attention to something terrible in the very room with them. In the absence of the lamplight it was clear that a faint phosphorescence had begun to pervade the entire apartment. It glowed on the broad-planked floor and the fragment of rag carpet, and shimmered over the sashes of the small-paned windows. It ran up and down the exposed corner-posts, coruscated about the shelf and mantel, and infected the very doors and furniture. Each minute saw it strengthen, and at last it was very plain that healthy living things must leave that house.\\n\\nAmmi showed them the back door and the path up through the fields to the ten-acre pasture. They walked and stumbled as in a dream, and did not dare look back till they were far away on the high ground. They were glad of the path, for they could not have gone the front way, by that well. It was bad enough passing the glowing barn and sheds, and those shining orchard trees with their gnarled, fiendish contours; but thank Heaven the branches did their worst twisting high up. The moon went under some very black clouds as they crossed the rustic bridge over Chapman\\'s Brook, and it was blind groping from there to the open meadows.\\n\\nWhen they looked back toward the valley and the distant Gardner place at the bottom they saw a fearsome sight. At the farm was shining with the hideous unknown blend of colour; trees, buildings, and even such grass and herbage as had not been wholly changed to lethal grey brittleness. The boughs were all straining skyward, tipped with tongues of foul flame, and lambent tricklings of the same monstrous fire were creeping about the ridgepoles of the house, barn and sheds. It was a scene from a vision of Fuseli, and over all the rest reigned that riot of luminous amorphousness, that alien and undimensioned rainbow of cryptic poison from the well--seething, feeling, lapping, reaching, scintillating, straining, and malignly bubbling in its cosmic and unrecognizable chromaticism.\\n\\nThen without warning the hideous thing shot vertically up toward the sky like a rocket or meteor, leaving behind no trail and disappearing through a round and curiously regular hole in the clouds before any man could gasp or cry out. No watcher can ever forget that sight, and Ammi stared blankly at the stars of Cygnus, Deneb twinkling above the others, where the unknown colour had melted into the Milky Way. But his gaze was the next moment called swiftly to earth by the crackling in the valley. It was just that. Only a wooden ripping and crackling, and not an explosion, as so many others of the party vowed. Yet the outcome was the same, for in one feverish kaleidoscopic instant there burst up from that doomed and accursed farm a gleamingly eruptive cataclysm of unnatural sparks and substance; blurring the glance of the few who saw it, and sending forth to the zenith a bombarding cloudburst of such coloured and fantastic fragments as our universe must needs disown. Through quickly reclosing vapours they followed the great morbidity that had vanished, and in another second they had vanished too. Behind and below was only a darkness to which the men dared not return, and all about was a mounting wind which seemed to sweep down in black, frore gusts from interstellar space. It shrieked and howled, and lashed the fields and distorted woods in a mad cosmic frenzy, till soon the trembling party realized it would be no use waiting for the moon to show what was left down there at Nahum\\'s.\\n\\nToo awed even to hint theories, the seven shaking men trudged back toward Arkham by the north road. Ammi was worse than his fellows, and begged them to see him inside his own kitchen, instead of keeping straight on to town. He did not wish to cross the blighted, wind-whipped woods alone to his home on the main road. For he had had an added shock that the others were spared, and was crushed forever with a brooding fear he dared not even mention for many years to come. As the rest of the watchers on that tempestuous hill had stolidly set their faces toward the road, Ammi had looked back an instant at the shadowed valley of desolation so lately sheltering his ill-starred friend. And from that stricken, far-away spot he had seen something feebly rise, only to sink down again upon the place from which the great shapeless horror had shot into the sky. It was just a colour--but not any colour of our earth or heavens. And because Ammi recognized that colour, and knew that this last faint remnant must still lurk down there in the well, he has never been quite right since.\\n\\nAmmi would never go near the place again. It is forty-four years now since the horror happened, but he has never been there, and will be glad when the new reservoir blots it out. I shall be glad, too, for I do not like the way the sunlight changed colour around the mouth of that abandoned well I passed. I hope the water will always be very deep--but even so, I shall never drink it. I do not think I shall visit the Arkham country hereafter. Three of the men who had been with Ammi returned the next morning to see the ruins by daylight, but there were not any real ruins. Only the bricks of the chimney, the stones of the cellar, some mineral and metallic litter here and there, and the rim of that nefandous well. Save for Ammi\\'s dead horse, which they towed away and buried, and the buggy which they shortly returned to him, everything that had ever been living had gone. Five eldritch acres of dusty grey desert remained, nor has anything ever grown there since. To this day it sprawls open to the sky like a great spot eaten by acid in the woods and fields, and the few who have ever dared glimpse it in spite of the rural tales have named it \"the blasted heath.\"\\n\\nThe rural tales are queer. They might be even queerer if city men and college chemists could be interested enough to analyze the water from that disused well, or the grey dust that no wind seems to disperse. Botanists, too, ought to study the stunted flora on the borders of that spot, for they might shed light on the country notion that the blight is spreading--little by little, perhaps an inch a year. People say the colour of the neighboring herbage is not quite right in the spring, and that wild things leave queer prints in the light winter snow. Snow never seems quite so heavy on the blasted heath as it is elsewhere. Horses--the few that are left in this motor age--grow skittish in the silent valley; and hunters cannot depend on their dogs too near the splotch of greyish dust.\\n\\nThey say the mental influences are very bad, too; numbers went queer in the years after Nahum\\'s taking, and always they lacked the power to get away. Then the stronger-minded folk all left the region, and only the foreigners tried to live in the crumbling old homesteads. They could not stay, though; and one sometimes wonders what insight beyond ours their wild, weird stories of whispered magic have given them. Their dreams at night, they protest, are very horrible in that grotesque country; and surely the very look of the dark realm is enough to stir a morbid fancy. No traveler has ever escaped a sense of strangeness in those deep ravines, and artists shiver as they paint thick woods whose mystery is as much of the spirits as of the eye. I myself am curious about the sensation I derived from my one lone walk before Ammi told me his tale. When twilight came I had vaguely wished some clouds would gather, for an odd timidity about the deep skyey voids above had crept into my soul.\\n\\nDo not ask me for my opinion. I do not know--that is all. There was no one but Ammi to question; for Arkham people will not talk about the strange days, and all three professors who saw the aerolite and its coloured globule are dead. There were other globules--depend upon that. One must have fed itself and escaped, and probably there was another which was too late. No doubt it is still down the well--I know there was something wrong with the sunlight I saw above the miasmal brink. The rustics say the blight creeps an inch a year, so perhaps there is a kind of growth or nourishment even now. But whatever demon hatchling is there, it must be tethered to something or else it would quickly spread. Is it fastened to the roots of those trees that claw the air? One of the current Arkham tales is about fat oaks that shine and move as they ought not to do at night.\\n\\nWhat it is, only God knows. In terms of matter I suppose the thing Ammi described would be called a gas, but this gas obeyed the laws that are not of our cosmos. This was no fruit of such worlds and suns as shine on the telescopes and photographic plates of our observatories. This was no breath from the skies whose motions and dimensions our astronomers measure or deem too vast to measure. It was just a colour out of space--a frightful messenger from unformed realms of infinity beyond all Nature as we know it; from realms whose mere existence stuns the brain and numbs us with the black cosmic gulfs it throws open before our frenzied eyes.\\n\\nI doubt very much if Ammi consciously lied to me, and I do not think his tale was all a freak of madness as the townsfolk had forewarned. Something terrible came to the hills and valleys on that meteor, and something terrible--though I know not in what proportion--still remains. I shall be glad to see the water come. Meanwhile I hope nothing will happen to Ammi. He saw so much of the thing--and its influence was so insidious. Why has he never been able to move away? How clearly he recalled those dying words of Nahum\\'s--\"Can\\'t git away--draws ye--ye know summ\\'at\\'s comin\\' but tain\\'t no use--\". Ammi is such a good old man--when the reservoir gang gets to work I must write the chief engineer to keep a sharp watch on him. I would hate to think of him as the grey, twisted, brittle monstrosity which persists more and more in troubling my sleep.\\n\\nTHE CALL OF CTHULHU\\n\\nOf such great powers or beings there may be conceivably a survival...a survival of a hugely remote period when...consciousness was manifested, perhaps, in shapes and forms long since withdrawn before the tide of advancing humanity...forms of which poetry and legend alone have caught a flying memory and called them gods, monsters, mythical beings of all sorts and kinds...\\n\\n--Algernon Blackwood\\n\\nI. The Horror In Clay\\n\\nThe most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the light into the peace and safety of a new dark age.\\n\\nTheosophists have guessed at the awesome grandeur of the cosmic cycle wherein our world and human race form transient incidents. They have hinted at strange survivals in terms which would freeze the blood if not masked by a bland optimism. But it is not from them that there came the single glimpse of forbidden eons which chills me when I think of it and maddens me when I dream of it. That glimpse, like all dread glimpses of truth, flashed out from an accidental piecing together of separated things--in this case an old newspaper item and the notes of a dead professor. I hope that no one else will accomplish this piecing out; certainly, if I live, I shall never knowingly supply a link in so hideous a chain. I think that the professor, too, intended to keep silent regarding the part he knew, and that he would have destroyed his notes had not sudden death seized him.\\n\\nMy knowledge of the thing began in the winter of 1926-27 with the death of my great-uncle, George Gammell Angell, Professor Emeritus of Semitic Languages in Brown University, Providence, Rhode Island. Professor Angell was widely known as an authority on ancient inscriptions, and had frequently been resorted to by the heads of prominent museums; so that his passing at the age of ninety-two may be recalled by many. Locally, interest was intensified by the obscurity of the cause of death. The professor had been stricken whilst returning from the Newport boat; falling suddenly; as witnesses said, after having been jostled by a nautical-looking negro who had come from one of the queer dark courts on the precipitous hillside which formed a short cut from the waterfront to the deceased\\'s home in Williams Street. Physicians were unable to find any visible disorder, but concluded after perplexed debate that some obscure lesion of the heart, induced by the brisk ascent of so steep a hill by so elderly a man, was responsible for the end. At the time I saw no reason to dissent from this dictum, but latterly I am inclined to wonder--and more than wonder.\\n\\nAs my great-uncle\\'s heir and executor, for he died a childless widower, I was expected to go over his papers with some thoroughness; and for that purpose moved his entire set of files and boxes to my quarters in Boston. Much of the material which I correlated will be later published by the American Archaeological Society, but there was one box which I found exceedingly puzzling, and which I felt much averse from showing to other eyes. It had been locked and I did not find the key till it occurred to me to examine the personal ring which the professor carried in his pocket. Then, indeed, I succeeded in opening it, but when I did so seemed only to be confronted by a greater and more closely locked barrier. For what could be the meaning of the queer clay bas-relief and the disjointed jottings, ramblings, and cuttings which I found? Had my uncle, in his latter years become credulous of the most superficial impostures? I resolved to search out the eccentric sculptor responsible for this apparent disturbance of an old man\\'s peace of mind.\\n\\nThe bas-relief was a rough rectangle less than an inch thick and about five by six inches in area; obviously of modern origin. Its designs, however, were far from modern in atmosphere and suggestion; for, although the vagaries of cubism and futurism are many and wild, they do not often reproduce that cryptic regularity which lurks in prehistoric writing. And writing of some kind the bulk of these designs seemed certainly to be; though my memory, despite much the papers and collections of my uncle, failed in any way to identify this particular species, or even hint at its remotest affiliations.\\n\\nAbove these apparent hieroglyphics was a figure of evident pictorial intent, though its impressionistic execution forbade a very clear idea of its nature. It seemed to be a sort of monster, or symbol representing a monster, of a form which only a diseased fancy could conceive. If I say that my somewhat extravagant imagination yielded simultaneous pictures of an octopus, a dragon, and a human caricature, I shall not be unfaithful to the spirit of the thing. A pulpy, tentacled head surmounted a grotesque and scaly body with rudimentary wings; but it was the general outline of the whole which made it most shockingly frightful. Behind the figure was a vague suggestions of a Cyclopean architectural background.\\n\\nThe writing accompanying this oddity was, aside from a stack of press cuttings, in Professor Angell\\'s most recent hand; and made no pretense to literary style. What seemed to be the main document was headed \"CTHULHU CULT\" in characters painstakingly printed to avoid the erroneous reading of a word so unheard-of. This manuscript was divided into two sections, the first of which was headed \"1925--Dream and Dream Work of H.A. Wilcox, 7 Thomas St., Providence, R. I.\", and the second, \"Narrative of Inspector John R. Legrasse, 121 Bienville St., New Orleans, La., at 1908 A. A. S. Mtg.--Notes on Same, & Prof. Webb\\'s Acct.\" The other manuscript papers were brief notes, some of them accounts of the queer dreams of different persons, some of them citations from theosophical books and magazines (notably W. Scott-Elliot\\'s Atlantis and the Lost Lemuria), and the rest comments on long-surviving secret societies and hidden cults, with references to passages in such mythological and anthropological source-books as Frazer\\'s Golden Bough and Miss Murray\\'s Witch-Cult in Western Europe. The cuttings largely alluded to outr mental illness and outbreaks of group folly or mania in the spring of 1925.\\n\\nThe first half of the principal manuscript told a very particular tale. It appears that on March 1st, 1925, a thin, dark young man of neurotic and excited aspect had called upon Professor Angell bearing the singular clay bas-relief, which was then exceedingly damp and fresh. His card bore the name of Henry Anthony Wilcox, and my uncle had recognized him as the youngest son of an excellent family slightly known to him, who had latterly been studying sculpture at the Rhode Island School of Design and living alone at the Fleur-de-Lys Building near that institution. Wilcox was a precocious youth of known genius but great eccentricity, and had from childhood excited attention through the strange stories and odd dreams he was in the habit of relating. He called himself \"psychically hypersensitive\", but the staid folk of the ancient commercial city dismissed him as merely \"queer.\" Never mingling much with his kind, he had dropped gradually from social visibility, and was now known only to a small group of aesthetes from other towns. Even the Providence Art Club, anxious to preserve its conservatism, had found him quite hopeless.\\n\\nOn the occasion of the visit, ran the professor\\'s manuscript, the sculptor abruptly asked for the benefit of his host\\'s archeological knowledge in identifying the hieroglyphics of the bas-relief. He spoke in a dreamy, stilted manner which suggested pose and alienated sympathy; and my uncle showed some sharpness in replying, for the conspicuous freshness of the tablet implied kinship with anything but archeology. Young Wilcox\\'s rejoinder, which impressed my uncle enough to make him recall and record it verbatim, was of a fantastically poetic cast which must have typified his whole conversation, and which I have since found highly characteristic of him. He said, \"It is new, indeed, for I made it last night in a dream of strange cities; and dreams are older than brooding Tyre, or the contemplative Sphinx, or garden-girdled Babylon.\"\\n\\nIt was then that he began that rambling tale which suddenly played upon a sleeping memory and won the fevered interest of my uncle. There had been a slight earthquake tremor the night before, the most considerable felt in New England for some years; and Wilcox\\'s imagination had been keenly affected. Upon retiring, he had had an unprecedented dream of great Cyclopean cities of Titan blocks and sky-flung monoliths, all dripping with green ooze and sinister with latent horror. Hieroglyphics had covered the walls and pillars, and from some undetermined point below had come a voice that was not a voice; a chaotic sensation which only fancy could transmute into sound, but which he attempted to render by the almost unpronounceable jumble of letters: \"Cthulhu fhtagn.\"\\n\\nThis verbal jumble was the key to the recollection which excited and disturbed Professor Angell. He questioned the sculptor with scientific minuteness; and studied with frantic intensity the bas-relief on which the youth had found himself working, chilled and clad only in his night clothes, when waking had stolen bewilderingly over him. My uncle blamed his old age, Wilcox afterwards said, for his slowness in recognizing both hieroglyphics and pictorial design. Many of his questions seemed highly out of place to his visitor, especially those which tried to connect the latter with strange cults or societies; and Wilcox could not understand the repeated promises of silence which he was offered in exchange for an admission of membership in some widespread mystical or paganly religious body. When Professor Angell became convinced that the sculptor was indeed ignorant of any cult or system of cryptic lore, he besieged his visitor with demands for future reports of dreams. This bore regular fruit, for after the first interview the manuscript records daily calls of the young man, during which he related startling fragments of nocturnal imaginery whose burden was always some terrible Cyclopean vista of dark and dripping stone, with a subterrene voice or intelligence shouting monotonously in enigmatical sense-impacts uninscribable save as gibberish. The two sounds frequently repeated are those rendered by the letters \"Cthulhu\" and \"R\\'lyeh.\"\\n\\nOn March 23, the manuscript continued, Wilcox failed to appear; and inquiries at his quarters revealed that he had been stricken with an obscure sort of fever and taken to the home of his family in Waterman Street. He had cried out in the night, arousing several other artists in the building, and had manifested since then only alternations of unconsciousness and delirium. My uncle at once telephoned the family, and from that time forward kept close watch of the case; calling often at the Thayer Street office of Dr. Tobey, whom he learned to be in charge. The youth\\'s febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them. They included not only a repetition of what he had formerly dreamed, but touched wildly on a gigantic thing \"miles high\" which walked or lumbered about.\\n\\nHe at no time fully described this object but occasional frantic words, as repeated by Dr. Tobey, convinced the professor that it must be identical with the nameless monstrosity he had sought to depict in his dream-sculpture. Reference to this object, the doctor added, was invariably a prelude to the young man\\'s subsidence into lethargy. His temperature, oddly enough, was not greatly above normal; but the whole condition was otherwise such as to suggest true fever rather than mental disorder.\\n\\nOn April 2 at about 3 P.M. every trace of Wilcox\\'s malady suddenly ceased. He sat upright in bed, astonished to find himself at home and completely ignorant of what had happened in dream or reality since the night of March 22. Pronounced well by his physician, he returned to his quarters in three days; but to Professor Angell he was of no further assistance. All traces of strange dreaming had vanished with his recovery, and my uncle kept no record of his night-thoughts after a week of pointless and irrelevant accounts of thoroughly usual visions.\\n\\nHere the first part of the manuscript ended, but references to certain of the scattered notes gave me much material for thought--so much, in fact, that only the ingrained skepticism then forming my philosophy can account for my continued distrust of the artist. The notes in question were those descriptive of the dreams of various persons covering the same period as that in which young Wilcox had had his strange visitations. My uncle, it seems, had quickly instituted a prodigiously far-flung body of inquires amongst nearly all the friends whom he could question without impertinence, asking for nightly reports of their dreams, and the dates of any notable visions for some time past. The reception of his request seems to have varied; but he must, at the very least, have received more responses than any ordinary man could have handled without a secretary. This original correspondence was not preserved, but his notes formed a thorough and really significant digest. Average people in society and business--New England\\'s traditional \"salt of the earth\"--gave an almost completely negative result, though scattered cases of uneasy but formless nocturnal impressions appear here and there, always between March 23 and April 2--the period of young Wilcox\\'s delirium. Scientific men were little more affected, though four cases of vague description suggest fugitive glimpses of strange landscapes, and in one case there is mentioned a dread of something abnormal.\\n\\nIt was from the artists and poets that the pertinent answers came, and I know that panic would have broken loose had they been able to compare notes. As it was, lacking their original letters, I half suspected the compiler of having asked leading questions, or of having edited the correspondence in corroboration of what he had latently resolved to see. That is why I continued to feel that Wilcox, somehow cognizant of the old data which my uncle had possessed, had been imposing on the veteran scientist. These responses from esthetes told disturbing tale. From February 28 to April 2 a large proportion of them had dreamed very bizarre things, the intensity of the dreams being immeasurably the stronger during the period of the sculptor\\'s delirium. Over a fourth of those who reported anything, reported scenes and half-sounds not unlike those which Wilcox had described; and some of the dreamers confessed acute fear of the gigantic nameless thing visible toward the last. One case, which the note describes with emphasis, was very sad. The subject, a widely known architect with leanings toward theosophy and occultism, went violently insane on the date of young Wilcox\\'s seizure, and expired several months later after incessant screamings to be saved from some escaped denizen of hell. Had my uncle referred to these cases by name instead of merely by number, I should have attempted some corroboration and personal investigation; but as it was, I succeeded in tracing down only a few. All of these, however, bore out the notes in full. I have often wondered if all the the objects of the professor\\'s questioning felt as puzzled as did this fraction. It is well that no explanation shall ever reach them.\\n\\nThe press cuttings, as I have intimated, touched on cases of panic, mania, and eccentricity during the given period. Professor Angell must have employed a cutting bureau, for the number of extracts was tremendous, and the sources scattered throughout the globe. Here was a nocturnal suicide in London, where a lone sleeper had leaped from a window after a shocking cry. Here likewise a rambling letter to the editor of a paper in South America, where a fanatic deduces a dire future from visions he has seen. A dispatch from California describes a theosophist colony as donning white robes en masse for some \"glorious fulfilment\" which never arrives, whilst items from India speak guardedly of serious native unrest toward the end of March 22-23.\\n\\nThe west of Ireland, too, is full of wild rumour and legendry, and a fantastic painter named Ardois-Bonnot hangs a blasphemous Dream Landscape in the Paris spring salon of 1926. And so numerous are the recorded troubles in insane asylums that only a miracle can have stopped the medical fraternity from noting strange parallelisms and drawing mystified conclusions. A weird bunch of cuttings, all told; and I can at this date scarcely envisage the callous rationalism with which I set them aside. But I was then convinced that young Wilcox had known of the older matters mentioned by the professor.\\n\\nII. The Tale of Inspector Legrasse\\n\\nThe older matters which had made the sculptor\\'s dream and bas-relief so significant to my uncle formed the subject of the second half of his long manuscript. Once before, it appears, Professor Angell had seen the hellish outlines of the nameless monstrosity, puzzled over the unknown hieroglyphics, and heard the ominous syllables which can be rendered only as \"Cthulhu\"; and all this in so stirring and horrible a connection that it is small wonder he pursued young Wilcox with queries and demands for data.\\n\\nThis earlier experience had come in 1908, seventeen years before, when the American Archaeological Society held its annual meeting in St. Louis. Professor Angell, as befitted one of his authority and attainments, had had a prominent part in all the deliberations; and was one of the first to be approached by the several outsiders who took advantage of the convocation to offer questions for correct answering and problems for expert solution.\\n\\nThe chief of these outsiders, and in a short time the focus of interest for the entire meeting, was a commonplace-looking middle-aged man who had travelled all the way from New Orleans for certain special information unobtainable from any local source. His name was John Raymond Legrasse, and he was by profession an Inspector of Police. With him he bore the subject of his visit, a grotesque, repulsive, and apparently very ancient stone statuette whose origin he was at a loss to determine. It must not be fancied that Inspector Legrasse had the least interest in archaeology. On the contrary, his wish for enlightenment was prompted by purely professional considerations. The statuette, idol, fetish, or whatever it was, had been captured some months before in the wooded swamps south of New Orleans during a raid on a supposed voodoo meeting; and so singular and hideous were the rites connected with it, that the police could not but realise that they had stumbled on a dark cult totally unknown to them, and infinitely more diabolic than even the blackest of the African voodoo circles. Of its origin, apart from the erratic and unbelievable tales extorted from the captured members, absolutely nothing was to be discovered; hence the anxiety of the police for any antiquarian lore which might help them to place the frightful symbol, and through it track down the cult to its fountain-head.\\n\\nInspector Legrasse was scarcely prepared for the sensation which his offering created. One sight of the thing had been enough to throw the assembled men of science into a state of tense excitement, and they lost no time in crowding around him to gaze at the diminutive figure whose utter strangeness and air of genuinely abysmal antiquity hinted so potently at unopened and archaic vistas. No recognised school of sculpture had animated this terrible object, yet centuries and even thousands of years seemed recorded in its dim and greenish surface of unplaceable stone.\\n\\nThe figure, which was finally passed slowly from man to man for close and careful study, was between seven and eight inches in height, and of exquisitely artistic workmanship. It represented a monster of vaguely anthropoid outline, but with an octopus-like head whose face was a mass of feelers, a scaly, rubbery-looking body, prodigious claws on hind and fore feet, and long, narrow wings behind. This thing, which seemed instinct with a fearsome and unnatural malignancy, was of a somewhat bloated corpulence, and squatted evilly on a rectangular block or pedestal covered with undecipherable characters. The tips of the wings touched the back edge of the block, the seat occupied the centre, whilst the long, curved claws of the doubled-up, crouching hind legs gripped the front edge and extended a quarter of the way down toward the bottom of the pedestal. The cephalopod head was bent forward, so that the ends of the facial feelers brushed the backs of huge fore paws which clasped the croucher\\'s elevated knees. The aspect of the whole was abnormally life-like, and the more subtly fearful because its source was so totally unknown. Its vast, awesome, and incalculable age was unmistakable; yet not one link did it shew with any known type of art belonging to civilisation\\'s youth--or indeed to any other time. Totally separate and apart, its very material was a mystery; for the soapy, greenish-black stone with its golden or iridescent flecks and striations resembled nothing familiar to geology or mineralogy. The characters along the base were equally baffling; and no member present, despite a representation of half the world\\'s expert learning in this field, could form the least notion of even their remotest linguistic kinship. They, like the subject and material, belonged to something horribly remote and distinct from mankind as we know it, something frightfully suggestive of old and unhallowed cycles of life in which our world and our conceptions have no part.\\n\\nAnd yet, as the members severally shook their heads and confessed defeat at the Inspector\\'s problem, there was one man in that gathering who suspected a touch of bizarre familiarity in the monstrous shape and writing, and who presently told with some diffidence of the odd trifle he knew. This person was the late William Channing Webb, Professor of Anthropology in Princeton University, and an explorer of no slight note. Professor Webb had been engaged, forty-eight years before, in a tour of Greenland and Iceland in search of some Runic inscriptions which he failed to unearth; and whilst high up on the West Greenland coast had encountered a singular tribe or cult of degenerate Esquimaux whose religion, a curious form of devil-worship, chilled him with its deliberate bloodthirstiness and repulsiveness. It was a faith of which other Esquimaux knew little, and which they mentioned only with shudders, saying that it had come down from horribly ancient aeons before ever the world was made. Besides nameless rites and human sacrifices there were certain queer hereditary rituals addressed to a supreme elder devil or tornasuk; and of this Professor Webb had taken a careful phonetic copy from an aged angekok or wizard-priest, expressing the sounds in Roman letters as best he knew how. But just now of prime significance was the fetish which this cult had cherished, and around which they danced when the aurora leaped high over the ice cliffs. It was, the professor stated, a very crude bas-relief of stone, comprising a hideous picture and some cryptic writing. And so far as he could tell, it was a rough parallel in all essential features of the bestial thing now lying before the meeting.\\n\\nThis data, received with suspense and astonishment by the assembled members, proved doubly exciting to Inspector Legrasse; and he began at once to ply his informant with questions. Having noted and copied an oral ritual among the swamp cult-worshippers his men had arrested, he besought the professor to remember as best he might the syllables taken down amongst the diabolist Esquimaux. There then followed an exhaustive comparison of details, and a moment of really awed silence when both detective and scientist agreed on the virtual identity of the phrase common to two hellish rituals so many worlds of distance apart. What, in substance, both the Esquimaux wizards and the Louisiana swamp-priests had chanted to their kindred idols was something very like this: the word-divisions being guessed at from traditional breaks in the phrase as chanted aloud:\\n\\n\"Ph\\'nglui mglw\\'nafh Cthulhu R\\'lyeh wgah\\'nagl fhtagn.\"\\n\\nLegrasse had one point in advance of Professor Webb, for several among his mongrel prisoners had repeated to him what older celebrants had told them the words meant. This text, as given, ran something like this:\\n\\n\"In his house at R\\'lyeh dead Cthulhu waits dreaming.\"\\n\\nAnd now, in response to a general and urgent demand, Inspector Legrasse related as fully as possible his experience with the swamp worshippers; telling a story to which I could see my uncle attached profound significance. It savoured of the wildest dreams of myth-maker and theosophist, and disclosed an astonishing degree of cosmic imagination among such half-castes and pariahs as might be least expected to possess it.\\n\\nOn November 1st, 1907, there had come to the New Orleans police a frantic summons from the swamp and lagoon country to the south. The squatters there, mostly primitive but good-natured descendants of Lafitte\\'s men, were in the grip of stark terror from an unknown thing which had stolen upon them in the night. It was voodoo, apparently, but voodoo of a more terrible sort than they had ever known; and some of their women and children had disappeared since the malevolent tom-tom had begun its incessant beating far within the black haunted woods where no dweller ventured. There were insane shouts and harrowing screams, soul-chilling chants and dancing devil-flames; and, the frightened messenger added, the people could stand it no more.\\n\\nSo a body of twenty police, filling two carriages and an automobile, had set out in the late afternoon with the shivering squatter as a guide. At the end of the passable road they alighted, and for miles splashed on in silence through the terrible cypress woods where day never came. Ugly roots and malignant hanging nooses of Spanish moss beset them, and now and then a pile of dank stones or fragment of a rotting wall intensified by its hint of morbid habitation a depression which every malformed tree and every fungous islet combined to create. At length the squatter settlement, a miserable huddle of huts, hove in sight; and hysterical dwellers ran out to cluster around the group of bobbing lanterns. The muffled beat of tom-toms was now faintly audible far, far ahead; and a curdling shriek came at infrequent intervals when the wind shifted. A reddish glare, too, seemed to filter through pale undergrowth beyond the endless avenues of forest night. Reluctant even to be left alone again, each one of the cowed squatters refused point-blank to advance another inch toward the scene of unholy worship, so Inspector Legrasse and his nineteen colleagues plunged on unguided into black arcades of horror that none of them had ever trod before.\\n\\nThe region now entered by the police was one of traditionally evil repute, substantially unknown and untraversed by white men. There were legends of a hidden lake unglimpsed by mortal sight, in which dwelt a huge, formless white polypous thing with luminous eyes; and squatters whispered that bat-winged devils flew up out of caverns in inner earth to worship it at midnight. They said it had been there before D\\'Iberville, before La Salle, before the Indians, and before even the wholesome beasts and birds of the woods. It was nightmare itself, and to see it was to die. But it made men dream, and so they knew enough to keep away. The present voodoo orgy was, indeed, on the merest fringe of this abhorred area, but that location was bad enough; hence perhaps the very place of the worship had terrified the squatters more than the shocking sounds and incidents.\\n\\nOnly poetry or madness could do justice to the noises heard by Legrasse\\'s men as they ploughed on through the black morass toward the red glare and muffled tom-toms. There are vocal qualities peculiar to men, and vocal qualities peculiar to beasts; and it is terrible to hear the one when the source should yield the other. Animal fury and orgiastic license here whipped themselves to daemoniac heights by howls and squawking ecstacies that tore and reverberated through those nighted woods like pestilential tempests from the gulfs of hell. Now and then the less organized ululation would cease, and from what seemed a well-drilled chorus of hoarse voices would rise in sing-song chant that hideous phrase or ritual:\\n\\n\"Ph\\'nglui mglw\\'nafh Cthulhu R\\'lyeh wgah\\'nagl fhtagn.\"\\n\\nThen the men, having reached a spot where the trees were thinner, came suddenly in sight of the spectacle itself. Four of them reeled, one fainted, and two were shaken into a frantic cry which the mad cacophony of the orgy fortunately deadened. Legrasse dashed swamp water on the face of the fainting man, and all stood trembling and nearly hypnotised with horror.\\n\\nIn a natural glade of the swamp stood a grassy island of perhaps an acre\\'s extent, clear of trees and tolerably dry. On this now leaped and twisted a more indescribable horde of human abnormality than any but a Sime or an Angarola could paint. Void of clothing, this hybrid spawn were braying, bellowing, and writhing about a monstrous ring-shaped bonfire; in the centre of which, revealed by occasional rifts in the curtain of flame, stood a great granite monolith some eight feet in height; on top of which, incongruous in its diminutiveness, rested the noxious carven statuette. From a wide circle of ten scaffolds set up at regular intervals with the flame-girt monolith as a centre hung, head downward, the oddly marred bodies of the helpless squatters who had disappeared. It was inside this circle that the ring of worshippers jumped and roared, the general direction of the mass motion being from left to right in endless Bacchanal between the ring of bodies and the ring of fire.\\n\\nIt may have been only imagination and it may have been only echoes which induced one of the men, an excitable Spaniard, to fancy he heard antiphonal responses to the ritual from some far and unillumined spot deeper within the wood of ancient legendry and horror. This man, Joseph D. Galvez, I later met and questioned; and he proved distractingly imaginative. He indeed went so far as to hint of the faint beating of great wings, and of a glimpse of shining eyes and a mountainous white bulk beyond the remotest trees but I suppose he had been hearing too much native superstition.\\n\\nActually, the horrified pause of the men was of comparatively brief duration. Duty came first; and although there must have been nearly a hundred mongrel celebrants in the throng, the police relied on their firearms and plunged determinedly into the nauseous rout. For five minutes the resultant din and chaos were beyond description. Wild blows were struck, shots were fired, and escapes were made; but in the end Legrasse was able to count some forty-seven sullen prisoners, whom he forced to dress in haste and fall into line between two rows of policemen. Five of the worshippers lay dead, and two severely wounded ones were carried away on improvised stretchers by their fellow-prisoners. The image on the monolith, of course, was carefully removed and carried back by Legrasse.\\n\\nExamined at headquarters after a trip of intense strain and weariness, the prisoners all proved to be men of a very low, mixed-blooded, and mentally aberrant type. Most were seamen, and a sprinkling of Negroes and mulattoes, largely West Indians or Brava Portuguese from the Cape Verde Islands, gave a colouring of voodooism to the heterogeneous cult. But before many questions were asked, it became manifest that something far deeper and older than Negro fetishism was involved. Degraded and ignorant as they were, the creatures held with surprising consistency to the central idea of their loathsome faith.\\n\\nThey worshipped, so they said, the Great Old Ones who lived ages before there were any men, and who came to the young world out of the sky. Those Old Ones were gone now, inside the earth and under the sea; but their dead bodies had told their secrets in dreams to the first men, who formed a cult which had never died. This was that cult, and the prisoners said it had always existed and always would exist, hidden in distant wastes and dark places all over the world until the time when the great priest Cthulhu, from his dark house in the mighty city of R\\'lyeh under the waters, should rise and bring the earth again beneath his sway. Some day he would call, when the stars were ready, and the secret cult would always be waiting to liberate him.\\n\\nMeanwhile no more must be told. There was a secret which even torture could not extract. Mankind was not absolutely alone among the conscious things of earth, for shapes came out of the dark to visit the faithful few. But these were not the Great Old Ones. No man had ever seen the Old Ones. The carven idol was great Cthulhu, but none might say whether or not the others were precisely like him. No one could read the old writing now, but things were told by word of mouth. The chanted ritual was not the secret--that was never spoken aloud, only whispered. The chant meant only this: \"In his house at R\\'lyeh dead Cthulhu waits dreaming.\"\\n\\nOnly two of the prisoners were found sane enough to be hanged, and the rest were committed to various institutions. All denied a part in the ritual murders, and averred that the killing had been done by Black Winged Ones which had come to them from their immemorial meeting-place in the haunted wood. But of those mysterious allies no coherent account could ever be gained. What the police did extract, came mainly from the immensely aged mestizo named Castro, who claimed to have sailed to strange ports and talked with undying leaders of the cult in the mountains of China.\\n\\nOld Castro remembered bits of hideous legend that paled the speculations of theosophists and made man and the world seem recent and transient indeed. There had been aeons when other Things ruled on the earth, and They had had great cities. Remains of Them, he said the deathless Chinamen had told him, were still be found as Cyclopean stones on islands in the Pacific. They all died vast epochs of time before men came, but there were arts which could revive Them when the stars had come round again to the right positions in the cycle of eternity. They had, indeed, come themselves from the stars, and brought Their images with Them.\\n\\nThese Great Old Ones, Castro continued, were not composed altogether of flesh and blood. They had shape--for did not this star-fashioned image prove it?--but that shape was not made of matter. When the stars were right, They could plunge from world to world through the sky; but when the stars were wrong, They could not live. But although They no longer lived, They would never really die. They all lay in stone houses in Their great city of R\\'lyeh, preserved by the spells of mighty Cthulhu for a glorious resurrection when the stars and the earth might once more be ready for Them. But at that time some force from outside must serve to liberate Their bodies. The spells that preserved them intact likewise prevented Them from making an initial move, and They could only lie awake in the dark and think whilst uncounted millions of years rolled by. They knew all that was occurring in the universe, for Their mode of speech was transmitted thought. Even now They talked in Their tombs. When, after infinities of chaos, the first men came, the Great Old Ones spoke to the sensitive among them by moulding their dreams; for only thus could Their language reach the fleshly minds of mammals.\\n\\nThen, whispered Castro, those first men formed the cult around tall idols which the Great Ones showed them; idols brought in dim eras from dark stars. That cult would never die till the stars came right again, and the secret priests would take great Cthulhu from His tomb to revive His subjects and resume His rule of earth. The time would be easy to know, for then mankind would have become as the Great Old Ones; free and wild and beyond good and evil, with laws and morals thrown aside and all men shouting and killing and revelling in joy. Then the liberated Old Ones would teach them new ways to shout and kill and revel and enjoy themselves, and all the earth would flame with a holocaust of ecstasy and freedom. Meanwhile the cult, by appropriate rites, must keep alive the memory of those ancient ways and shadow forth the prophecy of their return.\\n\\nIn the elder time chosen men had talked with the entombed Old Ones in dreams, but then something happened. The great stone city R\\'lyeh, with its monoliths and sepulchres, had sunk beneath the waves; and the deep waters, full of the one primal mystery through which not even thought can pass, had cut off the spectral intercourse. But memory never died, and the high-priests said that the city would rise again when the stars were right. Then came out of the earth the black spirits of earth, mouldy and shadowy, and full of dim rumours picked up in caverns beneath forgotten sea-bottoms. But of them old Castro dared not speak much. He cut himself off hurriedly, and no amount of persuasion or subtlety could elicit more in this direction. The size of the Old Ones, too, he curiously declined to mention. Of the cult, he said that he thought the centre lay amid the pathless desert of Arabia, where Irem, the City of Pillars, dreams hidden and untouched. It was not allied to the European witch-cult, and was virtually unknown beyond its members. No book had ever really hinted of it, though the deathless Chinamen said that there were double meanings in the Necronomicon of the mad Arab Abdul Alhazred which the initiated might read as they chose, especially the much-discussed couplet:\\n\\nThat is not dead which can eternal lie,\\nAnd with strange aeons even death may die.\\n\\nLegrasse, deeply impressed and not a little bewildered, had inquired in vain concerning the historic affiliations of the cult. Castro, apparently, had told the truth when he said that it was wholly secret. The authorities at Tulane University could shed no light upon either cult or image, and now the detective had come to the highest authorities in the country and met with no more than the Greenland tale of Professor Webb.\\n\\nThe feverish interest aroused at the meeting by Legrasse\\'s tale, corroborated as it was by the statuette, is echoed in the subsequent correspondence of those who attended; although scant mention occurs in the formal publications of the society. Caution is the first care of those accustomed to face occasional charlatanry and imposture. Legrasse for some time lent the image to Professor Webb, but at the latter\\'s death it was returned to him and remains in his possession, where I viewed it not long ago. It is truly a terrible thing, and unmistakably akin to the dream-sculpture of young Wilcox.\\n\\nThat my uncle was excited by the tale of the sculptor I did not wonder, for what thoughts must arise upon hearing, after a knowledge of what Legrasse had learned of the cult, of a sensitive young man who had dreamed not only the figure and exact hieroglyphics of the swamp-found image and the Greenland devil tablet, but had come in his dreams upon at least three of the precise words of the formula uttered alike by Esquimaux diabolists and mongrel Louisianans? Professor Angell\\'s instant start on an investigation of the utmost thoroughness was eminently natural; though privately I suspected young Wilcox of having heard of the cult in some indirect way, and of having invented a series of dreams to heighten and continue the mystery at my uncle\\'s expense. The dream-narratives and cuttings collected by the professor were, of course, strong corroboration; but the rationalism of my mind and the extravagance of the whole subject led me to adopt what I thought the most sensible conclusions. So, after thoroughly studying the manuscript again and correlating the theosophical and anthropological notes with the cult narrative of Legrasse, I made a trip to Providence to see the sculptor and give him the rebuke I thought proper for so boldly imposing upon a learned and aged man.\\n\\nWilcox still lived alone in the Fleur-de-Lys Building in Thomas Street, a hideous Victorian imitation of seventeenth century Breton Architecture which flaunts its stuccoed front amidst the lovely colonial houses on the ancient hill, and under the very shadow of the finest Georgian steeple in America, I found him at work in his rooms, and at once conceded from the specimens scattered about that his genius is indeed profound and authentic. He will, I believe, some time be heard from as one of the great decadents; for he has crystallised in clay and will one day mirror in marble those nightmares and phantasies which Arthur Machen evokes in prose, and Clark Ashton Smith makes visible in verse and in painting.\\n\\nDark, frail, and somewhat unkempt in aspect, he turned languidly at my knock and asked me my business without rising. Then I told him who I was, he displayed some interest; for my uncle had excited his curiosity in probing his strange dreams, yet had never explained the reason for the study. I did not enlarge his knowledge in this regard, but sought with some subtlety to draw him out. In a short time I became convinced of his absolute sincerity, for he spoke of the dreams in a manner none could mistake. They and their subconscious residuum had influenced his art profoundly, and he shewed me a morbid statue whose contours almost made me shake with the potency of its black suggestion. He could not recall having seen the original of this thing except in his own dream bas-relief, but the outlines had formed themselves insensibly under his hands. It was, no doubt, the giant shape he had raved of in delirium. That he really knew nothing of the hidden cult, save from what my uncle\\'s relentless catechism had let fall, he soon made clear; and again I strove to think of some way in which he could possibly have received the weird impressions.\\n\\nHe talked of his dreams in a strangely poetic fashion; making me see with terrible vividness the damp Cyclopean city of slimy green stone--whose geometry, he oddly said, was all wrong--and hear with frightened expectancy the ceaseless, half-mental calling from underground: \"Cthulhu fhtagn\", \"Cthulhu fhtagn.\"\\n\\nThese words had formed part of that dread ritual which told of dead Cthulhu\\'s dream-vigil in his stone vault at R\\'lyeh, and I felt deeply moved despite my rational beliefs. Wilcox, I was sure, had heard of the cult in some casual way, and had soon forgotten it amidst the mass of his equally weird reading and imagining. Later, by virtue of its sheer impressiveness, it had found subconscious expression in dreams, in the bas-relief, and in the terrible statue I now beheld; so that his imposture upon my uncle had been a very innocent one. The youth was of a type, at once slightly affected and slightly ill-mannered, which I could never like, but I was willing enough now to admit both his genius and his honesty. I took leave of him amicably, and wish him all the success his talent promises.\\n\\nThe matter of the cult still remained to fascinate me, and at times I had visions of personal fame from researches into its origin and connections. I visited New Orleans, talked with Legrasse and others of that old-time raiding-party, saw the frightful image, and even questioned such of the mongrel prisoners as still survived. Old Castro, unfortunately, had been dead for some years. What I now heard so graphically at first-hand, though it was really no more than a detailed confirmation of what my uncle had written, excited me afresh; for I felt sure that I was on the track of a very real, very secret, and very ancient religion whose discovery would make me an anthropologist of note. My attitude was still one of absolute materialism, as I wish it still were, and I discounted with almost inexplicable perversity the coincidence of the dream notes and odd cuttings collected by Professor Angell.\\n\\nOne thing I began to suspect, and which I now fear I know, is that my uncle\\'s death was far from natural. He fell on a narrow hill street leading up from an ancient waterfront swarming with foreign mongrels, after a careless push from a Negro sailor. I did not forget the mixed blood and marine pursuits of the cult-members in Louisiana, and would not be surprised to learn of secret methods and rites and beliefs. Legrasse and his men, it is true, have been let alone; but in Norway a certain seaman who saw things is dead. Might not the deeper inquiries of my uncle after encountering the sculptor\\'s data have come to sinister ears? I think Professor Angell died because he knew too much, or because he was likely to learn too much. Whether I shall go as he did remains to be seen, for I have learned much now.\\n\\nIII. The Madness from the Sea\\n\\nIf heaven ever wishes to grant me a boon, it will be a total effacing of the results of a mere chance which fixed my eye on a certain stray piece of shelf-paper. It was nothing on which I would naturally have stumbled in the course of my daily round, for it was an old number of an Australian journal, the Sydney Bulletin for April 18, 1925. It had escaped even the cutting bureau which had at the time of its issuance been avidly collecting material for my uncle\\'s research.\\n\\nI had largely given over my inquiries into what Professor Angell called the \"Cthulhu Cult\", and was visiting a learned friend in Paterson, New Jersey; the curator of a local museum and a mineralogist of note. Examining one day the reserve specimens roughly set on the storage shelves in a rear room of the museum, my eye was caught by an odd picture in one of the old papers spread beneath the stones. It was the Sydney Bulletin I have mentioned, for my friend had wide affiliations in all conceivable foreign parts; and the picture was a half-tone cut of a hideous stone image almost identical with that which Legrasse had found in the swamp.\\n\\nEagerly clearing the sheet of its precious contents, I scanned the item in detail; and was disappointed to find it of only moderate length. What it suggested, however, was of portentous significance to my flagging quest; and I carefully tore it out for immediate action. It read as follows:\\n\\nMYSTERY DERELICT FOUND AT SEA\\n\\nVigilant Arrives With Helpless Armed New Zealand Yacht in Tow. One Survivor and Dead Man Found Aboard. Tale of Desperate Battle and Deaths at Sea. Rescued Seaman Refuses Particulars of Strange Experience. Odd Idol Found in His Possession. Inquiry to Follow.\\n\\nThe Morrison Co.\\'s freighter Vigilant, bound from Valparaiso, arrived this morning at its wharf in Darling Harbour, having in tow the battled and disabled but heavily armed steam yacht Alert of Dunedin, N.Z., which was sighted April 12th in S. Latitude 3421\\', W. Longitude 15217\\', with one living and one dead man aboard.\\n\\nThe Vigilant left Valparaiso March 25th, and on April 2nd was driven considerably south of her course by exceptionally heavy storms and monster waves. On April 12th the derelict was sighted; and though apparently deserted, was found upon boarding to contain one survivor in a half-delirious condition and one man who had evidently been dead for more than a week. The living man was clutching a horrible stone idol of unknown origin, about foot in height, regarding whose nature authorities at Sydney University, the Royal Society, and the Museum in College Street all profess complete bafflement, and which the survivor says he found in the cabin of the yacht, in a small carved shrine of common pattern.\\n\\nThis man, after recovering his senses, told an exceedingly strange story of piracy and slaughter. He is Gustaf Johansen, a Norwegian of some intelligence, and had been second mate of the two-masted schooner Emma of Auckland, which sailed for Callao February 20th with a complement of eleven men. The Emma, he says, was delayed and thrown widely south of her course by the great storm of March 1st, and on March 22nd, in S. Latitude 4951\\' W. Longitude 12834\\', encountered the Alert, manned by a queer and evil-looking crew of Kanakas and half-castes. Being ordered peremptorily to turn back, Capt. Collins refused; whereupon the strange crew began to fire savagely and without warning upon the schooner with a peculiarly heavy battery of brass cannon forming part of the yacht\\'s equipment. The Emma\\'s men showed fight, says the survivor, and though the schooner began to sink from shots beneath the water-line they managed to heave alongside their enemy and board her, grappling with the savage crew on the yacht\\'s deck, and being forced to kill them all, the number being slightly superior, because of their particularly abhorrent and desperate though rather clumsy mode of fighting.\\n\\nThree of the Emma\\'s men, including Capt. Collins and First Mate Green, were killed; and the remaining eight under Second Mate Johansen proceeded to navigate the captured yacht, going ahead in their original direction to see if any reason for their ordering back had existed. The next day, it appears, they raised and landed on a small island, although none is known to exist in that part of the ocean; and six of the men somehow died ashore, though Johansen is queerly reticent about this part of his story, and speaks only of their falling into a rock chasm. Later, it seems, he and one companion boarded the yacht and tried to manage her, but were beaten about by the storm of April 2nd, From that time till his rescue on the 12th the man remembers little, and he does not even recall when William Briden, his companion, died. Briden\\'s death reveals no apparent cause, and was probably due to excitement or exposure. Cable advices from Dunedin report that the Alert was well known there as an island trader, and bore an evil reputation along the waterfront, It was owned by a curious group of half-castes whose frequent meetings and night trips to the woods attracted no little curiosity; and it had set sail in great haste just after the storm and earth tremors of March 1st. Our Auckland correspondent gives the Emma and her crew an excellent reputation, and Johansen is described as a sober and worthy man. The admiralty will institute an inquiry on the whole matter beginning tomorrow, at which every effort will be made to induce Johansen to speak more freely than he has done hitherto.\\n\\nThis was all, together with the picture of the hellish image; but what a train of ideas it started in my mind! Here were new treasuries of data on the Cthulhu Cult, and evidence that it had strange interests at sea as well as on land. What motive prompted the hybrid crew to order back the Emma as they sailed about with their hideous idol? What was the unknown island on which six of the Emma\\'s crew had died, and about which the mate Johansen was so secretive? What had the vice-admiralty\\'s investigation brought out, and what was known of the noxious cult in Dunedin? And most marvellous of all, what deep and more than natural linkage of dates was this which gave a malign and now undeniable significance to the various turns of events so carefully noted by my uncle?\\n\\nMarch 1st--or February 28th according to the International Date Line--the earthquake and storm had come. From Dunedin the Alert and her noisome crew had darted eagerly forth as if imperiously summoned, and on the other side of the earth poets and artists had begun to dream of a strange, dank Cyclopean city whilst a young sculptor had moulded in his sleep the form of the dreaded Cthulhu. March 23rd the crew of the Emma landed on an unknown island and left six men dead; and on that date the dreams of sensitive men assumed a heightened vividness and darkened with dread of a giant monster\\'s malign pursuit, whilst an architect had gone mad and a sculptor had lapsed suddenly into delirium! And what of this storm of April 2nd--the date on which all dreams of the dank city ceased, and Wilcox emerged unharmed from the bondage of strange fever? What of all this--and of those hints of old Castro about the sunken, star-born Old Ones and their coming reign; their faithful cult and their mastery of dreams? Was I tottering on the brink of cosmic horrors beyond man\\'s power to bear? If so, they must be horrors of the mind alone, for in some way the second of April had put a stop to whatever monstrous menace had begun its siege of mankind\\'s soul.\\n\\nThat evening, after a day of hurried cabling and arranging, I bade my host adieu and took a train for San Francisco. In less than a month I was in Dunedin; where, however, I found that little was known of the strange cult-members who had lingered in the old sea-taverns. Waterfront scum was far too common for special mention; though there was vague talk about one inland trip these mongrels had made, during which faint drumming and red flame were noted on the distant hills. In Auckland I learned that Johansen had returned with yellow hair turned white after a perfunctory and inconclusive questioning at Sydney, and had thereafter sold his cottage in West Street and sailed with his wife to his old home in Oslo. Of his stirring experience he would tell his friends no more than he had told the admiralty officials, and all they could do was to give me his Oslo address.\\n\\nAfter that I went to Sydney and talked profitlessly with seamen and members of the vice-admiralty court. I saw the Alert, now sold and in commercial use, at Circular Quay in Sydney Cove, but gained nothing from its non-committal bulk. The crouching image with its cuttlefish head, dragon body, scaly wings, and hieroglyphed pedestal, was preserved in the Museum at Hyde Park; and I studied it long and well, finding it a thing of balefully exquisite workmanship, and with the same utter mystery, terrible antiquity, and unearthly strangeness of material which I had noted in Legrasse\\'s smaller specimen. Geologists, the curator told me, had found it a monstrous puzzle; for they vowed that the world held no rock like it. Then I thought with a shudder of what Old Castro had told Legrasse about the Old Ones; \"They had come from the stars, and had brought Their images with Them.\"\\n\\nShaken with such a mental resolution as I had never before known, I now resolved to visit Mate Johansen in Oslo. Sailing for London, I reembarked at once for the Norwegian capital; and one autumn day landed at the trim wharves in the shadow of the Egeberg. Johansen\\'s address, I discovered, lay in the Old Town of King Harold Haardrada, which kept alive the name of Oslo during all the centuries that the greater city masqueraded as \"Christiana.\" I made the brief trip by taxicab, and knocked with palpitant heart at the door of a neat and ancient building with plastered front. A sad-faced woman in black answered my summons, and I was stung with disappointment when she told me in halting English that Gustaf Johansen was no more.\\n\\nHe had not long survived his return, said his wife, for the doings at sea in 1925 had broken him. He had told her no more than he told the public, but had left a long manuscript--of \"technical matters\" as he said--written in English, evidently in order to guard her from the peril of casual perusal. During a walk through a narrow lane near the Gothenburg dock, a bundle of papers falling from an attic window had knocked him down. Two Lascar sailors at once helped him to his feet, but before the ambulance could reach him he was dead. Physicians found no adequate cause the end, and laid it to heart trouble and a weakened constitution. I now felt gnawing at my vitals that dark terror which will never leave me till I, too, am at rest; \"accidentally\" or otherwise. Persuading the widow that my connection with her husband\\'s \"technical matters\" was sufficient to entitle me to his manuscript, I bore the document away and began to read it on the London boat.\\n\\nIt was a simple, rambling thing--a naive sailor\\'s effort at a post-facto diary--and strove to recall day by day that last awful voyage. I cannot attempt to transcribe it verbatim in all its cloudiness and redundance, but I will tell its gist enough to show why the sound the water against the vessel\\'s sides became so unendurable to me that I stopped my ears with cotton.\\n\\nJohansen, thank God, did not know quite all, even though he saw the city and the Thing, but I shall never sleep calmly again when I think of the horrors that lurk ceaselessly behind life in time and in space, and of those unhallowed blasphemies from elder stars which dream beneath the sea, known and favoured by a nightmare cult ready and eager to loose them upon the world whenever another earthquake shall heave their monstrous stone city again to the sun and air.\\n\\nJohansen\\'s voyage had begun just as he told it to the vice-admiralty. The Emma, in ballast, had cleared Auckland on February 20th, and had felt the full force of that earthquake-born tempest which must have heaved up from the sea-bottom the horrors that filled men\\'s dreams. Once more under control, the ship was making good progress when held up by the Alert on March 22nd, and I could feel the mate\\'s regret as he wrote of her bombardment and sinking. Of the swarthy cult-fiends on the Alert he speaks with significant horror. There was some peculiarly abominable quality about them which made their destruction seem almost a duty, and Johansen shows ingenuous wonder at the charge of ruthlessness brought against his party during the proceedings of the court of inquiry. Then, driven ahead by curiosity in their captured yacht under Johansen\\'s command, the men sight a great stone pillar sticking out of the sea, and in S. Latitude 479\\', W. Longitude l2343\\', come upon a coastline of mingled mud, ooze, and weedy Cyclopean masonry which can be nothing less than the tangible substance of earth\\'s supreme terror--the nightmare corpse-city of R\\'lyeh, that was built in measureless aeons behind history by the vast, loathsome shapes that seeped down from the dark stars. There lay great Cthulhu and his hordes, hidden in green slimy vaults and sending out at last, after cycles incalculable, the thoughts that spread fear to the dreams of the sensitive and called imperiously to the faithful to come on a pilgrimage of liberation and restoration. All this Johansen did not suspect, but God knows he soon saw enough!\\n\\nI suppose that only a single mountain-top, the hideous monolith-crowned citadel whereon great Cthulhu was buried, actually emerged from the waters. When I think of the extent of all that may be brooding down there I almost wish to kill myself forthwith. Johansen and his men were awed by the cosmic majesty of this dripping Babylon of elder daemons, and must have guessed without guidance that it was nothing of this or of any sane planet. Awe at the unbelievable size of the greenish stone blocks, at the dizzying height of the great carven monolith, and at the stupefying identity of the colossal statues and bas-reliefs with the queer image found in the shrine on the Alert, is poignantly visible in every line of the mates frightened description.\\n\\nWithout knowing what futurism is like, Johansen achieved something very close to it when he spoke of the city; for instead of describing any definite structure or building, he dwells only on broad impressions of vast angles and stone surfaces--surfaces too great to belong to anything right or proper for this earth, and impious with horrible images and hieroglyphs. I mention his talk about angles because it suggests something Wilcox had told me of his awful dreams. He said that the geometry of the dream-place he saw was abnormal, non-Euclidean, and loathsomely redolent of spheres and dimensions apart from ours. Now an unlettered seaman felt the same thing whilst gazing at the terrible reality.\\n\\nJohansen and his men landed at a sloping mud-bank on this monstrous Acropolis, and clambered slipperily up over titan oozy blocks which could have been no mortal staircase. The very sun of heaven seemed distorted when viewed through the polarising miasma welling out from this sea-soaked perversion, and twisted menace and suspense lurked leeringly in those crazily elusive angles of carven rock where a second glance showed concavity after the first showed convexity.\\n\\nSomething very like fright had come over all the explorers before anything more definite than rock and ooze and weed was seen. Each would have fled had he not feared the scorn of the others, and it was only half-heartedly that they searched--vainly, as it proved--for some portable souvenir to bear away.\\n\\nIt was Rodriguez the Portuguese who climbed up the foot of the monolith and shouted of what he had found. The rest followed him, and looked curiously at the immense carved door with the now familiar squid-dragon bas-relief. It was, Johansen said, like a great barn-door; and they all felt that it was a door because of the ornate lintel, threshold, and jambs around it, though they could not decide whether it lay flat like a trap-door or slantwise like an outside cellar-door. As Wilcox would have said, the geometry of the place was all wrong. One could not be sure that the sea and the ground were horizontal, hence the relative position of everything else seemed phantasmally variable.\\n\\nBriden pushed at the stone in several places without result. Then Donovan felt over it delicately around the edge, pressing each point separately as he went. He climbed interminably along the grotesque stone moulding--that is, one would call it climbing if the thing was not after all horizontal--and the men wondered how any door in the universe could be so vast. Then, very softly and slowly, the acre-great lintel began to give inward at the top; and they saw that it was balanced.\\n\\nDonovan slid or somehow propelled himself down or along the jamb and rejoined his fellows, and everyone watched the queer recession of the monstrously carven portal. In this phantasy of prismatic distortion it moved anomalously in a diagonal way, so that all the rules of matter and perspective seemed upset.\\n\\nThe aperture was black with a darkness almost material. That tenebrousness was indeed a positive quality; for it obscured such parts of the inner walls as ought to have been revealed, and actually burst forth like smoke from its aeon-long imprisonment, visibly darkening the sun as it slunk away into the shrunken and gibbous sky on flapping membraneous wings. The odour rising from the newly opened depths was intolerable, and at length the quick-eared Hawkins thought he heard a nasty, slopping sound down there. Everyone listened, and everyone was listening still when It lumbered slobberingly into sight and gropingly squeezed Its gelatinous green immensity through the black doorway into the tainted outside air of that poison city of madness.\\n\\nPoor Johansen\\'s handwriting almost gave out when he wrote of this. Of the six men who never reached the ship, he thinks two perished of pure fright in that accursed instant. The Thing cannot be described--there is no language for such abysms of shrieking and immemorial lunacy, such eldritch contradictions of all matter, force, and cosmic order. A mountain walked or stumbled. God! What wonder that across the earth a great architect went mad, and poor Wilcox raved with fever in that telepathic instant? The Thing of the idols, the green, sticky spawn of the stars, had awaked to claim his own. The stars were right again, and what an age-old cult had failed to do by design, a band of innocent sailors had done by accident. After vigintillions of years great Cthulhu was loose again, and ravening for delight.\\n\\nThree men were swept up by the flabby claws before anybody turned. God rest them, if there be any rest in the universe. They were Donovan, Guerrera, and Angstrom. Parker slipped as the other three were plunging frenziedly over endless vistas of green-crusted rock to the boat, and Johansen swears he was swallowed up by an angle of masonry which shouldn\\'t have been there; an angle which was acute, but behaved as if it were obtuse. So only Briden and Johansen reached the boat, and pulled desperately for the Alert as the mountainous monstrosity flopped down the slimy stones and hesitated, floundering at the edge of the water.\\n\\nSteam had not been suffered to go down entirely, despite the departure of all hands for the shore; and it was the work of only a few moments of feverish rushing up and down between wheel and engines to get the Alert under way. Slowly, amidst the distorted horrors of that indescribable scene, she began to churn the lethal waters; whilst on the masonry of that charnel shore that was not of earth the titan Thing from the stars slavered and gibbered like Polypheme cursing the fleeing ship of Odysseus. Then, bolder than the storied Cyclops, great Cthulhu slid greasily into the water and began to pursue with vast wave-raising strokes of cosmic potency. Briden looked back and went mad, laughing shrilly as he kept on laughing at intervals till death found him one night in the cabin whilst Johansen was wandering deliriously.\\n\\nBut Johansen had not given out yet. Knowing that the Thing could surely overtake the Alert until steam was fully up, he resolved on a desperate chance; and, setting the engine for full speed, ran lightning-like on deck and reversed the wheel. There was a mighty eddying and foaming in the noisome brine, and as the steam mounted higher and higher the brave Norwegian drove his vessel head on against the pursuing jelly which rose above the unclean froth like the stern of a daemon galleon. The awful squid-head with writhing feelers came nearly up to the bowsprit of the sturdy yacht, but Johansen drove on relentlessly. There was a bursting as of an exploding bladder, a slushy nastiness as of a cloven sunfish, a stench as of a thousand opened graves, and a sound that the chronicler could not put on paper. For an instant the ship was befouled by an acrid and blinding green cloud, and then there was only a venomous seething astern; where--God in heaven!--the scattered plasticity of that nameless sky-spawn was nebulously recombining in its hateful original form, whilst its distance widened every second as the Alert gained impetus from its mounting steam.\\n\\nThat was all. After that Johansen only brooded over the idol in the cabin and attended to a few matters of food for himself and the laughing maniac by his side. He did not try to navigate after the first bold flight, for the reaction had taken something out of his soul. Then came the storm of April 2nd, and a gathering of the clouds about his consciousness. There is a sense of spectral whirling through liquid gulfs of infinity, of dizzying rides through reeling universes on a comets tail, and of hysterical plunges from the pit to the moon and from the moon back again to the pit, all livened by a cachinnating chorus of the distorted, hilarious elder gods and the green, bat-winged mocking imps of Tartarus.\\n\\nOut of that dream came rescue--the Vigilant, the vice-admiralty court, the streets of Dunedin, and the long voyage back home to the old house by the Egeberg. He could not tell--they would think him mad. He would write of what he knew before death came, but his wife must not guess. Death would be a boon if only it could blot out the memories.\\n\\nThat was the document I read, and now I have placed it in the tin box beside the bas-relief and the papers of Professor Angell. With it shall go this record of mine--this test of my own sanity, wherein is pieced together that which I hope may never be pieced together again. I have looked upon all that the universe has to hold of horror, and even the skies of spring and the flowers of summer must ever afterward be poison to me. But I do not think my life will be long. As my uncle went, as poor Johansen went, so I shall go. I know too much, and the cult still lives.\\n\\nCthulhu still lives, too, I suppose, again in that chasm of stone which has shielded him since the sun was young. His accursed city is sunken once more, for the Vigilant sailed over the spot after the April storm; but his ministers on earth still bellow and prance and slay around idol-capped monoliths in lonely places. He must have been trapped by the sinking whilst within his black abyss, or else the world would by now be screaming with fright and frenzy. Who knows the end? What has risen may sink, and what has sunk may rise. Loathsomeness waits and dreams in the deep, and decay spreads over the tottering cities of men. A time will come--but I must not and cannot think! Let me pray that, if I do not survive this manuscript, my executors may put caution before audacity and see that it meets no other eye.\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE COLOUR OUT OF SPACE\\n\\nWest of Arkham the hills rise wild, and there are valleys with deep woods t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 13, 22, 41, 18, 50, 58, 50, 67, 57, 41, 50, 67, 26, 41, 50, 54,\n",
       "       41, 33, 27, 74, 18, 22,  9,  9, 56, 42, 45, 47, 41, 21, 61, 41, 74,\n",
       "       69, 40, 36,  5, 10, 41, 47, 36, 42, 41, 36, 15, 48, 48, 45, 41, 69,\n",
       "       15, 45, 42, 41,  8, 15, 48, 11, 66, 41,  5,  3, 11, 41, 47, 36, 42,\n",
       "       69, 42, 41,  5, 69, 42, 41, 23,  5, 48, 48, 42, 52, 45, 41,  8, 15,\n",
       "       47, 36, 41, 11, 42, 42, 37, 41,  8, 21, 21, 11, 45, 41, 47], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps \n",
    "    n_batches =  len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr =  arr[:n_batches*batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:] ,x[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[26 13 22 41 18 50 58 50 67 57]\n",
      " [41  8 36 15 38 36 41 69 42 45]\n",
      " [42 15  3  7 41 11 69  5 15  3]\n",
      " [22 23 42 69 52 41 47 69 15 61]\n",
      " [41 47 36  5 47 41 47 69 42 42]\n",
      " [ 5 38 40  8 21 21 11  9  9 46]\n",
      " [23 42 41  7 48 15 10 37 45 42]\n",
      " [11 41 47 36  5 47 41 35  5 47]\n",
      " [47 36 41 10  5 40 42 45 41 23]\n",
      " [48 15 23 42 41 47 36 42 41  3]]\n",
      "\n",
      "y\n",
      " [[ 13.  22.  41.  18.  50.  58.  50.  67.  57.  41.]\n",
      " [  8.  36.  15.  38.  36.  41.  69.  42.  45.  42.]\n",
      " [ 15.   3.   7.  41.  11.  69.   5.  15.   3.  42.]\n",
      " [ 23.  42.  69.  52.  41.  47.  69.  15.  61.  48.]\n",
      " [ 47.  36.   5.  47.  41.  47.  69.  42.  42.  41.]\n",
      " [ 38.  40.   8.  21.  21.  11.   9.   9.  46.   2.]\n",
      " [ 42.  41.   7.  48.  15.  10.  37.  45.  42.  45.]\n",
      " [ 41.  47.  36.   5.  47.  41.  35.   5.  47.  70.]\n",
      " [ 36.  41.  10.   5.  40.  42.  45.  41.  23.  15.]\n",
      " [ 15.  23.  42.  41.  47.  36.  42.  41.   3.   5.]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26, 13, 22, 41, 18, 50, 58, 50, 67, 57, 41, 50, 67, 26, 41, 50, 54,\n",
       "        41, 33, 27, 74, 18, 22,  9,  9, 56, 42, 45, 47, 41, 21, 61, 41, 74,\n",
       "        69, 40, 36,  5, 10, 41, 47, 36, 42, 41, 36, 15, 48, 48, 45, 41],\n",
       "       [41,  8, 36, 15, 38, 36, 41, 69, 42, 45, 42, 10, 35, 48, 42, 11, 41,\n",
       "        45, 21, 10, 42, 41, 21, 61, 41, 47, 36, 42, 41, 35,  5,  3, 11, 45,\n",
       "        41, 15,  3, 41, 47, 36, 42, 41, 10, 42, 47, 42, 21, 69, 71, 45],\n",
       "       [42, 15,  3,  7, 41, 11, 69,  5, 15,  3, 42, 11, 41, 21, 61, 41, 45,\n",
       "        21, 10, 42, 47, 36, 15,  3,  7, 70, 70, 45, 21, 10, 42, 47, 36, 15,\n",
       "         3,  7, 41,  8,  5, 45, 41, 61,  5, 45, 47, 42,  3, 15,  3,  7],\n",
       "       [22, 23, 42, 69, 52, 41, 47, 69, 15, 61, 48, 42, 41, 21, 61, 41, 47,\n",
       "        36, 42, 41, 45, 38, 42,  3, 42, 41, 35, 20, 69,  3, 42, 11, 41, 15,\n",
       "        47, 45, 42, 48, 61, 41, 15,  3, 47, 21, 41, 36, 15, 45, 41, 35],\n",
       "       [41, 47, 36,  5, 47, 41, 47, 69, 42, 42, 41, 47, 21, 37, 41, 36, 42,\n",
       "        15,  7, 36, 47, 41,  5, 41, 47, 36, 21, 20, 45,  5,  3, 11, 41, 47,\n",
       "        15,  3, 52, 41, 37, 21, 15,  3, 47, 45, 41, 21, 61, 41, 61,  5],\n",
       "       [ 5, 38, 40,  8, 21, 21, 11,  9,  9, 46,  2, 41, 26, 36, 42, 41, 13,\n",
       "        21, 69, 69, 21, 69, 41, 46,  3, 41, 18, 48,  5, 52,  9,  9, 26, 36,\n",
       "        42, 41, 10, 21, 45, 47, 41, 10, 42, 69, 38, 15, 61, 20, 48, 41],\n",
       "       [23, 42, 41,  7, 48, 15, 10, 37, 45, 42, 45, 41, 21, 61, 41, 45, 47,\n",
       "        69,  5,  3,  7, 42, 41, 48,  5,  3, 11, 45, 38,  5, 37, 42, 45, 66,\n",
       "        41,  5,  3, 11, 41, 15,  3, 41, 21,  3, 42, 41, 38,  5, 45, 42],\n",
       "       [11, 41, 47, 36,  5, 47, 41, 35,  5, 47, 70,  8, 15,  3,  7, 42, 11,\n",
       "        41, 11, 42, 23, 15, 48, 45, 41, 61, 48, 42,  8, 41, 20, 37, 41, 21,\n",
       "        20, 47, 41, 21, 61, 41, 38,  5, 23, 42, 69,  3, 45, 41, 15,  3],\n",
       "       [47, 36, 41, 10,  5, 40, 42, 45, 41, 23, 15, 45, 15, 35, 48, 42, 41,\n",
       "        15,  3, 41, 23, 42, 69, 45, 42, 41,  5,  3, 11, 41, 15,  3, 41, 37,\n",
       "         5, 15,  3, 47, 15,  3,  7,  2,  9,  9, 73,  5, 69, 40, 66, 41],\n",
       "       [48, 15, 23, 42, 41, 47, 36, 42, 41,  3,  5, 10, 42, 41, 21, 61, 41,\n",
       "        50, 45, 48, 21, 41, 11, 20, 69, 15,  3,  7, 41,  5, 48, 48, 41, 47,\n",
       "        36, 42, 41, 38, 42,  3, 47, 20, 69, 15, 42, 45, 41, 47, 36,  5]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.,  22.,  41.,  18.,  50.,  58.,  50.,  67.,  57.,  41.,  50.,\n",
       "         67.,  26.,  41.,  50.,  54.,  41.,  33.,  27.,  74.,  18.,  22.,\n",
       "          9.,   9.,  56.,  42.,  45.,  47.,  41.,  21.,  61.,  41.,  74.,\n",
       "         69.,  40.,  36.,   5.,  10.,  41.,  47.,  36.,  42.,  41.,  36.,\n",
       "         15.,  48.,  48.,  45.,  41.,  26.],\n",
       "       [  8.,  36.,  15.,  38.,  36.,  41.,  69.,  42.,  45.,  42.,  10.,\n",
       "         35.,  48.,  42.,  11.,  41.,  45.,  21.,  10.,  42.,  41.,  21.,\n",
       "         61.,  41.,  47.,  36.,  42.,  41.,  35.,   5.,   3.,  11.,  45.,\n",
       "         41.,  15.,   3.,  41.,  47.,  36.,  42.,  41.,  10.,  42.,  47.,\n",
       "         42.,  21.,  69.,  71.,  45.,  41.],\n",
       "       [ 15.,   3.,   7.,  41.,  11.,  69.,   5.,  15.,   3.,  42.,  11.,\n",
       "         41.,  21.,  61.,  41.,  45.,  21.,  10.,  42.,  47.,  36.,  15.,\n",
       "          3.,   7.,  70.,  70.,  45.,  21.,  10.,  42.,  47.,  36.,  15.,\n",
       "          3.,   7.,  41.,   8.,   5.,  45.,  41.,  61.,   5.,  45.,  47.,\n",
       "         42.,   3.,  15.,   3.,   7.,  42.],\n",
       "       [ 23.,  42.,  69.,  52.,  41.,  47.,  69.,  15.,  61.,  48.,  42.,\n",
       "         41.,  21.,  61.,  41.,  47.,  36.,  42.,  41.,  45.,  38.,  42.,\n",
       "          3.,  42.,  41.,  35.,  20.,  69.,   3.,  42.,  11.,  41.,  15.,\n",
       "         47.,  45.,  42.,  48.,  61.,  41.,  15.,   3.,  47.,  21.,  41.,\n",
       "         36.,  15.,  45.,  41.,  35.,  22.],\n",
       "       [ 47.,  36.,   5.,  47.,  41.,  47.,  69.,  42.,  42.,  41.,  47.,\n",
       "         21.,  37.,  41.,  36.,  42.,  15.,   7.,  36.,  47.,  41.,   5.,\n",
       "         41.,  47.,  36.,  21.,  20.,  45.,   5.,   3.,  11.,  41.,  47.,\n",
       "         15.,   3.,  52.,  41.,  37.,  21.,  15.,   3.,  47.,  45.,  41.,\n",
       "         21.,  61.,  41.,  61.,   5.,  41.],\n",
       "       [ 38.,  40.,   8.,  21.,  21.,  11.,   9.,   9.,  46.,   2.,  41.,\n",
       "         26.,  36.,  42.,  41.,  13.,  21.,  69.,  69.,  21.,  69.,  41.,\n",
       "         46.,   3.,  41.,  18.,  48.,   5.,  52.,   9.,   9.,  26.,  36.,\n",
       "         42.,  41.,  10.,  21.,  45.,  47.,  41.,  10.,  42.,  69.,  38.,\n",
       "         15.,  61.,  20.,  48.,  41.,   5.],\n",
       "       [ 42.,  41.,   7.,  48.,  15.,  10.,  37.,  45.,  42.,  45.,  41.,\n",
       "         21.,  61.,  41.,  45.,  47.,  69.,   5.,   3.,   7.,  42.,  41.,\n",
       "         48.,   5.,   3.,  11.,  45.,  38.,   5.,  37.,  42.,  45.,  66.,\n",
       "         41.,   5.,   3.,  11.,  41.,  15.,   3.,  41.,  21.,   3.,  42.,\n",
       "         41.,  38.,   5.,  45.,  42.,  23.],\n",
       "       [ 41.,  47.,  36.,   5.,  47.,  41.,  35.,   5.,  47.,  70.,   8.,\n",
       "         15.,   3.,   7.,  42.,  11.,  41.,  11.,  42.,  23.,  15.,  48.,\n",
       "         45.,  41.,  61.,  48.,  42.,   8.,  41.,  20.,  37.,  41.,  21.,\n",
       "         20.,  47.,  41.,  21.,  61.,  41.,  38.,   5.,  23.,  42.,  69.,\n",
       "          3.,  45.,  41.,  15.,   3.,  11.],\n",
       "       [ 36.,  41.,  10.,   5.,  40.,  42.,  45.,  41.,  23.,  15.,  45.,\n",
       "         15.,  35.,  48.,  42.,  41.,  15.,   3.,  41.,  23.,  42.,  69.,\n",
       "         45.,  42.,  41.,   5.,   3.,  11.,  41.,  15.,   3.,  41.,  37.,\n",
       "          5.,  15.,   3.,  47.,  15.,   3.,   7.,   2.,   9.,   9.,  73.,\n",
       "          5.,  69.,  40.,  66.,  41.,  47.],\n",
       "       [ 15.,  23.,  42.,  41.,  47.,  36.,  42.,  41.,   3.,   5.,  10.,\n",
       "         42.,  41.,  21.,  61.,  41.,  50.,  45.,  48.,  21.,  41.,  11.,\n",
       "         20.,  69.,  15.,   3.,   7.,  41.,   5.,  48.,  48.,  41.,  47.,\n",
       "         36.,  42.,  41.,  38.,  42.,   3.,  47.,  20.,  69.,  15.,  42.,\n",
       "         45.,  41.,  47.,  36.,   5.,  48.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"targets\")\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. For example,\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow will create different weight matrices for all `cell` objects. Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "> **Exercise:** Below, implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    # Add dropout to the cell outputs\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.BasicLSTMCell(lstm_size)) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output,axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros([out_size]))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits =  tf.add(tf.matmul(x,softmax_w),softmax_b) \n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits,name =\"out\")\n",
    "    print(out)\n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size,num_steps)\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs,lstm_size,num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits,self.targets,lstm_size,num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 500          # Number of sequence steps per batch\n",
    "lstm_size = 600         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(50000, 77), dtype=float32)\n",
      "Epoch: 1/1000...  Training Step: 1...  Training loss: 4.3408...  1.0878 sec/batch\n",
      "Epoch: 1/1000...  Training Step: 2...  Training loss: 4.2082...  1.0007 sec/batch\n",
      "Epoch: 2/1000...  Training Step: 3...  Training loss: 4.1279...  1.0007 sec/batch\n",
      "Epoch: 2/1000...  Training Step: 4...  Training loss: 3.7121...  1.0017 sec/batch\n",
      "Epoch: 3/1000...  Training Step: 5...  Training loss: 3.5425...  1.0019 sec/batch\n",
      "Epoch: 3/1000...  Training Step: 6...  Training loss: 3.3515...  1.0036 sec/batch\n",
      "Epoch: 4/1000...  Training Step: 7...  Training loss: 3.1737...  1.0029 sec/batch\n",
      "Epoch: 4/1000...  Training Step: 8...  Training loss: 3.0954...  1.0030 sec/batch\n",
      "Epoch: 5/1000...  Training Step: 9...  Training loss: 3.1594...  1.0037 sec/batch\n",
      "Epoch: 5/1000...  Training Step: 10...  Training loss: 3.0825...  1.0034 sec/batch\n",
      "Epoch: 6/1000...  Training Step: 11...  Training loss: 3.0662...  1.0035 sec/batch\n",
      "Epoch: 6/1000...  Training Step: 12...  Training loss: 3.0460...  1.0035 sec/batch\n",
      "Epoch: 7/1000...  Training Step: 13...  Training loss: 3.0658...  1.0027 sec/batch\n",
      "Epoch: 7/1000...  Training Step: 14...  Training loss: 3.0439...  1.0031 sec/batch\n",
      "Epoch: 8/1000...  Training Step: 15...  Training loss: 3.0490...  1.0029 sec/batch\n",
      "Epoch: 8/1000...  Training Step: 16...  Training loss: 3.0250...  1.0031 sec/batch\n",
      "Epoch: 9/1000...  Training Step: 17...  Training loss: 3.0376...  1.0031 sec/batch\n",
      "Epoch: 9/1000...  Training Step: 18...  Training loss: 3.0189...  1.0032 sec/batch\n",
      "Epoch: 10/1000...  Training Step: 19...  Training loss: 3.0309...  1.0036 sec/batch\n",
      "Epoch: 10/1000...  Training Step: 20...  Training loss: 3.0119...  1.0052 sec/batch\n",
      "Epoch: 11/1000...  Training Step: 21...  Training loss: 3.0254...  1.0055 sec/batch\n",
      "Epoch: 11/1000...  Training Step: 22...  Training loss: 3.0102...  1.0030 sec/batch\n",
      "Epoch: 12/1000...  Training Step: 23...  Training loss: 3.0231...  1.0022 sec/batch\n",
      "Epoch: 12/1000...  Training Step: 24...  Training loss: 3.0056...  1.0448 sec/batch\n",
      "Epoch: 13/1000...  Training Step: 25...  Training loss: 3.0159...  1.0299 sec/batch\n",
      "Epoch: 13/1000...  Training Step: 26...  Training loss: 2.9991...  1.0432 sec/batch\n",
      "Epoch: 14/1000...  Training Step: 27...  Training loss: 3.0129...  1.0032 sec/batch\n",
      "Epoch: 14/1000...  Training Step: 28...  Training loss: 2.9990...  1.0032 sec/batch\n",
      "Epoch: 15/1000...  Training Step: 29...  Training loss: 3.0137...  1.0044 sec/batch\n",
      "Epoch: 15/1000...  Training Step: 30...  Training loss: 2.9980...  1.0044 sec/batch\n",
      "Epoch: 16/1000...  Training Step: 31...  Training loss: 3.0094...  1.0043 sec/batch\n",
      "Epoch: 16/1000...  Training Step: 32...  Training loss: 2.9932...  1.0045 sec/batch\n",
      "Epoch: 17/1000...  Training Step: 33...  Training loss: 3.0048...  1.0049 sec/batch\n",
      "Epoch: 17/1000...  Training Step: 34...  Training loss: 2.9916...  1.0037 sec/batch\n",
      "Epoch: 18/1000...  Training Step: 35...  Training loss: 3.0035...  1.0052 sec/batch\n",
      "Epoch: 18/1000...  Training Step: 36...  Training loss: 2.9901...  1.0048 sec/batch\n",
      "Epoch: 19/1000...  Training Step: 37...  Training loss: 3.0008...  1.0054 sec/batch\n",
      "Epoch: 19/1000...  Training Step: 38...  Training loss: 2.9865...  1.0051 sec/batch\n",
      "Epoch: 20/1000...  Training Step: 39...  Training loss: 2.9981...  1.0049 sec/batch\n",
      "Epoch: 20/1000...  Training Step: 40...  Training loss: 2.9842...  1.0054 sec/batch\n",
      "Epoch: 21/1000...  Training Step: 41...  Training loss: 2.9961...  1.0049 sec/batch\n",
      "Epoch: 21/1000...  Training Step: 42...  Training loss: 2.9820...  1.0049 sec/batch\n",
      "Epoch: 22/1000...  Training Step: 43...  Training loss: 2.9928...  1.0050 sec/batch\n",
      "Epoch: 22/1000...  Training Step: 44...  Training loss: 2.9788...  1.0055 sec/batch\n",
      "Epoch: 23/1000...  Training Step: 45...  Training loss: 2.9889...  1.0054 sec/batch\n",
      "Epoch: 23/1000...  Training Step: 46...  Training loss: 2.9760...  1.0048 sec/batch\n",
      "Epoch: 24/1000...  Training Step: 47...  Training loss: 2.9858...  1.0044 sec/batch\n",
      "Epoch: 24/1000...  Training Step: 48...  Training loss: 2.9727...  1.0049 sec/batch\n",
      "Epoch: 25/1000...  Training Step: 49...  Training loss: 2.9818...  1.0045 sec/batch\n",
      "Epoch: 25/1000...  Training Step: 50...  Training loss: 2.9681...  1.0048 sec/batch\n",
      "Epoch: 26/1000...  Training Step: 51...  Training loss: 2.9771...  1.0117 sec/batch\n",
      "Epoch: 26/1000...  Training Step: 52...  Training loss: 2.9630...  1.0045 sec/batch\n",
      "Epoch: 27/1000...  Training Step: 53...  Training loss: 2.9709...  1.0120 sec/batch\n",
      "Epoch: 27/1000...  Training Step: 54...  Training loss: 2.9544...  1.0059 sec/batch\n",
      "Epoch: 28/1000...  Training Step: 55...  Training loss: 2.9839...  1.0053 sec/batch\n",
      "Epoch: 28/1000...  Training Step: 56...  Training loss: 3.0784...  1.0046 sec/batch\n",
      "Epoch: 29/1000...  Training Step: 57...  Training loss: 2.9762...  1.0048 sec/batch\n",
      "Epoch: 29/1000...  Training Step: 58...  Training loss: 2.9641...  1.0049 sec/batch\n",
      "Epoch: 30/1000...  Training Step: 59...  Training loss: 2.9828...  1.0050 sec/batch\n",
      "Epoch: 30/1000...  Training Step: 60...  Training loss: 2.9743...  1.0049 sec/batch\n",
      "Epoch: 31/1000...  Training Step: 61...  Training loss: 2.9766...  1.0047 sec/batch\n",
      "Epoch: 31/1000...  Training Step: 62...  Training loss: 2.9589...  1.0054 sec/batch\n",
      "Epoch: 32/1000...  Training Step: 63...  Training loss: 2.9673...  1.0050 sec/batch\n",
      "Epoch: 32/1000...  Training Step: 64...  Training loss: 2.9553...  1.0043 sec/batch\n",
      "Epoch: 33/1000...  Training Step: 65...  Training loss: 2.9672...  1.0059 sec/batch\n",
      "Epoch: 33/1000...  Training Step: 66...  Training loss: 2.9511...  1.0064 sec/batch\n",
      "Epoch: 34/1000...  Training Step: 67...  Training loss: 2.9593...  1.0063 sec/batch\n",
      "Epoch: 34/1000...  Training Step: 68...  Training loss: 2.9450...  1.0066 sec/batch\n",
      "Epoch: 35/1000...  Training Step: 69...  Training loss: 2.9524...  1.0078 sec/batch\n",
      "Epoch: 35/1000...  Training Step: 70...  Training loss: 2.9386...  1.0082 sec/batch\n",
      "Epoch: 36/1000...  Training Step: 71...  Training loss: 2.9473...  1.0082 sec/batch\n",
      "Epoch: 36/1000...  Training Step: 72...  Training loss: 2.9337...  1.0084 sec/batch\n",
      "Epoch: 37/1000...  Training Step: 73...  Training loss: 3.0234...  1.0064 sec/batch\n",
      "Epoch: 37/1000...  Training Step: 74...  Training loss: 2.9969...  1.0078 sec/batch\n",
      "Epoch: 38/1000...  Training Step: 75...  Training loss: 2.9844...  1.0083 sec/batch\n",
      "Epoch: 38/1000...  Training Step: 76...  Training loss: 2.9203...  1.0069 sec/batch\n",
      "Epoch: 39/1000...  Training Step: 77...  Training loss: 2.9220...  1.0066 sec/batch\n",
      "Epoch: 39/1000...  Training Step: 78...  Training loss: 2.8918...  1.0067 sec/batch\n",
      "Epoch: 40/1000...  Training Step: 79...  Training loss: 2.9601...  1.0066 sec/batch\n",
      "Epoch: 40/1000...  Training Step: 80...  Training loss: 3.0725...  1.0076 sec/batch\n",
      "Epoch: 41/1000...  Training Step: 81...  Training loss: 2.9386...  1.0064 sec/batch\n",
      "Epoch: 41/1000...  Training Step: 82...  Training loss: 2.9365...  1.0082 sec/batch\n",
      "Epoch: 42/1000...  Training Step: 83...  Training loss: 2.9432...  1.0067 sec/batch\n",
      "Epoch: 42/1000...  Training Step: 84...  Training loss: 2.9223...  1.0079 sec/batch\n",
      "Epoch: 43/1000...  Training Step: 85...  Training loss: 2.9222...  1.0080 sec/batch\n",
      "Epoch: 43/1000...  Training Step: 86...  Training loss: 2.9069...  1.0075 sec/batch\n",
      "Epoch: 44/1000...  Training Step: 87...  Training loss: 2.9207...  1.0081 sec/batch\n",
      "Epoch: 44/1000...  Training Step: 88...  Training loss: 2.9070...  1.0069 sec/batch\n",
      "Epoch: 45/1000...  Training Step: 89...  Training loss: 2.9127...  1.0079 sec/batch\n",
      "Epoch: 45/1000...  Training Step: 90...  Training loss: 2.8910...  1.0072 sec/batch\n",
      "Epoch: 46/1000...  Training Step: 91...  Training loss: 2.8976...  1.0075 sec/batch\n",
      "Epoch: 46/1000...  Training Step: 92...  Training loss: 2.8833...  1.0073 sec/batch\n",
      "Epoch: 47/1000...  Training Step: 93...  Training loss: 2.8915...  1.0069 sec/batch\n",
      "Epoch: 47/1000...  Training Step: 94...  Training loss: 2.8755...  1.0084 sec/batch\n",
      "Epoch: 48/1000...  Training Step: 95...  Training loss: 2.8793...  1.0092 sec/batch\n",
      "Epoch: 48/1000...  Training Step: 96...  Training loss: 2.8592...  1.0082 sec/batch\n",
      "Epoch: 49/1000...  Training Step: 97...  Training loss: 2.8657...  1.0084 sec/batch\n",
      "Epoch: 49/1000...  Training Step: 98...  Training loss: 2.8508...  1.0082 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/1000...  Training Step: 99...  Training loss: 2.8565...  1.0080 sec/batch\n",
      "Epoch: 50/1000...  Training Step: 100...  Training loss: 2.8350...  1.0085 sec/batch\n",
      "Epoch: 51/1000...  Training Step: 101...  Training loss: 2.8367...  1.0085 sec/batch\n",
      "Epoch: 51/1000...  Training Step: 102...  Training loss: 2.8177...  1.0082 sec/batch\n",
      "Epoch: 52/1000...  Training Step: 103...  Training loss: 2.8207...  1.0081 sec/batch\n",
      "Epoch: 52/1000...  Training Step: 104...  Training loss: 2.7981...  1.0078 sec/batch\n",
      "Epoch: 53/1000...  Training Step: 105...  Training loss: 2.7869...  1.0080 sec/batch\n",
      "Epoch: 53/1000...  Training Step: 106...  Training loss: 2.7884...  1.0083 sec/batch\n",
      "Epoch: 54/1000...  Training Step: 107...  Training loss: 3.0576...  1.0082 sec/batch\n",
      "Epoch: 54/1000...  Training Step: 108...  Training loss: 2.8580...  1.0086 sec/batch\n",
      "Epoch: 55/1000...  Training Step: 109...  Training loss: 2.8351...  1.0080 sec/batch\n",
      "Epoch: 55/1000...  Training Step: 110...  Training loss: 2.7999...  1.0078 sec/batch\n",
      "Epoch: 56/1000...  Training Step: 111...  Training loss: 2.8232...  1.0082 sec/batch\n",
      "Epoch: 56/1000...  Training Step: 112...  Training loss: 2.8085...  1.0070 sec/batch\n",
      "Epoch: 57/1000...  Training Step: 113...  Training loss: 2.7897...  1.0085 sec/batch\n",
      "Epoch: 57/1000...  Training Step: 114...  Training loss: 2.7653...  1.0066 sec/batch\n",
      "Epoch: 58/1000...  Training Step: 115...  Training loss: 2.7735...  1.0083 sec/batch\n",
      "Epoch: 58/1000...  Training Step: 116...  Training loss: 2.7658...  1.0080 sec/batch\n",
      "Epoch: 59/1000...  Training Step: 117...  Training loss: 2.7694...  1.0079 sec/batch\n",
      "Epoch: 59/1000...  Training Step: 118...  Training loss: 2.7374...  1.0064 sec/batch\n",
      "Epoch: 60/1000...  Training Step: 119...  Training loss: 2.7304...  1.0080 sec/batch\n",
      "Epoch: 60/1000...  Training Step: 120...  Training loss: 2.7180...  1.0070 sec/batch\n",
      "Epoch: 61/1000...  Training Step: 121...  Training loss: 2.7242...  1.0082 sec/batch\n",
      "Epoch: 61/1000...  Training Step: 122...  Training loss: 2.7084...  1.0078 sec/batch\n",
      "Epoch: 62/1000...  Training Step: 123...  Training loss: 2.7018...  1.0083 sec/batch\n",
      "Epoch: 62/1000...  Training Step: 124...  Training loss: 2.6777...  1.0070 sec/batch\n",
      "Epoch: 63/1000...  Training Step: 125...  Training loss: 2.6735...  1.0088 sec/batch\n",
      "Epoch: 63/1000...  Training Step: 126...  Training loss: 2.6562...  1.0081 sec/batch\n",
      "Epoch: 64/1000...  Training Step: 127...  Training loss: 2.6554...  1.0078 sec/batch\n",
      "Epoch: 64/1000...  Training Step: 128...  Training loss: 2.6307...  1.0080 sec/batch\n",
      "Epoch: 65/1000...  Training Step: 129...  Training loss: 2.6138...  1.0083 sec/batch\n",
      "Epoch: 65/1000...  Training Step: 130...  Training loss: 2.5999...  1.0082 sec/batch\n",
      "Epoch: 66/1000...  Training Step: 131...  Training loss: 2.9707...  1.0083 sec/batch\n",
      "Epoch: 66/1000...  Training Step: 132...  Training loss: 2.6625...  1.0079 sec/batch\n",
      "Epoch: 67/1000...  Training Step: 133...  Training loss: 2.6430...  1.0072 sec/batch\n",
      "Epoch: 67/1000...  Training Step: 134...  Training loss: 2.6595...  1.0085 sec/batch\n",
      "Epoch: 68/1000...  Training Step: 135...  Training loss: 2.6557...  1.0078 sec/batch\n",
      "Epoch: 68/1000...  Training Step: 136...  Training loss: 2.6224...  1.0091 sec/batch\n",
      "Epoch: 69/1000...  Training Step: 137...  Training loss: 2.6189...  1.0081 sec/batch\n",
      "Epoch: 69/1000...  Training Step: 138...  Training loss: 2.6097...  1.0069 sec/batch\n",
      "Epoch: 70/1000...  Training Step: 139...  Training loss: 2.6031...  1.0086 sec/batch\n",
      "Epoch: 70/1000...  Training Step: 140...  Training loss: 2.5802...  1.0088 sec/batch\n",
      "Epoch: 71/1000...  Training Step: 141...  Training loss: 2.5869...  1.0073 sec/batch\n",
      "Epoch: 71/1000...  Training Step: 142...  Training loss: 2.5761...  1.0072 sec/batch\n",
      "Epoch: 72/1000...  Training Step: 143...  Training loss: 2.5652...  1.0079 sec/batch\n",
      "Epoch: 72/1000...  Training Step: 144...  Training loss: 2.5460...  1.0084 sec/batch\n",
      "Epoch: 73/1000...  Training Step: 145...  Training loss: 2.5438...  1.0082 sec/batch\n",
      "Epoch: 73/1000...  Training Step: 146...  Training loss: 2.5349...  1.0073 sec/batch\n",
      "Epoch: 74/1000...  Training Step: 147...  Training loss: 2.5361...  1.0080 sec/batch\n",
      "Epoch: 74/1000...  Training Step: 148...  Training loss: 2.5164...  1.0081 sec/batch\n",
      "Epoch: 75/1000...  Training Step: 149...  Training loss: 2.5087...  1.0075 sec/batch\n",
      "Epoch: 75/1000...  Training Step: 150...  Training loss: 2.4945...  1.0072 sec/batch\n",
      "Epoch: 76/1000...  Training Step: 151...  Training loss: 2.4955...  1.0075 sec/batch\n",
      "Epoch: 76/1000...  Training Step: 152...  Training loss: 2.4861...  1.0075 sec/batch\n",
      "Epoch: 77/1000...  Training Step: 153...  Training loss: 2.4770...  1.0069 sec/batch\n",
      "Epoch: 77/1000...  Training Step: 154...  Training loss: 2.4797...  1.0068 sec/batch\n",
      "Epoch: 78/1000...  Training Step: 155...  Training loss: 2.4867...  1.0072 sec/batch\n",
      "Epoch: 78/1000...  Training Step: 156...  Training loss: 2.4731...  1.0074 sec/batch\n",
      "Epoch: 79/1000...  Training Step: 157...  Training loss: 2.4713...  1.0074 sec/batch\n",
      "Epoch: 79/1000...  Training Step: 158...  Training loss: 2.4606...  1.0075 sec/batch\n",
      "Epoch: 80/1000...  Training Step: 159...  Training loss: 2.4610...  1.0079 sec/batch\n",
      "Epoch: 80/1000...  Training Step: 160...  Training loss: 2.4573...  1.0095 sec/batch\n",
      "Epoch: 81/1000...  Training Step: 161...  Training loss: 2.4553...  1.0118 sec/batch\n",
      "Epoch: 81/1000...  Training Step: 162...  Training loss: 2.4384...  1.0120 sec/batch\n",
      "Epoch: 82/1000...  Training Step: 163...  Training loss: 2.4478...  1.0113 sec/batch\n",
      "Epoch: 82/1000...  Training Step: 164...  Training loss: 2.4331...  1.0098 sec/batch\n",
      "Epoch: 83/1000...  Training Step: 165...  Training loss: 2.4380...  1.0109 sec/batch\n",
      "Epoch: 83/1000...  Training Step: 166...  Training loss: 2.4248...  1.0097 sec/batch\n",
      "Epoch: 84/1000...  Training Step: 167...  Training loss: 2.4288...  1.0099 sec/batch\n",
      "Epoch: 84/1000...  Training Step: 168...  Training loss: 2.4174...  1.0114 sec/batch\n",
      "Epoch: 85/1000...  Training Step: 169...  Training loss: 2.4201...  1.0106 sec/batch\n",
      "Epoch: 85/1000...  Training Step: 170...  Training loss: 2.4075...  1.0167 sec/batch\n",
      "Epoch: 86/1000...  Training Step: 171...  Training loss: 2.4152...  1.0109 sec/batch\n",
      "Epoch: 86/1000...  Training Step: 172...  Training loss: 2.3998...  1.0183 sec/batch\n",
      "Epoch: 87/1000...  Training Step: 173...  Training loss: 2.4056...  1.0109 sec/batch\n",
      "Epoch: 87/1000...  Training Step: 174...  Training loss: 2.3943...  1.0101 sec/batch\n",
      "Epoch: 88/1000...  Training Step: 175...  Training loss: 2.3998...  1.0102 sec/batch\n",
      "Epoch: 88/1000...  Training Step: 176...  Training loss: 2.3868...  1.0120 sec/batch\n",
      "Epoch: 89/1000...  Training Step: 177...  Training loss: 2.3910...  1.0108 sec/batch\n",
      "Epoch: 89/1000...  Training Step: 178...  Training loss: 2.3800...  1.0097 sec/batch\n",
      "Epoch: 90/1000...  Training Step: 179...  Training loss: 2.3842...  1.0106 sec/batch\n",
      "Epoch: 90/1000...  Training Step: 180...  Training loss: 2.3765...  1.0104 sec/batch\n",
      "Epoch: 91/1000...  Training Step: 181...  Training loss: 2.3812...  1.0099 sec/batch\n",
      "Epoch: 91/1000...  Training Step: 182...  Training loss: 2.3664...  1.0116 sec/batch\n",
      "Epoch: 92/1000...  Training Step: 183...  Training loss: 2.3701...  1.0115 sec/batch\n",
      "Epoch: 92/1000...  Training Step: 184...  Training loss: 2.3612...  1.0108 sec/batch\n",
      "Epoch: 93/1000...  Training Step: 185...  Training loss: 2.3633...  1.0105 sec/batch\n",
      "Epoch: 93/1000...  Training Step: 186...  Training loss: 2.3595...  1.0112 sec/batch\n",
      "Epoch: 94/1000...  Training Step: 187...  Training loss: 2.3600...  1.0106 sec/batch\n",
      "Epoch: 94/1000...  Training Step: 188...  Training loss: 2.3481...  1.0113 sec/batch\n",
      "Epoch: 95/1000...  Training Step: 189...  Training loss: 2.3491...  1.0105 sec/batch\n",
      "Epoch: 95/1000...  Training Step: 190...  Training loss: 2.3447...  1.0107 sec/batch\n",
      "Epoch: 96/1000...  Training Step: 191...  Training loss: 2.3454...  1.0095 sec/batch\n",
      "Epoch: 96/1000...  Training Step: 192...  Training loss: 2.3352...  1.0108 sec/batch\n",
      "Epoch: 97/1000...  Training Step: 193...  Training loss: 2.3378...  1.0108 sec/batch\n",
      "Epoch: 97/1000...  Training Step: 194...  Training loss: 2.3398...  1.0094 sec/batch\n",
      "Epoch: 98/1000...  Training Step: 195...  Training loss: 2.3771...  1.0105 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/1000...  Training Step: 196...  Training loss: 2.3413...  1.0107 sec/batch\n",
      "Epoch: 99/1000...  Training Step: 197...  Training loss: 2.3448...  1.0105 sec/batch\n",
      "Epoch: 99/1000...  Training Step: 198...  Training loss: 2.3298...  1.0110 sec/batch\n",
      "Epoch: 100/1000...  Training Step: 199...  Training loss: 2.3454...  1.0111 sec/batch\n",
      "Epoch: 100/1000...  Training Step: 200...  Training loss: 2.3284...  1.0109 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 101/1000...  Training Step: 201...  Training loss: 2.3313...  1.0094 sec/batch\n",
      "Epoch: 101/1000...  Training Step: 202...  Training loss: 2.3283...  1.0112 sec/batch\n",
      "Epoch: 102/1000...  Training Step: 203...  Training loss: 2.3229...  1.0106 sec/batch\n",
      "Epoch: 102/1000...  Training Step: 204...  Training loss: 2.3189...  1.0109 sec/batch\n",
      "Epoch: 103/1000...  Training Step: 205...  Training loss: 2.3200...  1.0104 sec/batch\n",
      "Epoch: 103/1000...  Training Step: 206...  Training loss: 2.3087...  1.0106 sec/batch\n",
      "Epoch: 104/1000...  Training Step: 207...  Training loss: 2.3130...  1.0106 sec/batch\n",
      "Epoch: 104/1000...  Training Step: 208...  Training loss: 2.3042...  1.0116 sec/batch\n",
      "Epoch: 105/1000...  Training Step: 209...  Training loss: 2.3041...  1.0110 sec/batch\n",
      "Epoch: 105/1000...  Training Step: 210...  Training loss: 2.2978...  1.0104 sec/batch\n",
      "Epoch: 106/1000...  Training Step: 211...  Training loss: 2.2959...  1.0104 sec/batch\n",
      "Epoch: 106/1000...  Training Step: 212...  Training loss: 2.2905...  1.0102 sec/batch\n",
      "Epoch: 107/1000...  Training Step: 213...  Training loss: 2.2927...  1.0094 sec/batch\n",
      "Epoch: 107/1000...  Training Step: 214...  Training loss: 2.2845...  1.0108 sec/batch\n",
      "Epoch: 108/1000...  Training Step: 215...  Training loss: 2.2818...  1.0110 sec/batch\n",
      "Epoch: 108/1000...  Training Step: 216...  Training loss: 2.2750...  1.0101 sec/batch\n",
      "Epoch: 109/1000...  Training Step: 217...  Training loss: 2.2746...  1.0117 sec/batch\n",
      "Epoch: 109/1000...  Training Step: 218...  Training loss: 2.2726...  1.0109 sec/batch\n",
      "Epoch: 110/1000...  Training Step: 219...  Training loss: 2.2961...  1.0112 sec/batch\n",
      "Epoch: 110/1000...  Training Step: 220...  Training loss: 2.3188...  1.0113 sec/batch\n",
      "Epoch: 111/1000...  Training Step: 221...  Training loss: 2.3271...  1.0094 sec/batch\n",
      "Epoch: 111/1000...  Training Step: 222...  Training loss: 2.2811...  1.0108 sec/batch\n",
      "Epoch: 112/1000...  Training Step: 223...  Training loss: 2.2858...  1.0105 sec/batch\n",
      "Epoch: 112/1000...  Training Step: 224...  Training loss: 2.2971...  1.0107 sec/batch\n",
      "Epoch: 113/1000...  Training Step: 225...  Training loss: 2.2838...  1.0103 sec/batch\n",
      "Epoch: 113/1000...  Training Step: 226...  Training loss: 2.2787...  1.0109 sec/batch\n",
      "Epoch: 114/1000...  Training Step: 227...  Training loss: 2.2798...  1.0105 sec/batch\n",
      "Epoch: 114/1000...  Training Step: 228...  Training loss: 2.2713...  1.0114 sec/batch\n",
      "Epoch: 115/1000...  Training Step: 229...  Training loss: 2.2717...  1.0109 sec/batch\n",
      "Epoch: 115/1000...  Training Step: 230...  Training loss: 2.2630...  1.0117 sec/batch\n",
      "Epoch: 116/1000...  Training Step: 231...  Training loss: 2.2646...  1.0107 sec/batch\n",
      "Epoch: 116/1000...  Training Step: 232...  Training loss: 2.2527...  1.0116 sec/batch\n",
      "Epoch: 117/1000...  Training Step: 233...  Training loss: 2.2525...  1.0120 sec/batch\n",
      "Epoch: 117/1000...  Training Step: 234...  Training loss: 2.2489...  1.0107 sec/batch\n",
      "Epoch: 118/1000...  Training Step: 235...  Training loss: 2.2449...  1.0117 sec/batch\n",
      "Epoch: 118/1000...  Training Step: 236...  Training loss: 2.2345...  1.0121 sec/batch\n",
      "Epoch: 119/1000...  Training Step: 237...  Training loss: 2.2366...  1.0112 sec/batch\n",
      "Epoch: 119/1000...  Training Step: 238...  Training loss: 2.2305...  1.0102 sec/batch\n",
      "Epoch: 120/1000...  Training Step: 239...  Training loss: 2.2280...  1.0111 sec/batch\n",
      "Epoch: 120/1000...  Training Step: 240...  Training loss: 2.2192...  1.0114 sec/batch\n",
      "Epoch: 121/1000...  Training Step: 241...  Training loss: 2.2232...  1.0109 sec/batch\n",
      "Epoch: 121/1000...  Training Step: 242...  Training loss: 2.2145...  1.0112 sec/batch\n",
      "Epoch: 122/1000...  Training Step: 243...  Training loss: 2.2189...  1.0108 sec/batch\n",
      "Epoch: 122/1000...  Training Step: 244...  Training loss: 2.2848...  1.0097 sec/batch\n",
      "Epoch: 123/1000...  Training Step: 245...  Training loss: 2.5157...  1.0108 sec/batch\n",
      "Epoch: 123/1000...  Training Step: 246...  Training loss: 2.2942...  1.0107 sec/batch\n",
      "Epoch: 124/1000...  Training Step: 247...  Training loss: 2.2946...  1.0103 sec/batch\n",
      "Epoch: 124/1000...  Training Step: 248...  Training loss: 2.3403...  1.0109 sec/batch\n",
      "Epoch: 125/1000...  Training Step: 249...  Training loss: 2.2825...  1.0108 sec/batch\n",
      "Epoch: 125/1000...  Training Step: 250...  Training loss: 2.2667...  1.0105 sec/batch\n",
      "Epoch: 126/1000...  Training Step: 251...  Training loss: 2.2962...  1.0108 sec/batch\n",
      "Epoch: 126/1000...  Training Step: 252...  Training loss: 2.2910...  1.0101 sec/batch\n",
      "Epoch: 127/1000...  Training Step: 253...  Training loss: 2.2733...  1.0112 sec/batch\n",
      "Epoch: 127/1000...  Training Step: 254...  Training loss: 2.2532...  1.0108 sec/batch\n",
      "Epoch: 128/1000...  Training Step: 255...  Training loss: 2.2624...  1.0110 sec/batch\n",
      "Epoch: 128/1000...  Training Step: 256...  Training loss: 2.2616...  1.0112 sec/batch\n",
      "Epoch: 129/1000...  Training Step: 257...  Training loss: 2.2603...  1.0114 sec/batch\n",
      "Epoch: 129/1000...  Training Step: 258...  Training loss: 2.2450...  1.0111 sec/batch\n",
      "Epoch: 130/1000...  Training Step: 259...  Training loss: 2.2442...  1.0107 sec/batch\n",
      "Epoch: 130/1000...  Training Step: 260...  Training loss: 2.2369...  1.0106 sec/batch\n",
      "Epoch: 131/1000...  Training Step: 261...  Training loss: 2.2380...  1.0112 sec/batch\n",
      "Epoch: 131/1000...  Training Step: 262...  Training loss: 2.2254...  1.0108 sec/batch\n",
      "Epoch: 132/1000...  Training Step: 263...  Training loss: 2.2265...  1.0107 sec/batch\n",
      "Epoch: 132/1000...  Training Step: 264...  Training loss: 2.2186...  1.0107 sec/batch\n",
      "Epoch: 133/1000...  Training Step: 265...  Training loss: 2.2170...  1.0108 sec/batch\n",
      "Epoch: 133/1000...  Training Step: 266...  Training loss: 2.2080...  1.0109 sec/batch\n",
      "Epoch: 134/1000...  Training Step: 267...  Training loss: 2.2083...  1.0108 sec/batch\n",
      "Epoch: 134/1000...  Training Step: 268...  Training loss: 2.1993...  1.0114 sec/batch\n",
      "Epoch: 135/1000...  Training Step: 269...  Training loss: 2.1984...  1.0105 sec/batch\n",
      "Epoch: 135/1000...  Training Step: 270...  Training loss: 2.1921...  1.0100 sec/batch\n",
      "Epoch: 136/1000...  Training Step: 271...  Training loss: 2.1925...  1.0104 sec/batch\n",
      "Epoch: 136/1000...  Training Step: 272...  Training loss: 2.1831...  1.0111 sec/batch\n",
      "Epoch: 137/1000...  Training Step: 273...  Training loss: 2.1823...  1.0112 sec/batch\n",
      "Epoch: 137/1000...  Training Step: 274...  Training loss: 2.1795...  1.0100 sec/batch\n",
      "Epoch: 138/1000...  Training Step: 275...  Training loss: 2.1784...  1.0109 sec/batch\n",
      "Epoch: 138/1000...  Training Step: 276...  Training loss: 2.1714...  1.0110 sec/batch\n",
      "Epoch: 139/1000...  Training Step: 277...  Training loss: 2.1733...  1.0122 sec/batch\n",
      "Epoch: 139/1000...  Training Step: 278...  Training loss: 2.1653...  1.0108 sec/batch\n",
      "Epoch: 140/1000...  Training Step: 279...  Training loss: 2.1655...  1.0113 sec/batch\n",
      "Epoch: 140/1000...  Training Step: 280...  Training loss: 2.1573...  1.0111 sec/batch\n",
      "Epoch: 141/1000...  Training Step: 281...  Training loss: 2.1589...  1.0116 sec/batch\n",
      "Epoch: 141/1000...  Training Step: 282...  Training loss: 2.1510...  1.0108 sec/batch\n",
      "Epoch: 142/1000...  Training Step: 283...  Training loss: 2.1537...  1.0110 sec/batch\n",
      "Epoch: 142/1000...  Training Step: 284...  Training loss: 2.1518...  1.0099 sec/batch\n",
      "Epoch: 143/1000...  Training Step: 285...  Training loss: 2.1770...  1.0116 sec/batch\n",
      "Epoch: 143/1000...  Training Step: 286...  Training loss: 2.1984...  1.0093 sec/batch\n",
      "Epoch: 144/1000...  Training Step: 287...  Training loss: 2.1641...  1.0109 sec/batch\n",
      "Epoch: 144/1000...  Training Step: 288...  Training loss: 2.1635...  1.0197 sec/batch\n",
      "Epoch: 145/1000...  Training Step: 289...  Training loss: 2.1689...  1.0105 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145/1000...  Training Step: 290...  Training loss: 2.1577...  1.0176 sec/batch\n",
      "Epoch: 146/1000...  Training Step: 291...  Training loss: 2.1628...  1.0107 sec/batch\n",
      "Epoch: 146/1000...  Training Step: 292...  Training loss: 2.1506...  1.0111 sec/batch\n",
      "Epoch: 147/1000...  Training Step: 293...  Training loss: 2.1523...  1.0105 sec/batch\n",
      "Epoch: 147/1000...  Training Step: 294...  Training loss: 2.1466...  1.0107 sec/batch\n",
      "Epoch: 148/1000...  Training Step: 295...  Training loss: 2.1451...  1.0110 sec/batch\n",
      "Epoch: 148/1000...  Training Step: 296...  Training loss: 2.1356...  1.0111 sec/batch\n",
      "Epoch: 149/1000...  Training Step: 297...  Training loss: 2.1413...  1.0108 sec/batch\n",
      "Epoch: 149/1000...  Training Step: 298...  Training loss: 2.1322...  1.0112 sec/batch\n",
      "Epoch: 150/1000...  Training Step: 299...  Training loss: 2.1300...  1.0107 sec/batch\n",
      "Epoch: 150/1000...  Training Step: 300...  Training loss: 2.1236...  1.0117 sec/batch\n",
      "Epoch: 151/1000...  Training Step: 301...  Training loss: 2.1250...  1.0111 sec/batch\n",
      "Epoch: 151/1000...  Training Step: 302...  Training loss: 2.1177...  1.0108 sec/batch\n",
      "Epoch: 152/1000...  Training Step: 303...  Training loss: 2.1181...  1.0114 sec/batch\n",
      "Epoch: 152/1000...  Training Step: 304...  Training loss: 2.1106...  1.0105 sec/batch\n",
      "Epoch: 153/1000...  Training Step: 305...  Training loss: 2.1113...  1.0108 sec/batch\n",
      "Epoch: 153/1000...  Training Step: 306...  Training loss: 2.1054...  1.0111 sec/batch\n",
      "Epoch: 154/1000...  Training Step: 307...  Training loss: 2.1054...  1.0102 sec/batch\n",
      "Epoch: 154/1000...  Training Step: 308...  Training loss: 2.0990...  1.0110 sec/batch\n",
      "Epoch: 155/1000...  Training Step: 309...  Training loss: 2.0986...  1.0104 sec/batch\n",
      "Epoch: 155/1000...  Training Step: 310...  Training loss: 2.0925...  1.0105 sec/batch\n",
      "Epoch: 156/1000...  Training Step: 311...  Training loss: 2.0939...  1.0110 sec/batch\n",
      "Epoch: 156/1000...  Training Step: 312...  Training loss: 2.0883...  1.0112 sec/batch\n",
      "Epoch: 157/1000...  Training Step: 313...  Training loss: 2.0918...  1.0111 sec/batch\n",
      "Epoch: 157/1000...  Training Step: 314...  Training loss: 2.0861...  1.0115 sec/batch\n",
      "Epoch: 158/1000...  Training Step: 315...  Training loss: 2.0937...  1.0118 sec/batch\n",
      "Epoch: 158/1000...  Training Step: 316...  Training loss: 2.0856...  1.0115 sec/batch\n",
      "Epoch: 159/1000...  Training Step: 317...  Training loss: 2.0835...  1.0109 sec/batch\n",
      "Epoch: 159/1000...  Training Step: 318...  Training loss: 2.0711...  1.0106 sec/batch\n",
      "Epoch: 160/1000...  Training Step: 319...  Training loss: 2.0787...  1.0106 sec/batch\n",
      "Epoch: 160/1000...  Training Step: 320...  Training loss: 2.0686...  1.0111 sec/batch\n",
      "Epoch: 161/1000...  Training Step: 321...  Training loss: 2.0711...  1.0109 sec/batch\n",
      "Epoch: 161/1000...  Training Step: 322...  Training loss: 2.0651...  1.0113 sec/batch\n",
      "Epoch: 162/1000...  Training Step: 323...  Training loss: 2.0680...  1.0277 sec/batch\n",
      "Epoch: 162/1000...  Training Step: 324...  Training loss: 2.0614...  1.0389 sec/batch\n",
      "Epoch: 163/1000...  Training Step: 325...  Training loss: 2.0636...  1.0364 sec/batch\n",
      "Epoch: 163/1000...  Training Step: 326...  Training loss: 2.0515...  1.0333 sec/batch\n",
      "Epoch: 164/1000...  Training Step: 327...  Training loss: 2.0558...  1.0297 sec/batch\n",
      "Epoch: 164/1000...  Training Step: 328...  Training loss: 2.0479...  1.0270 sec/batch\n",
      "Epoch: 165/1000...  Training Step: 329...  Training loss: 2.0533...  1.0240 sec/batch\n",
      "Epoch: 165/1000...  Training Step: 330...  Training loss: 2.0576...  1.0199 sec/batch\n",
      "Epoch: 166/1000...  Training Step: 331...  Training loss: 2.0696...  1.0178 sec/batch\n",
      "Epoch: 166/1000...  Training Step: 332...  Training loss: 2.0545...  1.0136 sec/batch\n",
      "Epoch: 167/1000...  Training Step: 333...  Training loss: 2.0645...  1.0162 sec/batch\n",
      "Epoch: 167/1000...  Training Step: 334...  Training loss: 2.0437...  1.0087 sec/batch\n",
      "Epoch: 168/1000...  Training Step: 335...  Training loss: 2.0541...  1.0105 sec/batch\n",
      "Epoch: 168/1000...  Training Step: 336...  Training loss: 2.0419...  1.0081 sec/batch\n",
      "Epoch: 169/1000...  Training Step: 337...  Training loss: 2.0443...  1.0091 sec/batch\n",
      "Epoch: 169/1000...  Training Step: 338...  Training loss: 2.0352...  1.0096 sec/batch\n",
      "Epoch: 170/1000...  Training Step: 339...  Training loss: 2.0386...  1.0095 sec/batch\n",
      "Epoch: 170/1000...  Training Step: 340...  Training loss: 2.0289...  1.0089 sec/batch\n",
      "Epoch: 171/1000...  Training Step: 341...  Training loss: 2.0307...  1.0107 sec/batch\n",
      "Epoch: 171/1000...  Training Step: 342...  Training loss: 2.0241...  1.0097 sec/batch\n",
      "Epoch: 172/1000...  Training Step: 343...  Training loss: 2.0223...  1.0099 sec/batch\n",
      "Epoch: 172/1000...  Training Step: 344...  Training loss: 2.0157...  1.0096 sec/batch\n",
      "Epoch: 173/1000...  Training Step: 345...  Training loss: 2.0197...  1.0097 sec/batch\n",
      "Epoch: 173/1000...  Training Step: 346...  Training loss: 2.0175...  1.0102 sec/batch\n",
      "Epoch: 174/1000...  Training Step: 347...  Training loss: 2.0321...  1.0093 sec/batch\n",
      "Epoch: 174/1000...  Training Step: 348...  Training loss: 2.0468...  1.0099 sec/batch\n",
      "Epoch: 175/1000...  Training Step: 349...  Training loss: 2.1012...  1.0098 sec/batch\n",
      "Epoch: 175/1000...  Training Step: 350...  Training loss: 2.0517...  1.0098 sec/batch\n",
      "Epoch: 176/1000...  Training Step: 351...  Training loss: 2.0340...  1.0089 sec/batch\n",
      "Epoch: 176/1000...  Training Step: 352...  Training loss: 2.0466...  1.0113 sec/batch\n",
      "Epoch: 177/1000...  Training Step: 353...  Training loss: 2.0366...  1.0091 sec/batch\n",
      "Epoch: 177/1000...  Training Step: 354...  Training loss: 2.0325...  1.0096 sec/batch\n",
      "Epoch: 178/1000...  Training Step: 355...  Training loss: 2.0315...  1.0097 sec/batch\n",
      "Epoch: 178/1000...  Training Step: 356...  Training loss: 2.0255...  1.0097 sec/batch\n",
      "Epoch: 179/1000...  Training Step: 357...  Training loss: 2.0238...  1.0101 sec/batch\n",
      "Epoch: 179/1000...  Training Step: 358...  Training loss: 2.0163...  1.0100 sec/batch\n",
      "Epoch: 180/1000...  Training Step: 359...  Training loss: 2.0112...  1.0089 sec/batch\n",
      "Epoch: 180/1000...  Training Step: 360...  Training loss: 2.0070...  1.0091 sec/batch\n",
      "Epoch: 181/1000...  Training Step: 361...  Training loss: 2.0059...  1.0094 sec/batch\n",
      "Epoch: 181/1000...  Training Step: 362...  Training loss: 1.9978...  1.0093 sec/batch\n",
      "Epoch: 182/1000...  Training Step: 363...  Training loss: 1.9980...  1.0091 sec/batch\n",
      "Epoch: 182/1000...  Training Step: 364...  Training loss: 1.9915...  1.0099 sec/batch\n",
      "Epoch: 183/1000...  Training Step: 365...  Training loss: 1.9886...  1.0085 sec/batch\n",
      "Epoch: 183/1000...  Training Step: 366...  Training loss: 1.9880...  1.0084 sec/batch\n",
      "Epoch: 184/1000...  Training Step: 367...  Training loss: 1.9837...  1.0094 sec/batch\n",
      "Epoch: 184/1000...  Training Step: 368...  Training loss: 1.9785...  1.0091 sec/batch\n",
      "Epoch: 185/1000...  Training Step: 369...  Training loss: 1.9770...  1.0093 sec/batch\n",
      "Epoch: 185/1000...  Training Step: 370...  Training loss: 1.9724...  1.0097 sec/batch\n",
      "Epoch: 186/1000...  Training Step: 371...  Training loss: 1.9705...  1.0098 sec/batch\n",
      "Epoch: 186/1000...  Training Step: 372...  Training loss: 1.9674...  1.0096 sec/batch\n",
      "Epoch: 187/1000...  Training Step: 373...  Training loss: 1.9675...  1.0094 sec/batch\n",
      "Epoch: 187/1000...  Training Step: 374...  Training loss: 1.9725...  1.0096 sec/batch\n",
      "Epoch: 188/1000...  Training Step: 375...  Training loss: 1.9822...  1.0095 sec/batch\n",
      "Epoch: 188/1000...  Training Step: 376...  Training loss: 1.9807...  1.0099 sec/batch\n",
      "Epoch: 189/1000...  Training Step: 377...  Training loss: 1.9624...  1.0093 sec/batch\n",
      "Epoch: 189/1000...  Training Step: 378...  Training loss: 1.9610...  1.0100 sec/batch\n",
      "Epoch: 190/1000...  Training Step: 379...  Training loss: 1.9604...  1.0100 sec/batch\n",
      "Epoch: 190/1000...  Training Step: 380...  Training loss: 1.9560...  1.0103 sec/batch\n",
      "Epoch: 191/1000...  Training Step: 381...  Training loss: 1.9526...  1.0101 sec/batch\n",
      "Epoch: 191/1000...  Training Step: 382...  Training loss: 1.9487...  1.0103 sec/batch\n",
      "Epoch: 192/1000...  Training Step: 383...  Training loss: 1.9460...  1.0098 sec/batch\n",
      "Epoch: 192/1000...  Training Step: 384...  Training loss: 1.9431...  1.0081 sec/batch\n",
      "Epoch: 193/1000...  Training Step: 385...  Training loss: 1.9408...  1.0096 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193/1000...  Training Step: 386...  Training loss: 1.9351...  1.0094 sec/batch\n",
      "Epoch: 194/1000...  Training Step: 387...  Training loss: 1.9352...  1.0100 sec/batch\n",
      "Epoch: 194/1000...  Training Step: 388...  Training loss: 1.9304...  1.0093 sec/batch\n",
      "Epoch: 195/1000...  Training Step: 389...  Training loss: 1.9296...  1.0094 sec/batch\n",
      "Epoch: 195/1000...  Training Step: 390...  Training loss: 1.9232...  1.0097 sec/batch\n",
      "Epoch: 196/1000...  Training Step: 391...  Training loss: 1.9229...  1.0092 sec/batch\n",
      "Epoch: 196/1000...  Training Step: 392...  Training loss: 1.9189...  1.0093 sec/batch\n",
      "Epoch: 197/1000...  Training Step: 393...  Training loss: 1.9252...  1.0094 sec/batch\n",
      "Epoch: 197/1000...  Training Step: 394...  Training loss: 1.9327...  1.0099 sec/batch\n",
      "Epoch: 198/1000...  Training Step: 395...  Training loss: 1.9663...  1.0100 sec/batch\n",
      "Epoch: 198/1000...  Training Step: 396...  Training loss: 1.9256...  1.0096 sec/batch\n",
      "Epoch: 199/1000...  Training Step: 397...  Training loss: 1.9328...  1.0104 sec/batch\n",
      "Epoch: 199/1000...  Training Step: 398...  Training loss: 1.9218...  1.0094 sec/batch\n",
      "Epoch: 200/1000...  Training Step: 399...  Training loss: 1.9255...  1.0095 sec/batch\n",
      "Epoch: 200/1000...  Training Step: 400...  Training loss: 1.9152...  1.0100 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 201/1000...  Training Step: 401...  Training loss: 1.9157...  1.0080 sec/batch\n",
      "Epoch: 201/1000...  Training Step: 402...  Training loss: 1.9088...  1.0101 sec/batch\n",
      "Epoch: 202/1000...  Training Step: 403...  Training loss: 1.9080...  1.0104 sec/batch\n",
      "Epoch: 202/1000...  Training Step: 404...  Training loss: 1.8995...  1.0098 sec/batch\n",
      "Epoch: 203/1000...  Training Step: 405...  Training loss: 1.9032...  1.0100 sec/batch\n",
      "Epoch: 203/1000...  Training Step: 406...  Training loss: 1.8948...  1.0103 sec/batch\n",
      "Epoch: 204/1000...  Training Step: 407...  Training loss: 1.8951...  1.0156 sec/batch\n",
      "Epoch: 204/1000...  Training Step: 408...  Training loss: 1.8888...  1.0099 sec/batch\n",
      "Epoch: 205/1000...  Training Step: 409...  Training loss: 1.8873...  1.0141 sec/batch\n",
      "Epoch: 205/1000...  Training Step: 410...  Training loss: 1.8823...  1.0098 sec/batch\n",
      "Epoch: 206/1000...  Training Step: 411...  Training loss: 1.8823...  1.0098 sec/batch\n",
      "Epoch: 206/1000...  Training Step: 412...  Training loss: 1.8778...  1.0099 sec/batch\n",
      "Epoch: 207/1000...  Training Step: 413...  Training loss: 1.8805...  1.0097 sec/batch\n",
      "Epoch: 207/1000...  Training Step: 414...  Training loss: 1.8858...  1.0096 sec/batch\n",
      "Epoch: 208/1000...  Training Step: 415...  Training loss: 1.9001...  1.0091 sec/batch\n",
      "Epoch: 208/1000...  Training Step: 416...  Training loss: 1.8767...  1.0098 sec/batch\n",
      "Epoch: 209/1000...  Training Step: 417...  Training loss: 1.8734...  1.0108 sec/batch\n",
      "Epoch: 209/1000...  Training Step: 418...  Training loss: 1.8680...  1.0096 sec/batch\n",
      "Epoch: 210/1000...  Training Step: 419...  Training loss: 1.8691...  1.0101 sec/batch\n",
      "Epoch: 210/1000...  Training Step: 420...  Training loss: 1.8635...  1.0103 sec/batch\n",
      "Epoch: 211/1000...  Training Step: 421...  Training loss: 1.8626...  1.0102 sec/batch\n",
      "Epoch: 211/1000...  Training Step: 422...  Training loss: 1.8578...  1.0105 sec/batch\n",
      "Epoch: 212/1000...  Training Step: 423...  Training loss: 1.8590...  1.0098 sec/batch\n",
      "Epoch: 212/1000...  Training Step: 424...  Training loss: 1.8560...  1.0085 sec/batch\n",
      "Epoch: 213/1000...  Training Step: 425...  Training loss: 1.8560...  1.0100 sec/batch\n",
      "Epoch: 213/1000...  Training Step: 426...  Training loss: 1.8487...  1.0102 sec/batch\n",
      "Epoch: 214/1000...  Training Step: 427...  Training loss: 1.8480...  1.0096 sec/batch\n",
      "Epoch: 214/1000...  Training Step: 428...  Training loss: 1.8403...  1.0106 sec/batch\n",
      "Epoch: 215/1000...  Training Step: 429...  Training loss: 1.8412...  1.0102 sec/batch\n",
      "Epoch: 215/1000...  Training Step: 430...  Training loss: 1.8339...  1.0094 sec/batch\n",
      "Epoch: 216/1000...  Training Step: 431...  Training loss: 1.8354...  1.0095 sec/batch\n",
      "Epoch: 216/1000...  Training Step: 432...  Training loss: 1.8302...  1.0098 sec/batch\n",
      "Epoch: 217/1000...  Training Step: 433...  Training loss: 1.8359...  1.0100 sec/batch\n",
      "Epoch: 217/1000...  Training Step: 434...  Training loss: 1.8294...  1.0101 sec/batch\n",
      "Epoch: 218/1000...  Training Step: 435...  Training loss: 1.8269...  1.0093 sec/batch\n",
      "Epoch: 218/1000...  Training Step: 436...  Training loss: 1.8261...  1.0097 sec/batch\n",
      "Epoch: 219/1000...  Training Step: 437...  Training loss: 1.8478...  1.0098 sec/batch\n",
      "Epoch: 219/1000...  Training Step: 438...  Training loss: 1.8269...  1.0096 sec/batch\n",
      "Epoch: 220/1000...  Training Step: 439...  Training loss: 1.8274...  1.0104 sec/batch\n",
      "Epoch: 220/1000...  Training Step: 440...  Training loss: 1.8267...  1.0101 sec/batch\n",
      "Epoch: 221/1000...  Training Step: 441...  Training loss: 1.8414...  1.0096 sec/batch\n",
      "Epoch: 221/1000...  Training Step: 442...  Training loss: 1.8434...  1.0100 sec/batch\n",
      "Epoch: 222/1000...  Training Step: 443...  Training loss: 1.8205...  1.0105 sec/batch\n",
      "Epoch: 222/1000...  Training Step: 444...  Training loss: 1.8226...  1.0086 sec/batch\n",
      "Epoch: 223/1000...  Training Step: 445...  Training loss: 1.8160...  1.0100 sec/batch\n",
      "Epoch: 223/1000...  Training Step: 446...  Training loss: 1.8119...  1.0102 sec/batch\n",
      "Epoch: 224/1000...  Training Step: 447...  Training loss: 1.8098...  1.0095 sec/batch\n",
      "Epoch: 224/1000...  Training Step: 448...  Training loss: 1.8029...  1.0090 sec/batch\n",
      "Epoch: 225/1000...  Training Step: 449...  Training loss: 1.8027...  1.0096 sec/batch\n",
      "Epoch: 225/1000...  Training Step: 450...  Training loss: 1.7948...  1.0098 sec/batch\n",
      "Epoch: 226/1000...  Training Step: 451...  Training loss: 1.7941...  1.0102 sec/batch\n",
      "Epoch: 226/1000...  Training Step: 452...  Training loss: 1.7921...  1.0104 sec/batch\n",
      "Epoch: 227/1000...  Training Step: 453...  Training loss: 1.7837...  1.0110 sec/batch\n",
      "Epoch: 227/1000...  Training Step: 454...  Training loss: 1.7802...  1.0095 sec/batch\n",
      "Epoch: 228/1000...  Training Step: 455...  Training loss: 1.7795...  1.0096 sec/batch\n",
      "Epoch: 228/1000...  Training Step: 456...  Training loss: 1.7717...  1.0100 sec/batch\n",
      "Epoch: 229/1000...  Training Step: 457...  Training loss: 1.7737...  1.0092 sec/batch\n",
      "Epoch: 229/1000...  Training Step: 458...  Training loss: 1.7827...  1.0104 sec/batch\n",
      "Epoch: 230/1000...  Training Step: 459...  Training loss: 1.8276...  1.0098 sec/batch\n",
      "Epoch: 230/1000...  Training Step: 460...  Training loss: 1.8342...  1.0096 sec/batch\n",
      "Epoch: 231/1000...  Training Step: 461...  Training loss: 1.8786...  1.0102 sec/batch\n",
      "Epoch: 231/1000...  Training Step: 462...  Training loss: 1.9178...  1.0099 sec/batch\n",
      "Epoch: 232/1000...  Training Step: 463...  Training loss: 1.8422...  1.0097 sec/batch\n",
      "Epoch: 232/1000...  Training Step: 464...  Training loss: 1.8464...  1.0096 sec/batch\n",
      "Epoch: 233/1000...  Training Step: 465...  Training loss: 1.8523...  1.0099 sec/batch\n",
      "Epoch: 233/1000...  Training Step: 466...  Training loss: 1.8304...  1.0095 sec/batch\n",
      "Epoch: 234/1000...  Training Step: 467...  Training loss: 1.8412...  1.0094 sec/batch\n",
      "Epoch: 234/1000...  Training Step: 468...  Training loss: 1.8221...  1.0095 sec/batch\n",
      "Epoch: 235/1000...  Training Step: 469...  Training loss: 1.8194...  1.0109 sec/batch\n",
      "Epoch: 235/1000...  Training Step: 470...  Training loss: 1.8123...  1.0100 sec/batch\n",
      "Epoch: 236/1000...  Training Step: 471...  Training loss: 1.8061...  1.0100 sec/batch\n",
      "Epoch: 236/1000...  Training Step: 472...  Training loss: 1.7981...  1.0093 sec/batch\n",
      "Epoch: 237/1000...  Training Step: 473...  Training loss: 1.7924...  1.0100 sec/batch\n",
      "Epoch: 237/1000...  Training Step: 474...  Training loss: 1.7853...  1.0097 sec/batch\n",
      "Epoch: 238/1000...  Training Step: 475...  Training loss: 1.7828...  1.0099 sec/batch\n",
      "Epoch: 238/1000...  Training Step: 476...  Training loss: 1.7746...  1.0094 sec/batch\n",
      "Epoch: 239/1000...  Training Step: 477...  Training loss: 1.7725...  1.0092 sec/batch\n",
      "Epoch: 239/1000...  Training Step: 478...  Training loss: 1.7735...  1.0092 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240/1000...  Training Step: 479...  Training loss: 1.7936...  1.0096 sec/batch\n",
      "Epoch: 240/1000...  Training Step: 480...  Training loss: 1.7972...  1.0099 sec/batch\n",
      "Epoch: 241/1000...  Training Step: 481...  Training loss: 1.7701...  1.0107 sec/batch\n",
      "Epoch: 241/1000...  Training Step: 482...  Training loss: 1.7755...  1.0103 sec/batch\n",
      "Epoch: 242/1000...  Training Step: 483...  Training loss: 1.7694...  1.0099 sec/batch\n",
      "Epoch: 242/1000...  Training Step: 484...  Training loss: 1.7588...  1.0105 sec/batch\n",
      "Epoch: 243/1000...  Training Step: 485...  Training loss: 1.7635...  1.0101 sec/batch\n",
      "Epoch: 243/1000...  Training Step: 486...  Training loss: 1.7497...  1.0088 sec/batch\n",
      "Epoch: 244/1000...  Training Step: 487...  Training loss: 1.7529...  1.0095 sec/batch\n",
      "Epoch: 244/1000...  Training Step: 488...  Training loss: 1.7401...  1.0101 sec/batch\n",
      "Epoch: 245/1000...  Training Step: 489...  Training loss: 1.7417...  1.0089 sec/batch\n",
      "Epoch: 245/1000...  Training Step: 490...  Training loss: 1.7332...  1.0092 sec/batch\n",
      "Epoch: 246/1000...  Training Step: 491...  Training loss: 1.7318...  1.0102 sec/batch\n",
      "Epoch: 246/1000...  Training Step: 492...  Training loss: 1.7247...  1.0089 sec/batch\n",
      "Epoch: 247/1000...  Training Step: 493...  Training loss: 1.7236...  1.0093 sec/batch\n",
      "Epoch: 247/1000...  Training Step: 494...  Training loss: 1.7165...  1.0098 sec/batch\n",
      "Epoch: 248/1000...  Training Step: 495...  Training loss: 1.7172...  1.0098 sec/batch\n",
      "Epoch: 248/1000...  Training Step: 496...  Training loss: 1.7068...  1.0097 sec/batch\n",
      "Epoch: 249/1000...  Training Step: 497...  Training loss: 1.7105...  1.0093 sec/batch\n",
      "Epoch: 249/1000...  Training Step: 498...  Training loss: 1.7015...  1.0102 sec/batch\n",
      "Epoch: 250/1000...  Training Step: 499...  Training loss: 1.7023...  1.0095 sec/batch\n",
      "Epoch: 250/1000...  Training Step: 500...  Training loss: 1.6931...  1.0104 sec/batch\n",
      "Epoch: 251/1000...  Training Step: 501...  Training loss: 1.6962...  1.0088 sec/batch\n",
      "Epoch: 251/1000...  Training Step: 502...  Training loss: 1.6861...  1.0092 sec/batch\n",
      "Epoch: 252/1000...  Training Step: 503...  Training loss: 1.6892...  1.0100 sec/batch\n",
      "Epoch: 252/1000...  Training Step: 504...  Training loss: 1.6796...  1.0097 sec/batch\n",
      "Epoch: 253/1000...  Training Step: 505...  Training loss: 1.6828...  1.0107 sec/batch\n",
      "Epoch: 253/1000...  Training Step: 506...  Training loss: 1.6749...  1.0105 sec/batch\n",
      "Epoch: 254/1000...  Training Step: 507...  Training loss: 1.6791...  1.0099 sec/batch\n",
      "Epoch: 254/1000...  Training Step: 508...  Training loss: 1.6731...  1.0097 sec/batch\n",
      "Epoch: 255/1000...  Training Step: 509...  Training loss: 1.6714...  1.0097 sec/batch\n",
      "Epoch: 255/1000...  Training Step: 510...  Training loss: 1.6636...  1.0098 sec/batch\n",
      "Epoch: 256/1000...  Training Step: 511...  Training loss: 1.6666...  1.0095 sec/batch\n",
      "Epoch: 256/1000...  Training Step: 512...  Training loss: 1.6586...  1.0091 sec/batch\n",
      "Epoch: 257/1000...  Training Step: 513...  Training loss: 1.6665...  1.0102 sec/batch\n",
      "Epoch: 257/1000...  Training Step: 514...  Training loss: 1.6630...  1.0101 sec/batch\n",
      "Epoch: 258/1000...  Training Step: 515...  Training loss: 1.6713...  1.0102 sec/batch\n",
      "Epoch: 258/1000...  Training Step: 516...  Training loss: 1.6552...  1.0093 sec/batch\n",
      "Epoch: 259/1000...  Training Step: 517...  Training loss: 1.6582...  1.0085 sec/batch\n",
      "Epoch: 259/1000...  Training Step: 518...  Training loss: 1.6494...  1.0105 sec/batch\n",
      "Epoch: 260/1000...  Training Step: 519...  Training loss: 1.6494...  1.0086 sec/batch\n",
      "Epoch: 260/1000...  Training Step: 520...  Training loss: 1.6390...  1.0096 sec/batch\n",
      "Epoch: 261/1000...  Training Step: 521...  Training loss: 1.6451...  1.0094 sec/batch\n",
      "Epoch: 261/1000...  Training Step: 522...  Training loss: 1.6303...  1.0097 sec/batch\n",
      "Epoch: 262/1000...  Training Step: 523...  Training loss: 1.6370...  1.0097 sec/batch\n",
      "Epoch: 262/1000...  Training Step: 524...  Training loss: 1.6226...  1.0105 sec/batch\n",
      "Epoch: 263/1000...  Training Step: 525...  Training loss: 1.6274...  1.0153 sec/batch\n",
      "Epoch: 263/1000...  Training Step: 526...  Training loss: 1.6145...  1.0096 sec/batch\n",
      "Epoch: 264/1000...  Training Step: 527...  Training loss: 1.6199...  1.0145 sec/batch\n",
      "Epoch: 264/1000...  Training Step: 528...  Training loss: 1.6089...  1.0113 sec/batch\n",
      "Epoch: 265/1000...  Training Step: 529...  Training loss: 1.6189...  1.0107 sec/batch\n",
      "Epoch: 265/1000...  Training Step: 530...  Training loss: 1.6125...  1.0097 sec/batch\n",
      "Epoch: 266/1000...  Training Step: 531...  Training loss: 1.6342...  1.0099 sec/batch\n",
      "Epoch: 266/1000...  Training Step: 532...  Training loss: 1.6335...  1.0100 sec/batch\n",
      "Epoch: 267/1000...  Training Step: 533...  Training loss: 1.6471...  1.0092 sec/batch\n",
      "Epoch: 267/1000...  Training Step: 534...  Training loss: 1.6317...  1.0093 sec/batch\n",
      "Epoch: 268/1000...  Training Step: 535...  Training loss: 1.6319...  1.0092 sec/batch\n",
      "Epoch: 268/1000...  Training Step: 536...  Training loss: 1.6236...  1.0101 sec/batch\n",
      "Epoch: 269/1000...  Training Step: 537...  Training loss: 1.6170...  1.0102 sec/batch\n",
      "Epoch: 269/1000...  Training Step: 538...  Training loss: 1.6090...  1.0102 sec/batch\n",
      "Epoch: 270/1000...  Training Step: 539...  Training loss: 1.6101...  1.0099 sec/batch\n",
      "Epoch: 270/1000...  Training Step: 540...  Training loss: 1.5962...  1.0104 sec/batch\n",
      "Epoch: 271/1000...  Training Step: 541...  Training loss: 1.6025...  1.0089 sec/batch\n",
      "Epoch: 271/1000...  Training Step: 542...  Training loss: 1.5927...  1.0096 sec/batch\n",
      "Epoch: 272/1000...  Training Step: 543...  Training loss: 1.5929...  1.0090 sec/batch\n",
      "Epoch: 272/1000...  Training Step: 544...  Training loss: 1.5813...  1.0092 sec/batch\n",
      "Epoch: 273/1000...  Training Step: 545...  Training loss: 1.5906...  1.0102 sec/batch\n",
      "Epoch: 273/1000...  Training Step: 546...  Training loss: 1.5719...  1.0097 sec/batch\n",
      "Epoch: 274/1000...  Training Step: 547...  Training loss: 1.5770...  1.0115 sec/batch\n",
      "Epoch: 274/1000...  Training Step: 548...  Training loss: 1.5591...  1.0101 sec/batch\n",
      "Epoch: 275/1000...  Training Step: 549...  Training loss: 1.5665...  1.0106 sec/batch\n",
      "Epoch: 275/1000...  Training Step: 550...  Training loss: 1.5514...  1.0106 sec/batch\n",
      "Epoch: 276/1000...  Training Step: 551...  Training loss: 1.5577...  1.0101 sec/batch\n",
      "Epoch: 276/1000...  Training Step: 552...  Training loss: 1.5459...  1.0106 sec/batch\n",
      "Epoch: 277/1000...  Training Step: 553...  Training loss: 1.5488...  1.0108 sec/batch\n",
      "Epoch: 277/1000...  Training Step: 554...  Training loss: 1.5364...  1.0093 sec/batch\n",
      "Epoch: 278/1000...  Training Step: 555...  Training loss: 1.5452...  1.0096 sec/batch\n",
      "Epoch: 278/1000...  Training Step: 556...  Training loss: 1.5376...  1.0101 sec/batch\n",
      "Epoch: 279/1000...  Training Step: 557...  Training loss: 1.5404...  1.0094 sec/batch\n",
      "Epoch: 279/1000...  Training Step: 558...  Training loss: 1.5242...  1.0100 sec/batch\n",
      "Epoch: 280/1000...  Training Step: 559...  Training loss: 1.5382...  1.0098 sec/batch\n",
      "Epoch: 280/1000...  Training Step: 560...  Training loss: 1.5292...  1.0094 sec/batch\n",
      "Epoch: 281/1000...  Training Step: 561...  Training loss: 1.5571...  1.0103 sec/batch\n",
      "Epoch: 281/1000...  Training Step: 562...  Training loss: 1.5396...  1.0091 sec/batch\n",
      "Epoch: 282/1000...  Training Step: 563...  Training loss: 1.5431...  1.0094 sec/batch\n",
      "Epoch: 282/1000...  Training Step: 564...  Training loss: 1.5267...  1.0097 sec/batch\n",
      "Epoch: 283/1000...  Training Step: 565...  Training loss: 1.5320...  1.0097 sec/batch\n",
      "Epoch: 283/1000...  Training Step: 566...  Training loss: 1.5172...  1.0094 sec/batch\n",
      "Epoch: 284/1000...  Training Step: 567...  Training loss: 1.5203...  1.0098 sec/batch\n",
      "Epoch: 284/1000...  Training Step: 568...  Training loss: 1.5039...  1.0102 sec/batch\n",
      "Epoch: 285/1000...  Training Step: 569...  Training loss: 1.5075...  1.0095 sec/batch\n",
      "Epoch: 285/1000...  Training Step: 570...  Training loss: 1.4902...  1.0098 sec/batch\n",
      "Epoch: 286/1000...  Training Step: 571...  Training loss: 1.4939...  1.0096 sec/batch\n",
      "Epoch: 286/1000...  Training Step: 572...  Training loss: 1.4783...  1.0107 sec/batch\n",
      "Epoch: 287/1000...  Training Step: 573...  Training loss: 1.4875...  1.0104 sec/batch\n",
      "Epoch: 287/1000...  Training Step: 574...  Training loss: 1.4707...  1.0091 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288/1000...  Training Step: 575...  Training loss: 1.4735...  1.0105 sec/batch\n",
      "Epoch: 288/1000...  Training Step: 576...  Training loss: 1.4571...  1.0101 sec/batch\n",
      "Epoch: 289/1000...  Training Step: 577...  Training loss: 1.4613...  1.0094 sec/batch\n",
      "Epoch: 289/1000...  Training Step: 578...  Training loss: 1.4454...  1.0098 sec/batch\n",
      "Epoch: 290/1000...  Training Step: 579...  Training loss: 1.4612...  1.0094 sec/batch\n",
      "Epoch: 290/1000...  Training Step: 580...  Training loss: 1.4550...  1.0102 sec/batch\n",
      "Epoch: 291/1000...  Training Step: 581...  Training loss: 1.4677...  1.0100 sec/batch\n",
      "Epoch: 291/1000...  Training Step: 582...  Training loss: 1.4606...  1.0103 sec/batch\n",
      "Epoch: 292/1000...  Training Step: 583...  Training loss: 1.4602...  1.0090 sec/batch\n",
      "Epoch: 292/1000...  Training Step: 584...  Training loss: 1.4441...  1.0104 sec/batch\n",
      "Epoch: 293/1000...  Training Step: 585...  Training loss: 1.4721...  1.0096 sec/batch\n",
      "Epoch: 293/1000...  Training Step: 586...  Training loss: 1.4883...  1.0104 sec/batch\n",
      "Epoch: 294/1000...  Training Step: 587...  Training loss: 1.5099...  1.0108 sec/batch\n",
      "Epoch: 294/1000...  Training Step: 588...  Training loss: 1.4808...  1.0092 sec/batch\n",
      "Epoch: 295/1000...  Training Step: 589...  Training loss: 1.4909...  1.0105 sec/batch\n",
      "Epoch: 295/1000...  Training Step: 590...  Training loss: 1.4827...  1.0099 sec/batch\n",
      "Epoch: 296/1000...  Training Step: 591...  Training loss: 1.4756...  1.0095 sec/batch\n",
      "Epoch: 296/1000...  Training Step: 592...  Training loss: 1.4594...  1.0095 sec/batch\n",
      "Epoch: 297/1000...  Training Step: 593...  Training loss: 1.4617...  1.0101 sec/batch\n",
      "Epoch: 297/1000...  Training Step: 594...  Training loss: 1.4380...  1.0103 sec/batch\n",
      "Epoch: 298/1000...  Training Step: 595...  Training loss: 1.4417...  1.0108 sec/batch\n",
      "Epoch: 298/1000...  Training Step: 596...  Training loss: 1.4190...  1.0099 sec/batch\n",
      "Epoch: 299/1000...  Training Step: 597...  Training loss: 1.4197...  1.0097 sec/batch\n",
      "Epoch: 299/1000...  Training Step: 598...  Training loss: 1.4027...  1.0106 sec/batch\n",
      "Epoch: 300/1000...  Training Step: 599...  Training loss: 1.4034...  1.0098 sec/batch\n",
      "Epoch: 300/1000...  Training Step: 600...  Training loss: 1.3852...  1.0112 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 301/1000...  Training Step: 601...  Training loss: 1.3894...  1.0079 sec/batch\n",
      "Epoch: 301/1000...  Training Step: 602...  Training loss: 1.3720...  1.0095 sec/batch\n",
      "Epoch: 302/1000...  Training Step: 603...  Training loss: 1.3724...  1.0095 sec/batch\n",
      "Epoch: 302/1000...  Training Step: 604...  Training loss: 1.3550...  1.0101 sec/batch\n",
      "Epoch: 303/1000...  Training Step: 605...  Training loss: 1.3652...  1.0096 sec/batch\n",
      "Epoch: 303/1000...  Training Step: 606...  Training loss: 1.3633...  1.0111 sec/batch\n",
      "Epoch: 304/1000...  Training Step: 607...  Training loss: 1.3556...  1.0087 sec/batch\n",
      "Epoch: 304/1000...  Training Step: 608...  Training loss: 1.3514...  1.0101 sec/batch\n",
      "Epoch: 305/1000...  Training Step: 609...  Training loss: 1.3623...  1.0108 sec/batch\n",
      "Epoch: 305/1000...  Training Step: 610...  Training loss: 1.3447...  1.0104 sec/batch\n",
      "Epoch: 306/1000...  Training Step: 611...  Training loss: 1.3464...  1.0100 sec/batch\n",
      "Epoch: 306/1000...  Training Step: 612...  Training loss: 1.3312...  1.0104 sec/batch\n",
      "Epoch: 307/1000...  Training Step: 613...  Training loss: 1.3396...  1.0100 sec/batch\n",
      "Epoch: 307/1000...  Training Step: 614...  Training loss: 1.3202...  1.0093 sec/batch\n",
      "Epoch: 308/1000...  Training Step: 615...  Training loss: 1.3234...  1.0091 sec/batch\n",
      "Epoch: 308/1000...  Training Step: 616...  Training loss: 1.3133...  1.0097 sec/batch\n",
      "Epoch: 309/1000...  Training Step: 617...  Training loss: 1.3204...  1.0099 sec/batch\n",
      "Epoch: 309/1000...  Training Step: 618...  Training loss: 1.3016...  1.0099 sec/batch\n",
      "Epoch: 310/1000...  Training Step: 619...  Training loss: 1.3115...  1.0093 sec/batch\n",
      "Epoch: 310/1000...  Training Step: 620...  Training loss: 1.2973...  1.0104 sec/batch\n",
      "Epoch: 311/1000...  Training Step: 621...  Training loss: 1.3093...  1.0099 sec/batch\n",
      "Epoch: 311/1000...  Training Step: 622...  Training loss: 1.2793...  1.0082 sec/batch\n",
      "Epoch: 312/1000...  Training Step: 623...  Training loss: 1.2858...  1.0092 sec/batch\n",
      "Epoch: 312/1000...  Training Step: 624...  Training loss: 1.2723...  1.0097 sec/batch\n",
      "Epoch: 313/1000...  Training Step: 625...  Training loss: 1.2786...  1.0099 sec/batch\n",
      "Epoch: 313/1000...  Training Step: 626...  Training loss: 1.2586...  1.0100 sec/batch\n",
      "Epoch: 314/1000...  Training Step: 627...  Training loss: 1.2743...  1.0098 sec/batch\n",
      "Epoch: 314/1000...  Training Step: 628...  Training loss: 1.2621...  1.0100 sec/batch\n",
      "Epoch: 315/1000...  Training Step: 629...  Training loss: 1.2663...  1.0096 sec/batch\n",
      "Epoch: 315/1000...  Training Step: 630...  Training loss: 1.2559...  1.0103 sec/batch\n",
      "Epoch: 316/1000...  Training Step: 631...  Training loss: 1.2491...  1.0108 sec/batch\n",
      "Epoch: 316/1000...  Training Step: 632...  Training loss: 1.2356...  1.0102 sec/batch\n",
      "Epoch: 317/1000...  Training Step: 633...  Training loss: 1.2396...  1.0100 sec/batch\n",
      "Epoch: 317/1000...  Training Step: 634...  Training loss: 1.2156...  1.0104 sec/batch\n",
      "Epoch: 318/1000...  Training Step: 635...  Training loss: 1.2159...  1.0100 sec/batch\n",
      "Epoch: 318/1000...  Training Step: 636...  Training loss: 1.2033...  1.0104 sec/batch\n",
      "Epoch: 319/1000...  Training Step: 637...  Training loss: 1.2204...  1.0092 sec/batch\n",
      "Epoch: 319/1000...  Training Step: 638...  Training loss: 1.1940...  1.0095 sec/batch\n",
      "Epoch: 320/1000...  Training Step: 639...  Training loss: 1.2011...  1.0105 sec/batch\n",
      "Epoch: 320/1000...  Training Step: 640...  Training loss: 1.1963...  1.0105 sec/batch\n",
      "Epoch: 321/1000...  Training Step: 641...  Training loss: 1.1986...  1.0093 sec/batch\n",
      "Epoch: 321/1000...  Training Step: 642...  Training loss: 1.1710...  1.0100 sec/batch\n",
      "Epoch: 322/1000...  Training Step: 643...  Training loss: 1.1794...  1.0094 sec/batch\n",
      "Epoch: 322/1000...  Training Step: 644...  Training loss: 1.1570...  1.0170 sec/batch\n",
      "Epoch: 323/1000...  Training Step: 645...  Training loss: 1.1604...  1.0094 sec/batch\n",
      "Epoch: 323/1000...  Training Step: 646...  Training loss: 1.1472...  1.0144 sec/batch\n",
      "Epoch: 324/1000...  Training Step: 647...  Training loss: 1.1504...  1.0094 sec/batch\n",
      "Epoch: 324/1000...  Training Step: 648...  Training loss: 1.1334...  1.0103 sec/batch\n",
      "Epoch: 325/1000...  Training Step: 649...  Training loss: 1.1372...  1.0102 sec/batch\n",
      "Epoch: 325/1000...  Training Step: 650...  Training loss: 1.1155...  1.0113 sec/batch\n",
      "Epoch: 326/1000...  Training Step: 651...  Training loss: 1.1233...  1.0101 sec/batch\n",
      "Epoch: 326/1000...  Training Step: 652...  Training loss: 1.1029...  1.0094 sec/batch\n",
      "Epoch: 327/1000...  Training Step: 653...  Training loss: 1.1074...  1.0100 sec/batch\n",
      "Epoch: 327/1000...  Training Step: 654...  Training loss: 1.0935...  1.0104 sec/batch\n",
      "Epoch: 328/1000...  Training Step: 655...  Training loss: 1.0991...  1.0100 sec/batch\n",
      "Epoch: 328/1000...  Training Step: 656...  Training loss: 1.0741...  1.0111 sec/batch\n",
      "Epoch: 329/1000...  Training Step: 657...  Training loss: 1.0854...  1.0094 sec/batch\n",
      "Epoch: 329/1000...  Training Step: 658...  Training loss: 1.0643...  1.0096 sec/batch\n",
      "Epoch: 330/1000...  Training Step: 659...  Training loss: 1.0640...  1.0096 sec/batch\n",
      "Epoch: 330/1000...  Training Step: 660...  Training loss: 1.0416...  1.0099 sec/batch\n",
      "Epoch: 331/1000...  Training Step: 661...  Training loss: 1.0602...  1.0099 sec/batch\n",
      "Epoch: 331/1000...  Training Step: 662...  Training loss: 1.0486...  1.0086 sec/batch\n",
      "Epoch: 332/1000...  Training Step: 663...  Training loss: 1.0527...  1.0100 sec/batch\n",
      "Epoch: 332/1000...  Training Step: 664...  Training loss: 1.0307...  1.0098 sec/batch\n",
      "Epoch: 333/1000...  Training Step: 665...  Training loss: 1.0349...  1.0104 sec/batch\n",
      "Epoch: 333/1000...  Training Step: 666...  Training loss: 1.0142...  1.0085 sec/batch\n",
      "Epoch: 334/1000...  Training Step: 667...  Training loss: 1.0216...  1.0102 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334/1000...  Training Step: 668...  Training loss: 1.0041...  1.0098 sec/batch\n",
      "Epoch: 335/1000...  Training Step: 669...  Training loss: 1.0215...  1.0087 sec/batch\n",
      "Epoch: 335/1000...  Training Step: 670...  Training loss: 1.0044...  1.0101 sec/batch\n",
      "Epoch: 336/1000...  Training Step: 671...  Training loss: 0.9847...  1.0096 sec/batch\n",
      "Epoch: 336/1000...  Training Step: 672...  Training loss: 0.9933...  1.0095 sec/batch\n",
      "Epoch: 337/1000...  Training Step: 673...  Training loss: 0.9998...  1.0095 sec/batch\n",
      "Epoch: 337/1000...  Training Step: 674...  Training loss: 0.9806...  1.0101 sec/batch\n",
      "Epoch: 338/1000...  Training Step: 675...  Training loss: 0.9821...  1.0099 sec/batch\n",
      "Epoch: 338/1000...  Training Step: 676...  Training loss: 0.9479...  1.0100 sec/batch\n",
      "Epoch: 339/1000...  Training Step: 677...  Training loss: 0.9561...  1.0094 sec/batch\n",
      "Epoch: 339/1000...  Training Step: 678...  Training loss: 0.9291...  1.0104 sec/batch\n",
      "Epoch: 340/1000...  Training Step: 679...  Training loss: 0.9407...  1.0091 sec/batch\n",
      "Epoch: 340/1000...  Training Step: 680...  Training loss: 0.9117...  1.0097 sec/batch\n",
      "Epoch: 341/1000...  Training Step: 681...  Training loss: 0.9124...  1.0101 sec/batch\n",
      "Epoch: 341/1000...  Training Step: 682...  Training loss: 0.8959...  1.0107 sec/batch\n",
      "Epoch: 342/1000...  Training Step: 683...  Training loss: 0.8991...  1.0093 sec/batch\n",
      "Epoch: 342/1000...  Training Step: 684...  Training loss: 0.8794...  1.0102 sec/batch\n",
      "Epoch: 343/1000...  Training Step: 685...  Training loss: 0.9115...  1.0095 sec/batch\n",
      "Epoch: 343/1000...  Training Step: 686...  Training loss: 0.8985...  1.0101 sec/batch\n",
      "Epoch: 344/1000...  Training Step: 687...  Training loss: 0.8932...  1.0101 sec/batch\n",
      "Epoch: 344/1000...  Training Step: 688...  Training loss: 0.8717...  1.0096 sec/batch\n",
      "Epoch: 345/1000...  Training Step: 689...  Training loss: 0.8658...  1.0104 sec/batch\n",
      "Epoch: 345/1000...  Training Step: 690...  Training loss: 0.8553...  1.0086 sec/batch\n",
      "Epoch: 346/1000...  Training Step: 691...  Training loss: 0.8605...  1.0099 sec/batch\n",
      "Epoch: 346/1000...  Training Step: 692...  Training loss: 0.8578...  1.0094 sec/batch\n",
      "Epoch: 347/1000...  Training Step: 693...  Training loss: 0.8640...  1.0091 sec/batch\n",
      "Epoch: 347/1000...  Training Step: 694...  Training loss: 0.8314...  1.0094 sec/batch\n",
      "Epoch: 348/1000...  Training Step: 695...  Training loss: 0.8361...  1.0100 sec/batch\n",
      "Epoch: 348/1000...  Training Step: 696...  Training loss: 0.8093...  1.0101 sec/batch\n",
      "Epoch: 349/1000...  Training Step: 697...  Training loss: 0.8162...  1.0111 sec/batch\n",
      "Epoch: 349/1000...  Training Step: 698...  Training loss: 0.8048...  1.0104 sec/batch\n",
      "Epoch: 350/1000...  Training Step: 699...  Training loss: 0.8044...  1.0107 sec/batch\n",
      "Epoch: 350/1000...  Training Step: 700...  Training loss: 0.7761...  1.0102 sec/batch\n",
      "Epoch: 351/1000...  Training Step: 701...  Training loss: 0.7725...  1.0108 sec/batch\n",
      "Epoch: 351/1000...  Training Step: 702...  Training loss: 0.7496...  1.0094 sec/batch\n",
      "Epoch: 352/1000...  Training Step: 703...  Training loss: 0.7687...  1.0097 sec/batch\n",
      "Epoch: 352/1000...  Training Step: 704...  Training loss: 0.7552...  1.0103 sec/batch\n",
      "Epoch: 353/1000...  Training Step: 705...  Training loss: 0.7629...  1.0099 sec/batch\n",
      "Epoch: 353/1000...  Training Step: 706...  Training loss: 0.7515...  1.0095 sec/batch\n",
      "Epoch: 354/1000...  Training Step: 707...  Training loss: 0.7646...  1.0105 sec/batch\n",
      "Epoch: 354/1000...  Training Step: 708...  Training loss: 0.7439...  1.0102 sec/batch\n",
      "Epoch: 355/1000...  Training Step: 709...  Training loss: 0.7608...  1.0091 sec/batch\n",
      "Epoch: 355/1000...  Training Step: 710...  Training loss: 0.7463...  1.0100 sec/batch\n",
      "Epoch: 356/1000...  Training Step: 711...  Training loss: 0.7319...  1.0102 sec/batch\n",
      "Epoch: 356/1000...  Training Step: 712...  Training loss: 0.7094...  1.0097 sec/batch\n",
      "Epoch: 357/1000...  Training Step: 713...  Training loss: 0.7180...  1.0099 sec/batch\n",
      "Epoch: 357/1000...  Training Step: 714...  Training loss: 0.6924...  1.0096 sec/batch\n",
      "Epoch: 358/1000...  Training Step: 715...  Training loss: 0.6999...  1.0103 sec/batch\n",
      "Epoch: 358/1000...  Training Step: 716...  Training loss: 0.6692...  1.0106 sec/batch\n",
      "Epoch: 359/1000...  Training Step: 717...  Training loss: 0.6723...  1.0103 sec/batch\n",
      "Epoch: 359/1000...  Training Step: 718...  Training loss: 0.6437...  1.0104 sec/batch\n",
      "Epoch: 360/1000...  Training Step: 719...  Training loss: 0.6499...  1.0100 sec/batch\n",
      "Epoch: 360/1000...  Training Step: 720...  Training loss: 0.6274...  1.0106 sec/batch\n",
      "Epoch: 361/1000...  Training Step: 721...  Training loss: 0.6303...  1.0093 sec/batch\n",
      "Epoch: 361/1000...  Training Step: 722...  Training loss: 0.5987...  1.0094 sec/batch\n",
      "Epoch: 362/1000...  Training Step: 723...  Training loss: 0.6056...  1.0099 sec/batch\n",
      "Epoch: 362/1000...  Training Step: 724...  Training loss: 0.5807...  1.0095 sec/batch\n",
      "Epoch: 363/1000...  Training Step: 725...  Training loss: 0.5869...  1.0103 sec/batch\n",
      "Epoch: 363/1000...  Training Step: 726...  Training loss: 0.5575...  1.0087 sec/batch\n",
      "Epoch: 364/1000...  Training Step: 727...  Training loss: 0.5659...  1.0104 sec/batch\n",
      "Epoch: 364/1000...  Training Step: 728...  Training loss: 0.5424...  1.0102 sec/batch\n",
      "Epoch: 365/1000...  Training Step: 729...  Training loss: 0.5677...  1.0098 sec/batch\n",
      "Epoch: 365/1000...  Training Step: 730...  Training loss: 0.5503...  1.0105 sec/batch\n",
      "Epoch: 366/1000...  Training Step: 731...  Training loss: 0.5616...  1.0091 sec/batch\n",
      "Epoch: 366/1000...  Training Step: 732...  Training loss: 0.5327...  1.0099 sec/batch\n",
      "Epoch: 367/1000...  Training Step: 733...  Training loss: 0.5516...  1.0085 sec/batch\n",
      "Epoch: 367/1000...  Training Step: 734...  Training loss: 0.5334...  1.0098 sec/batch\n",
      "Epoch: 368/1000...  Training Step: 735...  Training loss: 0.5597...  1.0097 sec/batch\n",
      "Epoch: 368/1000...  Training Step: 736...  Training loss: 0.5395...  1.0109 sec/batch\n",
      "Epoch: 369/1000...  Training Step: 737...  Training loss: 0.5520...  1.0084 sec/batch\n",
      "Epoch: 369/1000...  Training Step: 738...  Training loss: 0.5221...  1.0101 sec/batch\n",
      "Epoch: 370/1000...  Training Step: 739...  Training loss: 0.5321...  1.0098 sec/batch\n",
      "Epoch: 370/1000...  Training Step: 740...  Training loss: 0.5097...  1.0105 sec/batch\n",
      "Epoch: 371/1000...  Training Step: 741...  Training loss: 0.5148...  1.0103 sec/batch\n",
      "Epoch: 371/1000...  Training Step: 742...  Training loss: 0.4912...  1.0100 sec/batch\n",
      "Epoch: 372/1000...  Training Step: 743...  Training loss: 0.4901...  1.0100 sec/batch\n",
      "Epoch: 372/1000...  Training Step: 744...  Training loss: 0.4683...  1.0098 sec/batch\n",
      "Epoch: 373/1000...  Training Step: 745...  Training loss: 0.4769...  1.0099 sec/batch\n",
      "Epoch: 373/1000...  Training Step: 746...  Training loss: 0.4494...  1.0097 sec/batch\n",
      "Epoch: 374/1000...  Training Step: 747...  Training loss: 0.4552...  1.0113 sec/batch\n",
      "Epoch: 374/1000...  Training Step: 748...  Training loss: 0.4310...  1.0103 sec/batch\n",
      "Epoch: 375/1000...  Training Step: 749...  Training loss: 0.4404...  1.0100 sec/batch\n",
      "Epoch: 375/1000...  Training Step: 750...  Training loss: 0.4046...  1.0102 sec/batch\n",
      "Epoch: 376/1000...  Training Step: 751...  Training loss: 0.4131...  1.0102 sec/batch\n",
      "Epoch: 376/1000...  Training Step: 752...  Training loss: 0.3975...  1.0095 sec/batch\n",
      "Epoch: 377/1000...  Training Step: 753...  Training loss: 0.4188...  1.0103 sec/batch\n",
      "Epoch: 377/1000...  Training Step: 754...  Training loss: 0.3973...  1.0094 sec/batch\n",
      "Epoch: 378/1000...  Training Step: 755...  Training loss: 0.3971...  1.0097 sec/batch\n",
      "Epoch: 378/1000...  Training Step: 756...  Training loss: 0.3722...  1.0105 sec/batch\n",
      "Epoch: 379/1000...  Training Step: 757...  Training loss: 0.3869...  1.0100 sec/batch\n",
      "Epoch: 379/1000...  Training Step: 758...  Training loss: 0.3676...  1.0091 sec/batch\n",
      "Epoch: 380/1000...  Training Step: 759...  Training loss: 0.3735...  1.0095 sec/batch\n",
      "Epoch: 380/1000...  Training Step: 760...  Training loss: 0.3504...  1.0103 sec/batch\n",
      "Epoch: 381/1000...  Training Step: 761...  Training loss: 0.3552...  1.0101 sec/batch\n",
      "Epoch: 381/1000...  Training Step: 762...  Training loss: 0.3322...  1.0111 sec/batch\n",
      "Epoch: 382/1000...  Training Step: 763...  Training loss: 0.3476...  1.0169 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 382/1000...  Training Step: 764...  Training loss: 0.3228...  1.0109 sec/batch\n",
      "Epoch: 383/1000...  Training Step: 765...  Training loss: 0.3388...  1.0147 sec/batch\n",
      "Epoch: 383/1000...  Training Step: 766...  Training loss: 0.3068...  1.0094 sec/batch\n",
      "Epoch: 384/1000...  Training Step: 767...  Training loss: 0.3203...  1.0097 sec/batch\n",
      "Epoch: 384/1000...  Training Step: 768...  Training loss: 0.2940...  1.0106 sec/batch\n",
      "Epoch: 385/1000...  Training Step: 769...  Training loss: 0.3089...  1.0091 sec/batch\n",
      "Epoch: 385/1000...  Training Step: 770...  Training loss: 0.2860...  1.0105 sec/batch\n",
      "Epoch: 386/1000...  Training Step: 771...  Training loss: 0.2960...  1.0100 sec/batch\n",
      "Epoch: 386/1000...  Training Step: 772...  Training loss: 0.2680...  1.0087 sec/batch\n",
      "Epoch: 387/1000...  Training Step: 773...  Training loss: 0.2833...  1.0079 sec/batch\n",
      "Epoch: 387/1000...  Training Step: 774...  Training loss: 0.2596...  1.0099 sec/batch\n",
      "Epoch: 388/1000...  Training Step: 775...  Training loss: 0.2739...  1.0098 sec/batch\n",
      "Epoch: 388/1000...  Training Step: 776...  Training loss: 0.2493...  1.0083 sec/batch\n",
      "Epoch: 389/1000...  Training Step: 777...  Training loss: 0.2609...  1.0094 sec/batch\n",
      "Epoch: 389/1000...  Training Step: 778...  Training loss: 0.2431...  1.0094 sec/batch\n",
      "Epoch: 390/1000...  Training Step: 779...  Training loss: 0.2572...  1.0101 sec/batch\n",
      "Epoch: 390/1000...  Training Step: 780...  Training loss: 0.2299...  1.0103 sec/batch\n",
      "Epoch: 391/1000...  Training Step: 781...  Training loss: 0.2493...  1.0096 sec/batch\n",
      "Epoch: 391/1000...  Training Step: 782...  Training loss: 0.2290...  1.0099 sec/batch\n",
      "Epoch: 392/1000...  Training Step: 783...  Training loss: 0.2461...  1.0097 sec/batch\n",
      "Epoch: 392/1000...  Training Step: 784...  Training loss: 0.2181...  1.0099 sec/batch\n",
      "Epoch: 393/1000...  Training Step: 785...  Training loss: 0.2404...  1.0099 sec/batch\n",
      "Epoch: 393/1000...  Training Step: 786...  Training loss: 0.2136...  1.0098 sec/batch\n",
      "Epoch: 394/1000...  Training Step: 787...  Training loss: 0.2371...  1.0103 sec/batch\n",
      "Epoch: 394/1000...  Training Step: 788...  Training loss: 0.2059...  1.0104 sec/batch\n",
      "Epoch: 395/1000...  Training Step: 789...  Training loss: 0.2220...  1.0098 sec/batch\n",
      "Epoch: 395/1000...  Training Step: 790...  Training loss: 0.1976...  1.0100 sec/batch\n",
      "Epoch: 396/1000...  Training Step: 791...  Training loss: 0.2109...  1.0095 sec/batch\n",
      "Epoch: 396/1000...  Training Step: 792...  Training loss: 0.1876...  1.0086 sec/batch\n",
      "Epoch: 397/1000...  Training Step: 793...  Training loss: 0.2020...  1.0108 sec/batch\n",
      "Epoch: 397/1000...  Training Step: 794...  Training loss: 0.1792...  1.0095 sec/batch\n",
      "Epoch: 398/1000...  Training Step: 795...  Training loss: 0.1925...  1.0101 sec/batch\n",
      "Epoch: 398/1000...  Training Step: 796...  Training loss: 0.1701...  1.0106 sec/batch\n",
      "Epoch: 399/1000...  Training Step: 797...  Training loss: 0.1874...  1.0099 sec/batch\n",
      "Epoch: 399/1000...  Training Step: 798...  Training loss: 0.1627...  1.0105 sec/batch\n",
      "Epoch: 400/1000...  Training Step: 799...  Training loss: 0.1795...  1.0094 sec/batch\n",
      "Epoch: 400/1000...  Training Step: 800...  Training loss: 0.1539...  1.0104 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 401/1000...  Training Step: 801...  Training loss: 0.1747...  1.0082 sec/batch\n",
      "Epoch: 401/1000...  Training Step: 802...  Training loss: 0.1463...  1.0105 sec/batch\n",
      "Epoch: 402/1000...  Training Step: 803...  Training loss: 0.1664...  1.0101 sec/batch\n",
      "Epoch: 402/1000...  Training Step: 804...  Training loss: 0.1405...  1.0096 sec/batch\n",
      "Epoch: 403/1000...  Training Step: 805...  Training loss: 0.1591...  1.0095 sec/batch\n",
      "Epoch: 403/1000...  Training Step: 806...  Training loss: 0.1332...  1.0102 sec/batch\n",
      "Epoch: 404/1000...  Training Step: 807...  Training loss: 0.1509...  1.0098 sec/batch\n",
      "Epoch: 404/1000...  Training Step: 808...  Training loss: 0.1287...  1.0099 sec/batch\n",
      "Epoch: 405/1000...  Training Step: 809...  Training loss: 0.1438...  1.0104 sec/batch\n",
      "Epoch: 405/1000...  Training Step: 810...  Training loss: 0.1226...  1.0098 sec/batch\n",
      "Epoch: 406/1000...  Training Step: 811...  Training loss: 0.1394...  1.0093 sec/batch\n",
      "Epoch: 406/1000...  Training Step: 812...  Training loss: 0.1164...  1.0102 sec/batch\n",
      "Epoch: 407/1000...  Training Step: 813...  Training loss: 0.1344...  1.0100 sec/batch\n",
      "Epoch: 407/1000...  Training Step: 814...  Training loss: 0.1103...  1.0093 sec/batch\n",
      "Epoch: 408/1000...  Training Step: 815...  Training loss: 0.1288...  1.0098 sec/batch\n",
      "Epoch: 408/1000...  Training Step: 816...  Training loss: 0.1063...  1.0099 sec/batch\n",
      "Epoch: 409/1000...  Training Step: 817...  Training loss: 0.1243...  1.0098 sec/batch\n",
      "Epoch: 409/1000...  Training Step: 818...  Training loss: 0.1011...  1.0096 sec/batch\n",
      "Epoch: 410/1000...  Training Step: 819...  Training loss: 0.1209...  1.0096 sec/batch\n",
      "Epoch: 410/1000...  Training Step: 820...  Training loss: 0.0985...  1.0098 sec/batch\n",
      "Epoch: 411/1000...  Training Step: 821...  Training loss: 0.1170...  1.0103 sec/batch\n",
      "Epoch: 411/1000...  Training Step: 822...  Training loss: 0.0937...  1.0100 sec/batch\n",
      "Epoch: 412/1000...  Training Step: 823...  Training loss: 0.1157...  1.0107 sec/batch\n",
      "Epoch: 412/1000...  Training Step: 824...  Training loss: 0.0916...  1.0113 sec/batch\n",
      "Epoch: 413/1000...  Training Step: 825...  Training loss: 0.1104...  1.0103 sec/batch\n",
      "Epoch: 413/1000...  Training Step: 826...  Training loss: 0.0909...  1.0107 sec/batch\n",
      "Epoch: 414/1000...  Training Step: 827...  Training loss: 0.1073...  1.0103 sec/batch\n",
      "Epoch: 414/1000...  Training Step: 828...  Training loss: 0.0863...  1.0102 sec/batch\n",
      "Epoch: 415/1000...  Training Step: 829...  Training loss: 0.1058...  1.0098 sec/batch\n",
      "Epoch: 415/1000...  Training Step: 830...  Training loss: 0.0834...  1.0099 sec/batch\n",
      "Epoch: 416/1000...  Training Step: 831...  Training loss: 0.1021...  1.0096 sec/batch\n",
      "Epoch: 416/1000...  Training Step: 832...  Training loss: 0.0804...  1.0099 sec/batch\n",
      "Epoch: 417/1000...  Training Step: 833...  Training loss: 0.1003...  1.0101 sec/batch\n",
      "Epoch: 417/1000...  Training Step: 834...  Training loss: 0.0769...  1.0089 sec/batch\n",
      "Epoch: 418/1000...  Training Step: 835...  Training loss: 0.0989...  1.0100 sec/batch\n",
      "Epoch: 418/1000...  Training Step: 836...  Training loss: 0.0742...  1.0103 sec/batch\n",
      "Epoch: 419/1000...  Training Step: 837...  Training loss: 0.0960...  1.0097 sec/batch\n",
      "Epoch: 419/1000...  Training Step: 838...  Training loss: 0.0732...  1.0086 sec/batch\n",
      "Epoch: 420/1000...  Training Step: 839...  Training loss: 0.0926...  1.0107 sec/batch\n",
      "Epoch: 420/1000...  Training Step: 840...  Training loss: 0.0718...  1.0099 sec/batch\n",
      "Epoch: 421/1000...  Training Step: 841...  Training loss: 0.0907...  1.0087 sec/batch\n",
      "Epoch: 421/1000...  Training Step: 842...  Training loss: 0.0689...  1.0099 sec/batch\n",
      "Epoch: 422/1000...  Training Step: 843...  Training loss: 0.0884...  1.0098 sec/batch\n",
      "Epoch: 422/1000...  Training Step: 844...  Training loss: 0.0671...  1.0100 sec/batch\n",
      "Epoch: 423/1000...  Training Step: 845...  Training loss: 0.0874...  1.0100 sec/batch\n",
      "Epoch: 423/1000...  Training Step: 846...  Training loss: 0.0651...  1.0109 sec/batch\n",
      "Epoch: 424/1000...  Training Step: 847...  Training loss: 0.0850...  1.0091 sec/batch\n",
      "Epoch: 424/1000...  Training Step: 848...  Training loss: 0.0643...  1.0096 sec/batch\n",
      "Epoch: 425/1000...  Training Step: 849...  Training loss: 0.0823...  1.0103 sec/batch\n",
      "Epoch: 425/1000...  Training Step: 850...  Training loss: 0.0627...  1.0090 sec/batch\n",
      "Epoch: 426/1000...  Training Step: 851...  Training loss: 0.0810...  1.0098 sec/batch\n",
      "Epoch: 426/1000...  Training Step: 852...  Training loss: 0.0600...  1.0079 sec/batch\n",
      "Epoch: 427/1000...  Training Step: 853...  Training loss: 0.0799...  1.0098 sec/batch\n",
      "Epoch: 427/1000...  Training Step: 854...  Training loss: 0.0578...  1.0105 sec/batch\n",
      "Epoch: 428/1000...  Training Step: 855...  Training loss: 0.0778...  1.0093 sec/batch\n",
      "Epoch: 428/1000...  Training Step: 856...  Training loss: 0.0563...  1.0109 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429/1000...  Training Step: 857...  Training loss: 0.0761...  1.0101 sec/batch\n",
      "Epoch: 429/1000...  Training Step: 858...  Training loss: 0.0548...  1.0098 sec/batch\n",
      "Epoch: 430/1000...  Training Step: 859...  Training loss: 0.0742...  1.0092 sec/batch\n",
      "Epoch: 430/1000...  Training Step: 860...  Training loss: 0.0538...  1.0096 sec/batch\n",
      "Epoch: 431/1000...  Training Step: 861...  Training loss: 0.0712...  1.0098 sec/batch\n",
      "Epoch: 431/1000...  Training Step: 862...  Training loss: 0.0526...  1.0104 sec/batch\n",
      "Epoch: 432/1000...  Training Step: 863...  Training loss: 0.0699...  1.0097 sec/batch\n",
      "Epoch: 432/1000...  Training Step: 864...  Training loss: 0.0506...  1.0097 sec/batch\n",
      "Epoch: 433/1000...  Training Step: 865...  Training loss: 0.0692...  1.0097 sec/batch\n",
      "Epoch: 433/1000...  Training Step: 866...  Training loss: 0.0488...  1.0100 sec/batch\n",
      "Epoch: 434/1000...  Training Step: 867...  Training loss: 0.0680...  1.0098 sec/batch\n",
      "Epoch: 434/1000...  Training Step: 868...  Training loss: 0.0476...  1.0104 sec/batch\n",
      "Epoch: 435/1000...  Training Step: 869...  Training loss: 0.0670...  1.0103 sec/batch\n",
      "Epoch: 435/1000...  Training Step: 870...  Training loss: 0.0464...  1.0099 sec/batch\n",
      "Epoch: 436/1000...  Training Step: 871...  Training loss: 0.0660...  1.0103 sec/batch\n",
      "Epoch: 436/1000...  Training Step: 872...  Training loss: 0.0459...  1.0103 sec/batch\n",
      "Epoch: 437/1000...  Training Step: 873...  Training loss: 0.0652...  1.0096 sec/batch\n",
      "Epoch: 437/1000...  Training Step: 874...  Training loss: 0.0445...  1.0087 sec/batch\n",
      "Epoch: 438/1000...  Training Step: 875...  Training loss: 0.0643...  1.0098 sec/batch\n",
      "Epoch: 438/1000...  Training Step: 876...  Training loss: 0.0438...  1.0107 sec/batch\n",
      "Epoch: 439/1000...  Training Step: 877...  Training loss: 0.0635...  1.0095 sec/batch\n",
      "Epoch: 439/1000...  Training Step: 878...  Training loss: 0.0430...  1.0098 sec/batch\n",
      "Epoch: 440/1000...  Training Step: 879...  Training loss: 0.0625...  1.0097 sec/batch\n",
      "Epoch: 440/1000...  Training Step: 880...  Training loss: 0.0425...  1.0098 sec/batch\n",
      "Epoch: 441/1000...  Training Step: 881...  Training loss: 0.0614...  1.0132 sec/batch\n",
      "Epoch: 441/1000...  Training Step: 882...  Training loss: 0.0416...  1.0083 sec/batch\n",
      "Epoch: 442/1000...  Training Step: 883...  Training loss: 0.0608...  1.0136 sec/batch\n",
      "Epoch: 442/1000...  Training Step: 884...  Training loss: 0.0409...  1.0114 sec/batch\n",
      "Epoch: 443/1000...  Training Step: 885...  Training loss: 0.0599...  1.0095 sec/batch\n",
      "Epoch: 443/1000...  Training Step: 886...  Training loss: 0.0401...  1.0089 sec/batch\n",
      "Epoch: 444/1000...  Training Step: 887...  Training loss: 0.0587...  1.0093 sec/batch\n",
      "Epoch: 444/1000...  Training Step: 888...  Training loss: 0.0396...  1.0097 sec/batch\n",
      "Epoch: 445/1000...  Training Step: 889...  Training loss: 0.0576...  1.0098 sec/batch\n",
      "Epoch: 445/1000...  Training Step: 890...  Training loss: 0.0385...  1.0104 sec/batch\n",
      "Epoch: 446/1000...  Training Step: 891...  Training loss: 0.0565...  1.0100 sec/batch\n",
      "Epoch: 446/1000...  Training Step: 892...  Training loss: 0.0376...  1.0104 sec/batch\n",
      "Epoch: 447/1000...  Training Step: 893...  Training loss: 0.0555...  1.0094 sec/batch\n",
      "Epoch: 447/1000...  Training Step: 894...  Training loss: 0.0366...  1.0099 sec/batch\n",
      "Epoch: 448/1000...  Training Step: 895...  Training loss: 0.0551...  1.0103 sec/batch\n",
      "Epoch: 448/1000...  Training Step: 896...  Training loss: 0.0362...  1.0099 sec/batch\n",
      "Epoch: 449/1000...  Training Step: 897...  Training loss: 0.0545...  1.0084 sec/batch\n",
      "Epoch: 449/1000...  Training Step: 898...  Training loss: 0.0352...  1.0108 sec/batch\n",
      "Epoch: 450/1000...  Training Step: 899...  Training loss: 0.0540...  1.0097 sec/batch\n",
      "Epoch: 450/1000...  Training Step: 900...  Training loss: 0.0350...  1.0091 sec/batch\n",
      "Epoch: 451/1000...  Training Step: 901...  Training loss: 0.0529...  1.0103 sec/batch\n",
      "Epoch: 451/1000...  Training Step: 902...  Training loss: 0.0346...  1.0093 sec/batch\n",
      "Epoch: 452/1000...  Training Step: 903...  Training loss: 0.0520...  1.0093 sec/batch\n",
      "Epoch: 452/1000...  Training Step: 904...  Training loss: 0.0340...  1.0094 sec/batch\n",
      "Epoch: 453/1000...  Training Step: 905...  Training loss: 0.0514...  1.0092 sec/batch\n",
      "Epoch: 453/1000...  Training Step: 906...  Training loss: 0.0328...  1.0102 sec/batch\n",
      "Epoch: 454/1000...  Training Step: 907...  Training loss: 0.0507...  1.0083 sec/batch\n",
      "Epoch: 454/1000...  Training Step: 908...  Training loss: 0.0320...  1.0097 sec/batch\n",
      "Epoch: 455/1000...  Training Step: 909...  Training loss: 0.0498...  1.0097 sec/batch\n",
      "Epoch: 455/1000...  Training Step: 910...  Training loss: 0.0313...  1.0104 sec/batch\n",
      "Epoch: 456/1000...  Training Step: 911...  Training loss: 0.0491...  1.0100 sec/batch\n",
      "Epoch: 456/1000...  Training Step: 912...  Training loss: 0.0305...  1.0102 sec/batch\n",
      "Epoch: 457/1000...  Training Step: 913...  Training loss: 0.0487...  1.0098 sec/batch\n",
      "Epoch: 457/1000...  Training Step: 914...  Training loss: 0.0297...  1.0101 sec/batch\n",
      "Epoch: 458/1000...  Training Step: 915...  Training loss: 0.0478...  1.0102 sec/batch\n",
      "Epoch: 458/1000...  Training Step: 916...  Training loss: 0.0294...  1.0103 sec/batch\n",
      "Epoch: 459/1000...  Training Step: 917...  Training loss: 0.0469...  1.0096 sec/batch\n",
      "Epoch: 459/1000...  Training Step: 918...  Training loss: 0.0289...  1.0083 sec/batch\n",
      "Epoch: 460/1000...  Training Step: 919...  Training loss: 0.0462...  1.0100 sec/batch\n",
      "Epoch: 460/1000...  Training Step: 920...  Training loss: 0.0283...  1.0098 sec/batch\n",
      "Epoch: 461/1000...  Training Step: 921...  Training loss: 0.0459...  1.0086 sec/batch\n",
      "Epoch: 461/1000...  Training Step: 922...  Training loss: 0.0277...  1.0087 sec/batch\n",
      "Epoch: 462/1000...  Training Step: 923...  Training loss: 0.0455...  1.0094 sec/batch\n",
      "Epoch: 462/1000...  Training Step: 924...  Training loss: 0.0276...  1.0087 sec/batch\n",
      "Epoch: 463/1000...  Training Step: 925...  Training loss: 0.0449...  1.0104 sec/batch\n",
      "Epoch: 463/1000...  Training Step: 926...  Training loss: 0.0272...  1.0102 sec/batch\n",
      "Epoch: 464/1000...  Training Step: 927...  Training loss: 0.0445...  1.0082 sec/batch\n",
      "Epoch: 464/1000...  Training Step: 928...  Training loss: 0.0268...  1.0099 sec/batch\n",
      "Epoch: 465/1000...  Training Step: 929...  Training loss: 0.0439...  1.0091 sec/batch\n",
      "Epoch: 465/1000...  Training Step: 930...  Training loss: 0.0264...  1.0093 sec/batch\n",
      "Epoch: 466/1000...  Training Step: 931...  Training loss: 0.0434...  1.0096 sec/batch\n",
      "Epoch: 466/1000...  Training Step: 932...  Training loss: 0.0256...  1.0096 sec/batch\n",
      "Epoch: 467/1000...  Training Step: 933...  Training loss: 0.0429...  1.0117 sec/batch\n",
      "Epoch: 467/1000...  Training Step: 934...  Training loss: 0.0252...  1.0099 sec/batch\n",
      "Epoch: 468/1000...  Training Step: 935...  Training loss: 0.0425...  1.0098 sec/batch\n",
      "Epoch: 468/1000...  Training Step: 936...  Training loss: 0.0247...  1.0089 sec/batch\n",
      "Epoch: 469/1000...  Training Step: 937...  Training loss: 0.0419...  1.0106 sec/batch\n",
      "Epoch: 469/1000...  Training Step: 938...  Training loss: 0.0243...  1.0104 sec/batch\n",
      "Epoch: 470/1000...  Training Step: 939...  Training loss: 0.0415...  1.0101 sec/batch\n",
      "Epoch: 470/1000...  Training Step: 940...  Training loss: 0.0239...  1.0103 sec/batch\n",
      "Epoch: 471/1000...  Training Step: 941...  Training loss: 0.0410...  1.0099 sec/batch\n",
      "Epoch: 471/1000...  Training Step: 942...  Training loss: 0.0234...  1.0096 sec/batch\n",
      "Epoch: 472/1000...  Training Step: 943...  Training loss: 0.0405...  1.0105 sec/batch\n",
      "Epoch: 472/1000...  Training Step: 944...  Training loss: 0.0231...  1.0102 sec/batch\n",
      "Epoch: 473/1000...  Training Step: 945...  Training loss: 0.0401...  1.0102 sec/batch\n",
      "Epoch: 473/1000...  Training Step: 946...  Training loss: 0.0228...  1.0085 sec/batch\n",
      "Epoch: 474/1000...  Training Step: 947...  Training loss: 0.0397...  1.0096 sec/batch\n",
      "Epoch: 474/1000...  Training Step: 948...  Training loss: 0.0225...  1.0097 sec/batch\n",
      "Epoch: 475/1000...  Training Step: 949...  Training loss: 0.0395...  1.0103 sec/batch\n",
      "Epoch: 475/1000...  Training Step: 950...  Training loss: 0.0222...  1.0094 sec/batch\n",
      "Epoch: 476/1000...  Training Step: 951...  Training loss: 0.0391...  1.0091 sec/batch\n",
      "Epoch: 476/1000...  Training Step: 952...  Training loss: 0.0222...  1.0098 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477/1000...  Training Step: 953...  Training loss: 0.0386...  1.0106 sec/batch\n",
      "Epoch: 477/1000...  Training Step: 954...  Training loss: 0.0218...  1.0099 sec/batch\n",
      "Epoch: 478/1000...  Training Step: 955...  Training loss: 0.0386...  1.0092 sec/batch\n",
      "Epoch: 478/1000...  Training Step: 956...  Training loss: 0.0216...  1.0100 sec/batch\n",
      "Epoch: 479/1000...  Training Step: 957...  Training loss: 0.0384...  1.0097 sec/batch\n",
      "Epoch: 479/1000...  Training Step: 958...  Training loss: 0.0214...  1.0097 sec/batch\n",
      "Epoch: 480/1000...  Training Step: 959...  Training loss: 0.0380...  1.0100 sec/batch\n",
      "Epoch: 480/1000...  Training Step: 960...  Training loss: 0.0217...  1.0111 sec/batch\n",
      "Epoch: 481/1000...  Training Step: 961...  Training loss: 0.0379...  1.0098 sec/batch\n",
      "Epoch: 481/1000...  Training Step: 962...  Training loss: 0.0215...  1.0088 sec/batch\n",
      "Epoch: 482/1000...  Training Step: 963...  Training loss: 0.0381...  1.0104 sec/batch\n",
      "Epoch: 482/1000...  Training Step: 964...  Training loss: 0.0211...  1.0100 sec/batch\n",
      "Epoch: 483/1000...  Training Step: 965...  Training loss: 0.0380...  1.0106 sec/batch\n",
      "Epoch: 483/1000...  Training Step: 966...  Training loss: 0.0210...  1.0101 sec/batch\n",
      "Epoch: 484/1000...  Training Step: 967...  Training loss: 0.0378...  1.0108 sec/batch\n",
      "Epoch: 484/1000...  Training Step: 968...  Training loss: 0.0207...  1.0101 sec/batch\n",
      "Epoch: 485/1000...  Training Step: 969...  Training loss: 0.0377...  1.0101 sec/batch\n",
      "Epoch: 485/1000...  Training Step: 970...  Training loss: 0.0206...  1.0102 sec/batch\n",
      "Epoch: 486/1000...  Training Step: 971...  Training loss: 0.0372...  1.0095 sec/batch\n",
      "Epoch: 486/1000...  Training Step: 972...  Training loss: 0.0210...  1.0103 sec/batch\n",
      "Epoch: 487/1000...  Training Step: 973...  Training loss: 0.0367...  1.0097 sec/batch\n",
      "Epoch: 487/1000...  Training Step: 974...  Training loss: 0.0208...  1.0092 sec/batch\n",
      "Epoch: 488/1000...  Training Step: 975...  Training loss: 0.0364...  1.0089 sec/batch\n",
      "Epoch: 488/1000...  Training Step: 976...  Training loss: 0.0203...  1.0094 sec/batch\n",
      "Epoch: 489/1000...  Training Step: 977...  Training loss: 0.0363...  1.0105 sec/batch\n",
      "Epoch: 489/1000...  Training Step: 978...  Training loss: 0.0201...  1.0097 sec/batch\n",
      "Epoch: 490/1000...  Training Step: 979...  Training loss: 0.0364...  1.0105 sec/batch\n",
      "Epoch: 490/1000...  Training Step: 980...  Training loss: 0.0197...  1.0093 sec/batch\n",
      "Epoch: 491/1000...  Training Step: 981...  Training loss: 0.0356...  1.0109 sec/batch\n",
      "Epoch: 491/1000...  Training Step: 982...  Training loss: 0.0196...  1.0111 sec/batch\n",
      "Epoch: 492/1000...  Training Step: 983...  Training loss: 0.0354...  1.0100 sec/batch\n",
      "Epoch: 492/1000...  Training Step: 984...  Training loss: 0.0192...  1.0099 sec/batch\n",
      "Epoch: 493/1000...  Training Step: 985...  Training loss: 0.0352...  1.0101 sec/batch\n",
      "Epoch: 493/1000...  Training Step: 986...  Training loss: 0.0189...  1.0096 sec/batch\n",
      "Epoch: 494/1000...  Training Step: 987...  Training loss: 0.0349...  1.0097 sec/batch\n",
      "Epoch: 494/1000...  Training Step: 988...  Training loss: 0.0185...  1.0098 sec/batch\n",
      "Epoch: 495/1000...  Training Step: 989...  Training loss: 0.0345...  1.0098 sec/batch\n",
      "Epoch: 495/1000...  Training Step: 990...  Training loss: 0.0185...  1.0095 sec/batch\n",
      "Epoch: 496/1000...  Training Step: 991...  Training loss: 0.0343...  1.0095 sec/batch\n",
      "Epoch: 496/1000...  Training Step: 992...  Training loss: 0.0182...  1.0100 sec/batch\n",
      "Epoch: 497/1000...  Training Step: 993...  Training loss: 0.0338...  1.0102 sec/batch\n",
      "Epoch: 497/1000...  Training Step: 994...  Training loss: 0.0179...  1.0100 sec/batch\n",
      "Epoch: 498/1000...  Training Step: 995...  Training loss: 0.0334...  1.0098 sec/batch\n",
      "Epoch: 498/1000...  Training Step: 996...  Training loss: 0.0177...  1.0100 sec/batch\n",
      "Epoch: 499/1000...  Training Step: 997...  Training loss: 0.0331...  1.0102 sec/batch\n",
      "Epoch: 499/1000...  Training Step: 998...  Training loss: 0.0172...  1.0103 sec/batch\n",
      "Epoch: 500/1000...  Training Step: 999...  Training loss: 0.0329...  1.0104 sec/batch\n",
      "Epoch: 500/1000...  Training Step: 1000...  Training loss: 0.0169...  1.0156 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 501/1000...  Training Step: 1001...  Training loss: 0.0326...  1.0098 sec/batch\n",
      "Epoch: 501/1000...  Training Step: 1002...  Training loss: 0.0164...  1.0149 sec/batch\n",
      "Epoch: 502/1000...  Training Step: 1003...  Training loss: 0.0322...  1.0084 sec/batch\n",
      "Epoch: 502/1000...  Training Step: 1004...  Training loss: 0.0163...  1.0101 sec/batch\n",
      "Epoch: 503/1000...  Training Step: 1005...  Training loss: 0.0318...  1.0105 sec/batch\n",
      "Epoch: 503/1000...  Training Step: 1006...  Training loss: 0.0161...  1.0103 sec/batch\n",
      "Epoch: 504/1000...  Training Step: 1007...  Training loss: 0.0314...  1.0107 sec/batch\n",
      "Epoch: 504/1000...  Training Step: 1008...  Training loss: 0.0158...  1.0101 sec/batch\n",
      "Epoch: 505/1000...  Training Step: 1009...  Training loss: 0.0311...  1.0097 sec/batch\n",
      "Epoch: 505/1000...  Training Step: 1010...  Training loss: 0.0153...  1.0097 sec/batch\n",
      "Epoch: 506/1000...  Training Step: 1011...  Training loss: 0.0308...  1.0092 sec/batch\n",
      "Epoch: 506/1000...  Training Step: 1012...  Training loss: 0.0151...  1.0092 sec/batch\n",
      "Epoch: 507/1000...  Training Step: 1013...  Training loss: 0.0306...  1.0099 sec/batch\n",
      "Epoch: 507/1000...  Training Step: 1014...  Training loss: 0.0148...  1.0095 sec/batch\n",
      "Epoch: 508/1000...  Training Step: 1015...  Training loss: 0.0303...  1.0109 sec/batch\n",
      "Epoch: 508/1000...  Training Step: 1016...  Training loss: 0.0145...  1.0106 sec/batch\n",
      "Epoch: 509/1000...  Training Step: 1017...  Training loss: 0.0301...  1.0103 sec/batch\n",
      "Epoch: 509/1000...  Training Step: 1018...  Training loss: 0.0143...  1.0102 sec/batch\n",
      "Epoch: 510/1000...  Training Step: 1019...  Training loss: 0.0299...  1.0114 sec/batch\n",
      "Epoch: 510/1000...  Training Step: 1020...  Training loss: 0.0141...  1.0099 sec/batch\n",
      "Epoch: 511/1000...  Training Step: 1021...  Training loss: 0.0296...  1.0110 sec/batch\n",
      "Epoch: 511/1000...  Training Step: 1022...  Training loss: 0.0140...  1.0097 sec/batch\n",
      "Epoch: 512/1000...  Training Step: 1023...  Training loss: 0.0293...  1.0092 sec/batch\n",
      "Epoch: 512/1000...  Training Step: 1024...  Training loss: 0.0138...  1.0097 sec/batch\n",
      "Epoch: 513/1000...  Training Step: 1025...  Training loss: 0.0291...  1.0097 sec/batch\n",
      "Epoch: 513/1000...  Training Step: 1026...  Training loss: 0.0136...  1.0098 sec/batch\n",
      "Epoch: 514/1000...  Training Step: 1027...  Training loss: 0.0289...  1.0098 sec/batch\n",
      "Epoch: 514/1000...  Training Step: 1028...  Training loss: 0.0134...  1.0086 sec/batch\n",
      "Epoch: 515/1000...  Training Step: 1029...  Training loss: 0.0287...  1.0090 sec/batch\n",
      "Epoch: 515/1000...  Training Step: 1030...  Training loss: 0.0132...  1.0097 sec/batch\n",
      "Epoch: 516/1000...  Training Step: 1031...  Training loss: 0.0285...  1.0091 sec/batch\n",
      "Epoch: 516/1000...  Training Step: 1032...  Training loss: 0.0131...  1.0097 sec/batch\n",
      "Epoch: 517/1000...  Training Step: 1033...  Training loss: 0.0283...  1.0097 sec/batch\n",
      "Epoch: 517/1000...  Training Step: 1034...  Training loss: 0.0129...  1.0084 sec/batch\n",
      "Epoch: 518/1000...  Training Step: 1035...  Training loss: 0.0280...  1.0096 sec/batch\n",
      "Epoch: 518/1000...  Training Step: 1036...  Training loss: 0.0128...  1.0098 sec/batch\n",
      "Epoch: 519/1000...  Training Step: 1037...  Training loss: 0.0279...  1.0096 sec/batch\n",
      "Epoch: 519/1000...  Training Step: 1038...  Training loss: 0.0127...  1.0135 sec/batch\n",
      "Epoch: 520/1000...  Training Step: 1039...  Training loss: 0.0277...  1.0096 sec/batch\n",
      "Epoch: 520/1000...  Training Step: 1040...  Training loss: 0.0125...  1.0101 sec/batch\n",
      "Epoch: 521/1000...  Training Step: 1041...  Training loss: 0.0276...  1.0108 sec/batch\n",
      "Epoch: 521/1000...  Training Step: 1042...  Training loss: 0.0124...  1.0099 sec/batch\n",
      "Epoch: 522/1000...  Training Step: 1043...  Training loss: 0.0274...  1.0103 sec/batch\n",
      "Epoch: 522/1000...  Training Step: 1044...  Training loss: 0.0123...  1.0103 sec/batch\n",
      "Epoch: 523/1000...  Training Step: 1045...  Training loss: 0.0272...  1.0102 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 523/1000...  Training Step: 1046...  Training loss: 0.0122...  1.0104 sec/batch\n",
      "Epoch: 524/1000...  Training Step: 1047...  Training loss: 0.0271...  1.0097 sec/batch\n",
      "Epoch: 524/1000...  Training Step: 1048...  Training loss: 0.0121...  1.0102 sec/batch\n",
      "Epoch: 525/1000...  Training Step: 1049...  Training loss: 0.0269...  1.0102 sec/batch\n",
      "Epoch: 525/1000...  Training Step: 1050...  Training loss: 0.0119...  1.0092 sec/batch\n",
      "Epoch: 526/1000...  Training Step: 1051...  Training loss: 0.0269...  1.0098 sec/batch\n",
      "Epoch: 526/1000...  Training Step: 1052...  Training loss: 0.0119...  1.0098 sec/batch\n",
      "Epoch: 527/1000...  Training Step: 1053...  Training loss: 0.0267...  1.0104 sec/batch\n",
      "Epoch: 527/1000...  Training Step: 1054...  Training loss: 0.0118...  1.0096 sec/batch\n",
      "Epoch: 528/1000...  Training Step: 1055...  Training loss: 0.0266...  1.0099 sec/batch\n",
      "Epoch: 528/1000...  Training Step: 1056...  Training loss: 0.0119...  1.0097 sec/batch\n",
      "Epoch: 529/1000...  Training Step: 1057...  Training loss: 0.0266...  1.0103 sec/batch\n",
      "Epoch: 529/1000...  Training Step: 1058...  Training loss: 0.0117...  1.0082 sec/batch\n",
      "Epoch: 530/1000...  Training Step: 1059...  Training loss: 0.0266...  1.0098 sec/batch\n",
      "Epoch: 530/1000...  Training Step: 1060...  Training loss: 0.0116...  1.0100 sec/batch\n",
      "Epoch: 531/1000...  Training Step: 1061...  Training loss: 0.0266...  1.0102 sec/batch\n",
      "Epoch: 531/1000...  Training Step: 1062...  Training loss: 0.0118...  1.0124 sec/batch\n",
      "Epoch: 532/1000...  Training Step: 1063...  Training loss: 0.0264...  1.0102 sec/batch\n",
      "Epoch: 532/1000...  Training Step: 1064...  Training loss: 0.0118...  1.0102 sec/batch\n",
      "Epoch: 533/1000...  Training Step: 1065...  Training loss: 0.0261...  1.0103 sec/batch\n",
      "Epoch: 533/1000...  Training Step: 1066...  Training loss: 0.0118...  1.0093 sec/batch\n",
      "Epoch: 534/1000...  Training Step: 1067...  Training loss: 0.0260...  1.0102 sec/batch\n",
      "Epoch: 534/1000...  Training Step: 1068...  Training loss: 0.0117...  1.0093 sec/batch\n",
      "Epoch: 535/1000...  Training Step: 1069...  Training loss: 0.0259...  1.0102 sec/batch\n",
      "Epoch: 535/1000...  Training Step: 1070...  Training loss: 0.0115...  1.0102 sec/batch\n",
      "Epoch: 536/1000...  Training Step: 1071...  Training loss: 0.0258...  1.0102 sec/batch\n",
      "Epoch: 536/1000...  Training Step: 1072...  Training loss: 0.0114...  1.0106 sec/batch\n",
      "Epoch: 537/1000...  Training Step: 1073...  Training loss: 0.0257...  1.0104 sec/batch\n",
      "Epoch: 537/1000...  Training Step: 1074...  Training loss: 0.0114...  1.0103 sec/batch\n",
      "Epoch: 538/1000...  Training Step: 1075...  Training loss: 0.0257...  1.0094 sec/batch\n",
      "Epoch: 538/1000...  Training Step: 1076...  Training loss: 0.0112...  1.0088 sec/batch\n",
      "Epoch: 539/1000...  Training Step: 1077...  Training loss: 0.0256...  1.0096 sec/batch\n",
      "Epoch: 539/1000...  Training Step: 1078...  Training loss: 0.0112...  1.0102 sec/batch\n",
      "Epoch: 540/1000...  Training Step: 1079...  Training loss: 0.0255...  1.0092 sec/batch\n",
      "Epoch: 540/1000...  Training Step: 1080...  Training loss: 0.0112...  1.0101 sec/batch\n",
      "Epoch: 541/1000...  Training Step: 1081...  Training loss: 0.0254...  1.0106 sec/batch\n",
      "Epoch: 541/1000...  Training Step: 1082...  Training loss: 0.0111...  1.0098 sec/batch\n",
      "Epoch: 542/1000...  Training Step: 1083...  Training loss: 0.0253...  1.0101 sec/batch\n",
      "Epoch: 542/1000...  Training Step: 1084...  Training loss: 0.0110...  1.0105 sec/batch\n",
      "Epoch: 543/1000...  Training Step: 1085...  Training loss: 0.0256...  1.0103 sec/batch\n",
      "Epoch: 543/1000...  Training Step: 1086...  Training loss: 0.0108...  1.0099 sec/batch\n",
      "Epoch: 544/1000...  Training Step: 1087...  Training loss: 0.0256...  1.0103 sec/batch\n",
      "Epoch: 544/1000...  Training Step: 1088...  Training loss: 0.0111...  1.0101 sec/batch\n",
      "Epoch: 545/1000...  Training Step: 1089...  Training loss: 0.0255...  1.0097 sec/batch\n",
      "Epoch: 545/1000...  Training Step: 1090...  Training loss: 0.0112...  1.0091 sec/batch\n",
      "Epoch: 546/1000...  Training Step: 1091...  Training loss: 0.0253...  1.0101 sec/batch\n",
      "Epoch: 546/1000...  Training Step: 1092...  Training loss: 0.0112...  1.0100 sec/batch\n",
      "Epoch: 547/1000...  Training Step: 1093...  Training loss: 0.0253...  1.0099 sec/batch\n",
      "Epoch: 547/1000...  Training Step: 1094...  Training loss: 0.0111...  1.0097 sec/batch\n",
      "Epoch: 548/1000...  Training Step: 1095...  Training loss: 0.0251...  1.0097 sec/batch\n",
      "Epoch: 548/1000...  Training Step: 1096...  Training loss: 0.0112...  1.0099 sec/batch\n",
      "Epoch: 549/1000...  Training Step: 1097...  Training loss: 0.0249...  1.0101 sec/batch\n",
      "Epoch: 549/1000...  Training Step: 1098...  Training loss: 0.0112...  1.0093 sec/batch\n",
      "Epoch: 550/1000...  Training Step: 1099...  Training loss: 0.0249...  1.0091 sec/batch\n",
      "Epoch: 550/1000...  Training Step: 1100...  Training loss: 0.0114...  1.0094 sec/batch\n",
      "Epoch: 551/1000...  Training Step: 1101...  Training loss: 0.0248...  1.0103 sec/batch\n",
      "Epoch: 551/1000...  Training Step: 1102...  Training loss: 0.0112...  1.0100 sec/batch\n",
      "Epoch: 552/1000...  Training Step: 1103...  Training loss: 0.0247...  1.0089 sec/batch\n",
      "Epoch: 552/1000...  Training Step: 1104...  Training loss: 0.0115...  1.0099 sec/batch\n",
      "Epoch: 553/1000...  Training Step: 1105...  Training loss: 0.0247...  1.0099 sec/batch\n",
      "Epoch: 553/1000...  Training Step: 1106...  Training loss: 0.0117...  1.0103 sec/batch\n",
      "Epoch: 554/1000...  Training Step: 1107...  Training loss: 0.0245...  1.0099 sec/batch\n",
      "Epoch: 554/1000...  Training Step: 1108...  Training loss: 0.0114...  1.0087 sec/batch\n",
      "Epoch: 555/1000...  Training Step: 1109...  Training loss: 0.0245...  1.0100 sec/batch\n",
      "Epoch: 555/1000...  Training Step: 1110...  Training loss: 0.0114...  1.0100 sec/batch\n",
      "Epoch: 556/1000...  Training Step: 1111...  Training loss: 0.0244...  1.0106 sec/batch\n",
      "Epoch: 556/1000...  Training Step: 1112...  Training loss: 0.0112...  1.0097 sec/batch\n",
      "Epoch: 557/1000...  Training Step: 1113...  Training loss: 0.0242...  1.0095 sec/batch\n",
      "Epoch: 557/1000...  Training Step: 1114...  Training loss: 0.0110...  1.0105 sec/batch\n",
      "Epoch: 558/1000...  Training Step: 1115...  Training loss: 0.0239...  1.0099 sec/batch\n",
      "Epoch: 558/1000...  Training Step: 1116...  Training loss: 0.0106...  1.0099 sec/batch\n",
      "Epoch: 559/1000...  Training Step: 1117...  Training loss: 0.0237...  1.0096 sec/batch\n",
      "Epoch: 559/1000...  Training Step: 1118...  Training loss: 0.0102...  1.0147 sec/batch\n",
      "Epoch: 560/1000...  Training Step: 1119...  Training loss: 0.0236...  1.0088 sec/batch\n",
      "Epoch: 560/1000...  Training Step: 1120...  Training loss: 0.0099...  1.0149 sec/batch\n",
      "Epoch: 561/1000...  Training Step: 1121...  Training loss: 0.0233...  1.0104 sec/batch\n",
      "Epoch: 561/1000...  Training Step: 1122...  Training loss: 0.0096...  1.0099 sec/batch\n",
      "Epoch: 562/1000...  Training Step: 1123...  Training loss: 0.0230...  1.0099 sec/batch\n",
      "Epoch: 562/1000...  Training Step: 1124...  Training loss: 0.0094...  1.0101 sec/batch\n",
      "Epoch: 563/1000...  Training Step: 1125...  Training loss: 0.0228...  1.0101 sec/batch\n",
      "Epoch: 563/1000...  Training Step: 1126...  Training loss: 0.0091...  1.0094 sec/batch\n",
      "Epoch: 564/1000...  Training Step: 1127...  Training loss: 0.0227...  1.0108 sec/batch\n",
      "Epoch: 564/1000...  Training Step: 1128...  Training loss: 0.0089...  1.0106 sec/batch\n",
      "Epoch: 565/1000...  Training Step: 1129...  Training loss: 0.0225...  1.0105 sec/batch\n",
      "Epoch: 565/1000...  Training Step: 1130...  Training loss: 0.0087...  1.0103 sec/batch\n",
      "Epoch: 566/1000...  Training Step: 1131...  Training loss: 0.0223...  1.0101 sec/batch\n",
      "Epoch: 566/1000...  Training Step: 1132...  Training loss: 0.0086...  1.0100 sec/batch\n",
      "Epoch: 567/1000...  Training Step: 1133...  Training loss: 0.0221...  1.0099 sec/batch\n",
      "Epoch: 567/1000...  Training Step: 1134...  Training loss: 0.0084...  1.0087 sec/batch\n",
      "Epoch: 568/1000...  Training Step: 1135...  Training loss: 0.0219...  1.0097 sec/batch\n",
      "Epoch: 568/1000...  Training Step: 1136...  Training loss: 0.0083...  1.0103 sec/batch\n",
      "Epoch: 569/1000...  Training Step: 1137...  Training loss: 0.0218...  1.0100 sec/batch\n",
      "Epoch: 569/1000...  Training Step: 1138...  Training loss: 0.0082...  1.0104 sec/batch\n",
      "Epoch: 570/1000...  Training Step: 1139...  Training loss: 0.0217...  1.0100 sec/batch\n",
      "Epoch: 570/1000...  Training Step: 1140...  Training loss: 0.0081...  1.0097 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 571/1000...  Training Step: 1141...  Training loss: 0.0215...  1.0104 sec/batch\n",
      "Epoch: 571/1000...  Training Step: 1142...  Training loss: 0.0080...  1.0096 sec/batch\n",
      "Epoch: 572/1000...  Training Step: 1143...  Training loss: 0.0214...  1.0103 sec/batch\n",
      "Epoch: 572/1000...  Training Step: 1144...  Training loss: 0.0079...  1.0107 sec/batch\n",
      "Epoch: 573/1000...  Training Step: 1145...  Training loss: 0.0213...  1.0103 sec/batch\n",
      "Epoch: 573/1000...  Training Step: 1146...  Training loss: 0.0078...  1.0122 sec/batch\n",
      "Epoch: 574/1000...  Training Step: 1147...  Training loss: 0.0212...  1.0109 sec/batch\n",
      "Epoch: 574/1000...  Training Step: 1148...  Training loss: 0.0077...  1.0103 sec/batch\n",
      "Epoch: 575/1000...  Training Step: 1149...  Training loss: 0.0211...  1.0100 sec/batch\n",
      "Epoch: 575/1000...  Training Step: 1150...  Training loss: 0.0076...  1.0102 sec/batch\n",
      "Epoch: 576/1000...  Training Step: 1151...  Training loss: 0.0210...  1.0100 sec/batch\n",
      "Epoch: 576/1000...  Training Step: 1152...  Training loss: 0.0076...  1.0101 sec/batch\n",
      "Epoch: 577/1000...  Training Step: 1153...  Training loss: 0.0208...  1.0093 sec/batch\n",
      "Epoch: 577/1000...  Training Step: 1154...  Training loss: 0.0075...  1.0103 sec/batch\n",
      "Epoch: 578/1000...  Training Step: 1155...  Training loss: 0.0208...  1.0102 sec/batch\n",
      "Epoch: 578/1000...  Training Step: 1156...  Training loss: 0.0074...  1.0097 sec/batch\n",
      "Epoch: 579/1000...  Training Step: 1157...  Training loss: 0.0207...  1.0095 sec/batch\n",
      "Epoch: 579/1000...  Training Step: 1158...  Training loss: 0.0074...  1.0107 sec/batch\n",
      "Epoch: 580/1000...  Training Step: 1159...  Training loss: 0.0206...  1.0097 sec/batch\n",
      "Epoch: 580/1000...  Training Step: 1160...  Training loss: 0.0073...  1.0104 sec/batch\n",
      "Epoch: 581/1000...  Training Step: 1161...  Training loss: 0.0205...  1.0096 sec/batch\n",
      "Epoch: 581/1000...  Training Step: 1162...  Training loss: 0.0072...  1.0095 sec/batch\n",
      "Epoch: 582/1000...  Training Step: 1163...  Training loss: 0.0204...  1.0101 sec/batch\n",
      "Epoch: 582/1000...  Training Step: 1164...  Training loss: 0.0072...  1.0097 sec/batch\n",
      "Epoch: 583/1000...  Training Step: 1165...  Training loss: 0.0203...  1.0084 sec/batch\n",
      "Epoch: 583/1000...  Training Step: 1166...  Training loss: 0.0071...  1.0082 sec/batch\n",
      "Epoch: 584/1000...  Training Step: 1167...  Training loss: 0.0202...  1.0096 sec/batch\n",
      "Epoch: 584/1000...  Training Step: 1168...  Training loss: 0.0071...  1.0110 sec/batch\n",
      "Epoch: 585/1000...  Training Step: 1169...  Training loss: 0.0201...  1.0096 sec/batch\n",
      "Epoch: 585/1000...  Training Step: 1170...  Training loss: 0.0070...  1.0101 sec/batch\n",
      "Epoch: 586/1000...  Training Step: 1171...  Training loss: 0.0200...  1.0107 sec/batch\n",
      "Epoch: 586/1000...  Training Step: 1172...  Training loss: 0.0070...  1.0102 sec/batch\n",
      "Epoch: 587/1000...  Training Step: 1173...  Training loss: 0.0200...  1.0102 sec/batch\n",
      "Epoch: 587/1000...  Training Step: 1174...  Training loss: 0.0069...  1.0099 sec/batch\n",
      "Epoch: 588/1000...  Training Step: 1175...  Training loss: 0.0199...  1.0097 sec/batch\n",
      "Epoch: 588/1000...  Training Step: 1176...  Training loss: 0.0069...  1.0102 sec/batch\n",
      "Epoch: 589/1000...  Training Step: 1177...  Training loss: 0.0198...  1.0093 sec/batch\n",
      "Epoch: 589/1000...  Training Step: 1178...  Training loss: 0.0068...  1.0095 sec/batch\n",
      "Epoch: 590/1000...  Training Step: 1179...  Training loss: 0.0197...  1.0104 sec/batch\n",
      "Epoch: 590/1000...  Training Step: 1180...  Training loss: 0.0068...  1.0117 sec/batch\n",
      "Epoch: 591/1000...  Training Step: 1181...  Training loss: 0.0196...  1.0093 sec/batch\n",
      "Epoch: 591/1000...  Training Step: 1182...  Training loss: 0.0067...  1.0097 sec/batch\n",
      "Epoch: 592/1000...  Training Step: 1183...  Training loss: 0.0196...  1.0095 sec/batch\n",
      "Epoch: 592/1000...  Training Step: 1184...  Training loss: 0.0067...  1.0103 sec/batch\n",
      "Epoch: 593/1000...  Training Step: 1185...  Training loss: 0.0195...  1.0096 sec/batch\n",
      "Epoch: 593/1000...  Training Step: 1186...  Training loss: 0.0066...  1.0097 sec/batch\n",
      "Epoch: 594/1000...  Training Step: 1187...  Training loss: 0.0194...  1.0096 sec/batch\n",
      "Epoch: 594/1000...  Training Step: 1188...  Training loss: 0.0066...  1.0101 sec/batch\n",
      "Epoch: 595/1000...  Training Step: 1189...  Training loss: 0.0193...  1.0105 sec/batch\n",
      "Epoch: 595/1000...  Training Step: 1190...  Training loss: 0.0065...  1.0104 sec/batch\n",
      "Epoch: 596/1000...  Training Step: 1191...  Training loss: 0.0193...  1.0097 sec/batch\n",
      "Epoch: 596/1000...  Training Step: 1192...  Training loss: 0.0065...  1.0107 sec/batch\n",
      "Epoch: 597/1000...  Training Step: 1193...  Training loss: 0.0192...  1.0104 sec/batch\n",
      "Epoch: 597/1000...  Training Step: 1194...  Training loss: 0.0065...  1.0111 sec/batch\n",
      "Epoch: 598/1000...  Training Step: 1195...  Training loss: 0.0191...  1.0098 sec/batch\n",
      "Epoch: 598/1000...  Training Step: 1196...  Training loss: 0.0064...  1.0101 sec/batch\n",
      "Epoch: 599/1000...  Training Step: 1197...  Training loss: 0.0191...  1.0104 sec/batch\n",
      "Epoch: 599/1000...  Training Step: 1198...  Training loss: 0.0064...  1.0085 sec/batch\n",
      "Epoch: 600/1000...  Training Step: 1199...  Training loss: 0.0190...  1.0097 sec/batch\n",
      "Epoch: 600/1000...  Training Step: 1200...  Training loss: 0.0064...  1.0098 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 601/1000...  Training Step: 1201...  Training loss: 0.0190...  1.0096 sec/batch\n",
      "Epoch: 601/1000...  Training Step: 1202...  Training loss: 0.0064...  1.0111 sec/batch\n",
      "Epoch: 602/1000...  Training Step: 1203...  Training loss: 0.0190...  1.0098 sec/batch\n",
      "Epoch: 602/1000...  Training Step: 1204...  Training loss: 0.0065...  1.0121 sec/batch\n",
      "Epoch: 603/1000...  Training Step: 1205...  Training loss: 0.0190...  1.0105 sec/batch\n",
      "Epoch: 603/1000...  Training Step: 1206...  Training loss: 0.0065...  1.0094 sec/batch\n",
      "Epoch: 604/1000...  Training Step: 1207...  Training loss: 0.0191...  1.0098 sec/batch\n",
      "Epoch: 604/1000...  Training Step: 1208...  Training loss: 0.0076...  1.0098 sec/batch\n",
      "Epoch: 605/1000...  Training Step: 1209...  Training loss: 0.0199...  1.0102 sec/batch\n",
      "Epoch: 605/1000...  Training Step: 1210...  Training loss: 0.0106...  1.0101 sec/batch\n",
      "Epoch: 606/1000...  Training Step: 1211...  Training loss: 0.0258...  1.0097 sec/batch\n",
      "Epoch: 606/1000...  Training Step: 1212...  Training loss: 0.0313...  1.0098 sec/batch\n",
      "Epoch: 607/1000...  Training Step: 1213...  Training loss: 0.2832...  1.0098 sec/batch\n",
      "Epoch: 607/1000...  Training Step: 1214...  Training loss: 0.5717...  1.0104 sec/batch\n",
      "Epoch: 608/1000...  Training Step: 1215...  Training loss: 1.7953...  1.0099 sec/batch\n",
      "Epoch: 608/1000...  Training Step: 1216...  Training loss: 1.3176...  1.0096 sec/batch\n",
      "Epoch: 609/1000...  Training Step: 1217...  Training loss: 1.6394...  1.0099 sec/batch\n",
      "Epoch: 609/1000...  Training Step: 1218...  Training loss: 1.4829...  1.0089 sec/batch\n",
      "Epoch: 610/1000...  Training Step: 1219...  Training loss: 1.5958...  1.0112 sec/batch\n",
      "Epoch: 610/1000...  Training Step: 1220...  Training loss: 1.3657...  1.0095 sec/batch\n",
      "Epoch: 611/1000...  Training Step: 1221...  Training loss: 1.3304...  1.0094 sec/batch\n",
      "Epoch: 611/1000...  Training Step: 1222...  Training loss: 1.2718...  1.0096 sec/batch\n",
      "Epoch: 612/1000...  Training Step: 1223...  Training loss: 1.1383...  1.0099 sec/batch\n",
      "Epoch: 612/1000...  Training Step: 1224...  Training loss: 1.0425...  1.0090 sec/batch\n",
      "Epoch: 613/1000...  Training Step: 1225...  Training loss: 0.9367...  1.0096 sec/batch\n",
      "Epoch: 613/1000...  Training Step: 1226...  Training loss: 0.8847...  1.0123 sec/batch\n",
      "Epoch: 614/1000...  Training Step: 1227...  Training loss: 0.8197...  1.0108 sec/batch\n",
      "Epoch: 614/1000...  Training Step: 1228...  Training loss: 0.7540...  1.0100 sec/batch\n",
      "Epoch: 615/1000...  Training Step: 1229...  Training loss: 0.6990...  1.0107 sec/batch\n",
      "Epoch: 615/1000...  Training Step: 1230...  Training loss: 0.6586...  1.0093 sec/batch\n",
      "Epoch: 616/1000...  Training Step: 1231...  Training loss: 0.6013...  1.0101 sec/batch\n",
      "Epoch: 616/1000...  Training Step: 1232...  Training loss: 0.5652...  1.0093 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 617/1000...  Training Step: 1233...  Training loss: 0.5189...  1.0092 sec/batch\n",
      "Epoch: 617/1000...  Training Step: 1234...  Training loss: 0.4759...  1.0106 sec/batch\n",
      "Epoch: 618/1000...  Training Step: 1235...  Training loss: 0.4451...  1.0099 sec/batch\n",
      "Epoch: 618/1000...  Training Step: 1236...  Training loss: 0.4097...  1.0096 sec/batch\n",
      "Epoch: 619/1000...  Training Step: 1237...  Training loss: 0.3820...  1.0147 sec/batch\n",
      "Epoch: 619/1000...  Training Step: 1238...  Training loss: 0.3463...  1.0100 sec/batch\n",
      "Epoch: 620/1000...  Training Step: 1239...  Training loss: 0.3264...  1.0149 sec/batch\n",
      "Epoch: 620/1000...  Training Step: 1240...  Training loss: 0.2951...  1.0105 sec/batch\n",
      "Epoch: 621/1000...  Training Step: 1241...  Training loss: 0.2810...  1.0095 sec/batch\n",
      "Epoch: 621/1000...  Training Step: 1242...  Training loss: 0.2505...  1.0088 sec/batch\n",
      "Epoch: 622/1000...  Training Step: 1243...  Training loss: 0.2383...  1.0093 sec/batch\n",
      "Epoch: 622/1000...  Training Step: 1244...  Training loss: 0.2151...  1.0098 sec/batch\n",
      "Epoch: 623/1000...  Training Step: 1245...  Training loss: 0.2063...  1.0094 sec/batch\n",
      "Epoch: 623/1000...  Training Step: 1246...  Training loss: 0.1830...  1.0097 sec/batch\n",
      "Epoch: 624/1000...  Training Step: 1247...  Training loss: 0.1796...  1.0101 sec/batch\n",
      "Epoch: 624/1000...  Training Step: 1248...  Training loss: 0.1572...  1.0103 sec/batch\n",
      "Epoch: 625/1000...  Training Step: 1249...  Training loss: 0.1571...  1.0096 sec/batch\n",
      "Epoch: 625/1000...  Training Step: 1250...  Training loss: 0.1373...  1.0101 sec/batch\n",
      "Epoch: 626/1000...  Training Step: 1251...  Training loss: 0.1382...  1.0100 sec/batch\n",
      "Epoch: 626/1000...  Training Step: 1252...  Training loss: 0.1192...  1.0097 sec/batch\n",
      "Epoch: 627/1000...  Training Step: 1253...  Training loss: 0.1224...  1.0103 sec/batch\n",
      "Epoch: 627/1000...  Training Step: 1254...  Training loss: 0.1046...  1.0092 sec/batch\n",
      "Epoch: 628/1000...  Training Step: 1255...  Training loss: 0.1092...  1.0105 sec/batch\n",
      "Epoch: 628/1000...  Training Step: 1256...  Training loss: 0.0927...  1.0104 sec/batch\n",
      "Epoch: 629/1000...  Training Step: 1257...  Training loss: 0.0982...  1.0101 sec/batch\n",
      "Epoch: 629/1000...  Training Step: 1258...  Training loss: 0.0822...  1.0115 sec/batch\n",
      "Epoch: 630/1000...  Training Step: 1259...  Training loss: 0.0891...  1.0105 sec/batch\n",
      "Epoch: 630/1000...  Training Step: 1260...  Training loss: 0.0737...  1.0102 sec/batch\n",
      "Epoch: 631/1000...  Training Step: 1261...  Training loss: 0.0813...  1.0098 sec/batch\n",
      "Epoch: 631/1000...  Training Step: 1262...  Training loss: 0.0664...  1.0086 sec/batch\n",
      "Epoch: 632/1000...  Training Step: 1263...  Training loss: 0.0749...  1.0091 sec/batch\n",
      "Epoch: 632/1000...  Training Step: 1264...  Training loss: 0.0601...  1.0104 sec/batch\n",
      "Epoch: 633/1000...  Training Step: 1265...  Training loss: 0.0694...  1.0082 sec/batch\n",
      "Epoch: 633/1000...  Training Step: 1266...  Training loss: 0.0551...  1.0104 sec/batch\n",
      "Epoch: 634/1000...  Training Step: 1267...  Training loss: 0.0648...  1.0106 sec/batch\n",
      "Epoch: 634/1000...  Training Step: 1268...  Training loss: 0.0507...  1.0091 sec/batch\n",
      "Epoch: 635/1000...  Training Step: 1269...  Training loss: 0.0607...  1.0109 sec/batch\n",
      "Epoch: 635/1000...  Training Step: 1270...  Training loss: 0.0469...  1.0106 sec/batch\n",
      "Epoch: 636/1000...  Training Step: 1271...  Training loss: 0.0572...  1.0102 sec/batch\n",
      "Epoch: 636/1000...  Training Step: 1272...  Training loss: 0.0437...  1.0102 sec/batch\n",
      "Epoch: 637/1000...  Training Step: 1273...  Training loss: 0.0542...  1.0102 sec/batch\n",
      "Epoch: 637/1000...  Training Step: 1274...  Training loss: 0.0409...  1.0107 sec/batch\n",
      "Epoch: 638/1000...  Training Step: 1275...  Training loss: 0.0516...  1.0098 sec/batch\n",
      "Epoch: 638/1000...  Training Step: 1276...  Training loss: 0.0384...  1.0103 sec/batch\n",
      "Epoch: 639/1000...  Training Step: 1277...  Training loss: 0.0493...  1.0102 sec/batch\n",
      "Epoch: 639/1000...  Training Step: 1278...  Training loss: 0.0362...  1.0098 sec/batch\n",
      "Epoch: 640/1000...  Training Step: 1279...  Training loss: 0.0472...  1.0092 sec/batch\n",
      "Epoch: 640/1000...  Training Step: 1280...  Training loss: 0.0342...  1.0103 sec/batch\n",
      "Epoch: 641/1000...  Training Step: 1281...  Training loss: 0.0453...  1.0099 sec/batch\n",
      "Epoch: 641/1000...  Training Step: 1282...  Training loss: 0.0325...  1.0098 sec/batch\n",
      "Epoch: 642/1000...  Training Step: 1283...  Training loss: 0.0437...  1.0100 sec/batch\n",
      "Epoch: 642/1000...  Training Step: 1284...  Training loss: 0.0309...  1.0099 sec/batch\n",
      "Epoch: 643/1000...  Training Step: 1285...  Training loss: 0.0422...  1.0100 sec/batch\n",
      "Epoch: 643/1000...  Training Step: 1286...  Training loss: 0.0296...  1.0099 sec/batch\n",
      "Epoch: 644/1000...  Training Step: 1287...  Training loss: 0.0409...  1.0096 sec/batch\n",
      "Epoch: 644/1000...  Training Step: 1288...  Training loss: 0.0283...  1.0100 sec/batch\n",
      "Epoch: 645/1000...  Training Step: 1289...  Training loss: 0.0397...  1.0096 sec/batch\n",
      "Epoch: 645/1000...  Training Step: 1290...  Training loss: 0.0272...  1.0102 sec/batch\n",
      "Epoch: 646/1000...  Training Step: 1291...  Training loss: 0.0386...  1.0108 sec/batch\n",
      "Epoch: 646/1000...  Training Step: 1292...  Training loss: 0.0261...  1.0106 sec/batch\n",
      "Epoch: 647/1000...  Training Step: 1293...  Training loss: 0.0376...  1.0105 sec/batch\n",
      "Epoch: 647/1000...  Training Step: 1294...  Training loss: 0.0252...  1.0107 sec/batch\n",
      "Epoch: 648/1000...  Training Step: 1295...  Training loss: 0.0366...  1.0103 sec/batch\n",
      "Epoch: 648/1000...  Training Step: 1296...  Training loss: 0.0244...  1.0100 sec/batch\n",
      "Epoch: 649/1000...  Training Step: 1297...  Training loss: 0.0358...  1.0093 sec/batch\n",
      "Epoch: 649/1000...  Training Step: 1298...  Training loss: 0.0236...  1.0099 sec/batch\n",
      "Epoch: 650/1000...  Training Step: 1299...  Training loss: 0.0350...  1.0114 sec/batch\n",
      "Epoch: 650/1000...  Training Step: 1300...  Training loss: 0.0228...  1.0104 sec/batch\n",
      "Epoch: 651/1000...  Training Step: 1301...  Training loss: 0.0343...  1.0098 sec/batch\n",
      "Epoch: 651/1000...  Training Step: 1302...  Training loss: 0.0221...  1.0096 sec/batch\n",
      "Epoch: 652/1000...  Training Step: 1303...  Training loss: 0.0336...  1.0099 sec/batch\n",
      "Epoch: 652/1000...  Training Step: 1304...  Training loss: 0.0215...  1.0105 sec/batch\n",
      "Epoch: 653/1000...  Training Step: 1305...  Training loss: 0.0329...  1.0099 sec/batch\n",
      "Epoch: 653/1000...  Training Step: 1306...  Training loss: 0.0209...  1.0099 sec/batch\n",
      "Epoch: 654/1000...  Training Step: 1307...  Training loss: 0.0323...  1.0105 sec/batch\n",
      "Epoch: 654/1000...  Training Step: 1308...  Training loss: 0.0203...  1.0107 sec/batch\n",
      "Epoch: 655/1000...  Training Step: 1309...  Training loss: 0.0318...  1.0095 sec/batch\n",
      "Epoch: 655/1000...  Training Step: 1310...  Training loss: 0.0198...  1.0096 sec/batch\n",
      "Epoch: 656/1000...  Training Step: 1311...  Training loss: 0.0312...  1.0103 sec/batch\n",
      "Epoch: 656/1000...  Training Step: 1312...  Training loss: 0.0193...  1.0129 sec/batch\n",
      "Epoch: 657/1000...  Training Step: 1313...  Training loss: 0.0307...  1.0101 sec/batch\n",
      "Epoch: 657/1000...  Training Step: 1314...  Training loss: 0.0188...  1.0107 sec/batch\n",
      "Epoch: 658/1000...  Training Step: 1315...  Training loss: 0.0303...  1.0103 sec/batch\n",
      "Epoch: 658/1000...  Training Step: 1316...  Training loss: 0.0184...  1.0104 sec/batch\n",
      "Epoch: 659/1000...  Training Step: 1317...  Training loss: 0.0298...  1.0091 sec/batch\n",
      "Epoch: 659/1000...  Training Step: 1318...  Training loss: 0.0180...  1.0099 sec/batch\n",
      "Epoch: 660/1000...  Training Step: 1319...  Training loss: 0.0294...  1.0106 sec/batch\n",
      "Epoch: 660/1000...  Training Step: 1320...  Training loss: 0.0176...  1.0098 sec/batch\n",
      "Epoch: 661/1000...  Training Step: 1321...  Training loss: 0.0290...  1.0103 sec/batch\n",
      "Epoch: 661/1000...  Training Step: 1322...  Training loss: 0.0172...  1.0118 sec/batch\n",
      "Epoch: 662/1000...  Training Step: 1323...  Training loss: 0.0286...  1.0096 sec/batch\n",
      "Epoch: 662/1000...  Training Step: 1324...  Training loss: 0.0168...  1.0102 sec/batch\n",
      "Epoch: 663/1000...  Training Step: 1325...  Training loss: 0.0282...  1.0106 sec/batch\n",
      "Epoch: 663/1000...  Training Step: 1326...  Training loss: 0.0165...  1.0097 sec/batch\n",
      "Epoch: 664/1000...  Training Step: 1327...  Training loss: 0.0278...  1.0096 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 664/1000...  Training Step: 1328...  Training loss: 0.0161...  1.0098 sec/batch\n",
      "Epoch: 665/1000...  Training Step: 1329...  Training loss: 0.0275...  1.0094 sec/batch\n",
      "Epoch: 665/1000...  Training Step: 1330...  Training loss: 0.0158...  1.0104 sec/batch\n",
      "Epoch: 666/1000...  Training Step: 1331...  Training loss: 0.0271...  1.0094 sec/batch\n",
      "Epoch: 666/1000...  Training Step: 1332...  Training loss: 0.0155...  1.0123 sec/batch\n",
      "Epoch: 667/1000...  Training Step: 1333...  Training loss: 0.0268...  1.0100 sec/batch\n",
      "Epoch: 667/1000...  Training Step: 1334...  Training loss: 0.0152...  1.0103 sec/batch\n",
      "Epoch: 668/1000...  Training Step: 1335...  Training loss: 0.0265...  1.0101 sec/batch\n",
      "Epoch: 668/1000...  Training Step: 1336...  Training loss: 0.0149...  1.0093 sec/batch\n",
      "Epoch: 669/1000...  Training Step: 1337...  Training loss: 0.0262...  1.0104 sec/batch\n",
      "Epoch: 669/1000...  Training Step: 1338...  Training loss: 0.0147...  1.0100 sec/batch\n",
      "Epoch: 670/1000...  Training Step: 1339...  Training loss: 0.0259...  1.0098 sec/batch\n",
      "Epoch: 670/1000...  Training Step: 1340...  Training loss: 0.0144...  1.0099 sec/batch\n",
      "Epoch: 671/1000...  Training Step: 1341...  Training loss: 0.0256...  1.0100 sec/batch\n",
      "Epoch: 671/1000...  Training Step: 1342...  Training loss: 0.0141...  1.0099 sec/batch\n",
      "Epoch: 672/1000...  Training Step: 1343...  Training loss: 0.0254...  1.0095 sec/batch\n",
      "Epoch: 672/1000...  Training Step: 1344...  Training loss: 0.0139...  1.0099 sec/batch\n",
      "Epoch: 673/1000...  Training Step: 1345...  Training loss: 0.0251...  1.0105 sec/batch\n",
      "Epoch: 673/1000...  Training Step: 1346...  Training loss: 0.0137...  1.0102 sec/batch\n",
      "Epoch: 674/1000...  Training Step: 1347...  Training loss: 0.0248...  1.0082 sec/batch\n",
      "Epoch: 674/1000...  Training Step: 1348...  Training loss: 0.0134...  1.0104 sec/batch\n",
      "Epoch: 675/1000...  Training Step: 1349...  Training loss: 0.0246...  1.0097 sec/batch\n",
      "Epoch: 675/1000...  Training Step: 1350...  Training loss: 0.0132...  1.0105 sec/batch\n",
      "Epoch: 676/1000...  Training Step: 1351...  Training loss: 0.0244...  1.0095 sec/batch\n",
      "Epoch: 676/1000...  Training Step: 1352...  Training loss: 0.0130...  1.0089 sec/batch\n",
      "Epoch: 677/1000...  Training Step: 1353...  Training loss: 0.0241...  1.0102 sec/batch\n",
      "Epoch: 677/1000...  Training Step: 1354...  Training loss: 0.0128...  1.0102 sec/batch\n",
      "Epoch: 678/1000...  Training Step: 1355...  Training loss: 0.0239...  1.0103 sec/batch\n",
      "Epoch: 678/1000...  Training Step: 1356...  Training loss: 0.0126...  1.0139 sec/batch\n",
      "Epoch: 679/1000...  Training Step: 1357...  Training loss: 0.0237...  1.0093 sec/batch\n",
      "Epoch: 679/1000...  Training Step: 1358...  Training loss: 0.0124...  1.0129 sec/batch\n",
      "Epoch: 680/1000...  Training Step: 1359...  Training loss: 0.0235...  1.0100 sec/batch\n",
      "Epoch: 680/1000...  Training Step: 1360...  Training loss: 0.0122...  1.0099 sec/batch\n",
      "Epoch: 681/1000...  Training Step: 1361...  Training loss: 0.0233...  1.0096 sec/batch\n",
      "Epoch: 681/1000...  Training Step: 1362...  Training loss: 0.0120...  1.0108 sec/batch\n",
      "Epoch: 682/1000...  Training Step: 1363...  Training loss: 0.0231...  1.0097 sec/batch\n",
      "Epoch: 682/1000...  Training Step: 1364...  Training loss: 0.0119...  1.0101 sec/batch\n",
      "Epoch: 683/1000...  Training Step: 1365...  Training loss: 0.0229...  1.0096 sec/batch\n",
      "Epoch: 683/1000...  Training Step: 1366...  Training loss: 0.0117...  1.0093 sec/batch\n",
      "Epoch: 684/1000...  Training Step: 1367...  Training loss: 0.0227...  1.0098 sec/batch\n",
      "Epoch: 684/1000...  Training Step: 1368...  Training loss: 0.0115...  1.0100 sec/batch\n",
      "Epoch: 685/1000...  Training Step: 1369...  Training loss: 0.0225...  1.0095 sec/batch\n",
      "Epoch: 685/1000...  Training Step: 1370...  Training loss: 0.0114...  1.0098 sec/batch\n",
      "Epoch: 686/1000...  Training Step: 1371...  Training loss: 0.0223...  1.0096 sec/batch\n",
      "Epoch: 686/1000...  Training Step: 1372...  Training loss: 0.0112...  1.0102 sec/batch\n",
      "Epoch: 687/1000...  Training Step: 1373...  Training loss: 0.0221...  1.0095 sec/batch\n",
      "Epoch: 687/1000...  Training Step: 1374...  Training loss: 0.0110...  1.0097 sec/batch\n",
      "Epoch: 688/1000...  Training Step: 1375...  Training loss: 0.0220...  1.0098 sec/batch\n",
      "Epoch: 688/1000...  Training Step: 1376...  Training loss: 0.0109...  1.0115 sec/batch\n",
      "Epoch: 689/1000...  Training Step: 1377...  Training loss: 0.0218...  1.0099 sec/batch\n",
      "Epoch: 689/1000...  Training Step: 1378...  Training loss: 0.0107...  1.0100 sec/batch\n",
      "Epoch: 690/1000...  Training Step: 1379...  Training loss: 0.0216...  1.0101 sec/batch\n",
      "Epoch: 690/1000...  Training Step: 1380...  Training loss: 0.0106...  1.0091 sec/batch\n",
      "Epoch: 691/1000...  Training Step: 1381...  Training loss: 0.0215...  1.0105 sec/batch\n",
      "Epoch: 691/1000...  Training Step: 1382...  Training loss: 0.0105...  1.0110 sec/batch\n",
      "Epoch: 692/1000...  Training Step: 1383...  Training loss: 0.0213...  1.0107 sec/batch\n",
      "Epoch: 692/1000...  Training Step: 1384...  Training loss: 0.0103...  1.0102 sec/batch\n",
      "Epoch: 693/1000...  Training Step: 1385...  Training loss: 0.0212...  1.0096 sec/batch\n",
      "Epoch: 693/1000...  Training Step: 1386...  Training loss: 0.0102...  1.0098 sec/batch\n",
      "Epoch: 694/1000...  Training Step: 1387...  Training loss: 0.0210...  1.0098 sec/batch\n",
      "Epoch: 694/1000...  Training Step: 1388...  Training loss: 0.0101...  1.0100 sec/batch\n",
      "Epoch: 695/1000...  Training Step: 1389...  Training loss: 0.0209...  1.0105 sec/batch\n",
      "Epoch: 695/1000...  Training Step: 1390...  Training loss: 0.0099...  1.0096 sec/batch\n",
      "Epoch: 696/1000...  Training Step: 1391...  Training loss: 0.0207...  1.0084 sec/batch\n",
      "Epoch: 696/1000...  Training Step: 1392...  Training loss: 0.0098...  1.0101 sec/batch\n",
      "Epoch: 697/1000...  Training Step: 1393...  Training loss: 0.0206...  1.0095 sec/batch\n",
      "Epoch: 697/1000...  Training Step: 1394...  Training loss: 0.0097...  1.0093 sec/batch\n",
      "Epoch: 698/1000...  Training Step: 1395...  Training loss: 0.0204...  1.0095 sec/batch\n",
      "Epoch: 698/1000...  Training Step: 1396...  Training loss: 0.0096...  1.0100 sec/batch\n",
      "Epoch: 699/1000...  Training Step: 1397...  Training loss: 0.0203...  1.0109 sec/batch\n",
      "Epoch: 699/1000...  Training Step: 1398...  Training loss: 0.0095...  1.0084 sec/batch\n",
      "Epoch: 700/1000...  Training Step: 1399...  Training loss: 0.0202...  1.0110 sec/batch\n",
      "Epoch: 700/1000...  Training Step: 1400...  Training loss: 0.0094...  1.0101 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 701/1000...  Training Step: 1401...  Training loss: 0.0200...  1.0084 sec/batch\n",
      "Epoch: 701/1000...  Training Step: 1402...  Training loss: 0.0093...  1.0097 sec/batch\n",
      "Epoch: 702/1000...  Training Step: 1403...  Training loss: 0.0199...  1.0090 sec/batch\n",
      "Epoch: 702/1000...  Training Step: 1404...  Training loss: 0.0091...  1.0079 sec/batch\n",
      "Epoch: 703/1000...  Training Step: 1405...  Training loss: 0.0198...  1.0098 sec/batch\n",
      "Epoch: 703/1000...  Training Step: 1406...  Training loss: 0.0090...  1.0104 sec/batch\n",
      "Epoch: 704/1000...  Training Step: 1407...  Training loss: 0.0197...  1.0098 sec/batch\n",
      "Epoch: 704/1000...  Training Step: 1408...  Training loss: 0.0089...  1.0095 sec/batch\n",
      "Epoch: 705/1000...  Training Step: 1409...  Training loss: 0.0196...  1.0098 sec/batch\n",
      "Epoch: 705/1000...  Training Step: 1410...  Training loss: 0.0088...  1.0092 sec/batch\n",
      "Epoch: 706/1000...  Training Step: 1411...  Training loss: 0.0194...  1.0095 sec/batch\n",
      "Epoch: 706/1000...  Training Step: 1412...  Training loss: 0.0087...  1.0094 sec/batch\n",
      "Epoch: 707/1000...  Training Step: 1413...  Training loss: 0.0193...  1.0097 sec/batch\n",
      "Epoch: 707/1000...  Training Step: 1414...  Training loss: 0.0086...  1.0091 sec/batch\n",
      "Epoch: 708/1000...  Training Step: 1415...  Training loss: 0.0192...  1.0095 sec/batch\n",
      "Epoch: 708/1000...  Training Step: 1416...  Training loss: 0.0086...  1.0103 sec/batch\n",
      "Epoch: 709/1000...  Training Step: 1417...  Training loss: 0.0191...  1.0106 sec/batch\n",
      "Epoch: 709/1000...  Training Step: 1418...  Training loss: 0.0085...  1.0097 sec/batch\n",
      "Epoch: 710/1000...  Training Step: 1419...  Training loss: 0.0190...  1.0097 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 710/1000...  Training Step: 1420...  Training loss: 0.0084...  1.0102 sec/batch\n",
      "Epoch: 711/1000...  Training Step: 1421...  Training loss: 0.0189...  1.0103 sec/batch\n",
      "Epoch: 711/1000...  Training Step: 1422...  Training loss: 0.0083...  1.0102 sec/batch\n",
      "Epoch: 712/1000...  Training Step: 1423...  Training loss: 0.0188...  1.0102 sec/batch\n",
      "Epoch: 712/1000...  Training Step: 1424...  Training loss: 0.0082...  1.0087 sec/batch\n",
      "Epoch: 713/1000...  Training Step: 1425...  Training loss: 0.0187...  1.0095 sec/batch\n",
      "Epoch: 713/1000...  Training Step: 1426...  Training loss: 0.0081...  1.0098 sec/batch\n",
      "Epoch: 714/1000...  Training Step: 1427...  Training loss: 0.0186...  1.0098 sec/batch\n",
      "Epoch: 714/1000...  Training Step: 1428...  Training loss: 0.0080...  1.0100 sec/batch\n",
      "Epoch: 715/1000...  Training Step: 1429...  Training loss: 0.0185...  1.0103 sec/batch\n",
      "Epoch: 715/1000...  Training Step: 1430...  Training loss: 0.0080...  1.0094 sec/batch\n",
      "Epoch: 716/1000...  Training Step: 1431...  Training loss: 0.0184...  1.0102 sec/batch\n",
      "Epoch: 716/1000...  Training Step: 1432...  Training loss: 0.0079...  1.0100 sec/batch\n",
      "Epoch: 717/1000...  Training Step: 1433...  Training loss: 0.0183...  1.0093 sec/batch\n",
      "Epoch: 717/1000...  Training Step: 1434...  Training loss: 0.0078...  1.0103 sec/batch\n",
      "Epoch: 718/1000...  Training Step: 1435...  Training loss: 0.0182...  1.0100 sec/batch\n",
      "Epoch: 718/1000...  Training Step: 1436...  Training loss: 0.0077...  1.0094 sec/batch\n",
      "Epoch: 719/1000...  Training Step: 1437...  Training loss: 0.0181...  1.0095 sec/batch\n",
      "Epoch: 719/1000...  Training Step: 1438...  Training loss: 0.0076...  1.0094 sec/batch\n",
      "Epoch: 720/1000...  Training Step: 1439...  Training loss: 0.0180...  1.0090 sec/batch\n",
      "Epoch: 720/1000...  Training Step: 1440...  Training loss: 0.0076...  1.0098 sec/batch\n",
      "Epoch: 721/1000...  Training Step: 1441...  Training loss: 0.0179...  1.0103 sec/batch\n",
      "Epoch: 721/1000...  Training Step: 1442...  Training loss: 0.0075...  1.0100 sec/batch\n",
      "Epoch: 722/1000...  Training Step: 1443...  Training loss: 0.0178...  1.0104 sec/batch\n",
      "Epoch: 722/1000...  Training Step: 1444...  Training loss: 0.0074...  1.0109 sec/batch\n",
      "Epoch: 723/1000...  Training Step: 1445...  Training loss: 0.0177...  1.0101 sec/batch\n",
      "Epoch: 723/1000...  Training Step: 1446...  Training loss: 0.0074...  1.0106 sec/batch\n",
      "Epoch: 724/1000...  Training Step: 1447...  Training loss: 0.0176...  1.0101 sec/batch\n",
      "Epoch: 724/1000...  Training Step: 1448...  Training loss: 0.0073...  1.0095 sec/batch\n",
      "Epoch: 725/1000...  Training Step: 1449...  Training loss: 0.0175...  1.0095 sec/batch\n",
      "Epoch: 725/1000...  Training Step: 1450...  Training loss: 0.0072...  1.0095 sec/batch\n",
      "Epoch: 726/1000...  Training Step: 1451...  Training loss: 0.0175...  1.0100 sec/batch\n",
      "Epoch: 726/1000...  Training Step: 1452...  Training loss: 0.0072...  1.0096 sec/batch\n",
      "Epoch: 727/1000...  Training Step: 1453...  Training loss: 0.0174...  1.0097 sec/batch\n",
      "Epoch: 727/1000...  Training Step: 1454...  Training loss: 0.0071...  1.0106 sec/batch\n",
      "Epoch: 728/1000...  Training Step: 1455...  Training loss: 0.0173...  1.0100 sec/batch\n",
      "Epoch: 728/1000...  Training Step: 1456...  Training loss: 0.0070...  1.0098 sec/batch\n",
      "Epoch: 729/1000...  Training Step: 1457...  Training loss: 0.0172...  1.0094 sec/batch\n",
      "Epoch: 729/1000...  Training Step: 1458...  Training loss: 0.0070...  1.0086 sec/batch\n",
      "Epoch: 730/1000...  Training Step: 1459...  Training loss: 0.0171...  1.0105 sec/batch\n",
      "Epoch: 730/1000...  Training Step: 1460...  Training loss: 0.0069...  1.0092 sec/batch\n",
      "Epoch: 731/1000...  Training Step: 1461...  Training loss: 0.0171...  1.0094 sec/batch\n",
      "Epoch: 731/1000...  Training Step: 1462...  Training loss: 0.0068...  1.0099 sec/batch\n",
      "Epoch: 732/1000...  Training Step: 1463...  Training loss: 0.0170...  1.0108 sec/batch\n",
      "Epoch: 732/1000...  Training Step: 1464...  Training loss: 0.0068...  1.0101 sec/batch\n",
      "Epoch: 733/1000...  Training Step: 1465...  Training loss: 0.0169...  1.0106 sec/batch\n",
      "Epoch: 733/1000...  Training Step: 1466...  Training loss: 0.0067...  1.0102 sec/batch\n",
      "Epoch: 734/1000...  Training Step: 1467...  Training loss: 0.0168...  1.0099 sec/batch\n",
      "Epoch: 734/1000...  Training Step: 1468...  Training loss: 0.0067...  1.0092 sec/batch\n",
      "Epoch: 735/1000...  Training Step: 1469...  Training loss: 0.0167...  1.0094 sec/batch\n",
      "Epoch: 735/1000...  Training Step: 1470...  Training loss: 0.0066...  1.0100 sec/batch\n",
      "Epoch: 736/1000...  Training Step: 1471...  Training loss: 0.0167...  1.0093 sec/batch\n",
      "Epoch: 736/1000...  Training Step: 1472...  Training loss: 0.0065...  1.0095 sec/batch\n",
      "Epoch: 737/1000...  Training Step: 1473...  Training loss: 0.0166...  1.0098 sec/batch\n",
      "Epoch: 737/1000...  Training Step: 1474...  Training loss: 0.0065...  1.0162 sec/batch\n",
      "Epoch: 738/1000...  Training Step: 1475...  Training loss: 0.0165...  1.0093 sec/batch\n",
      "Epoch: 738/1000...  Training Step: 1476...  Training loss: 0.0064...  1.0152 sec/batch\n",
      "Epoch: 739/1000...  Training Step: 1477...  Training loss: 0.0165...  1.0103 sec/batch\n",
      "Epoch: 739/1000...  Training Step: 1478...  Training loss: 0.0064...  1.0094 sec/batch\n",
      "Epoch: 740/1000...  Training Step: 1479...  Training loss: 0.0164...  1.0097 sec/batch\n",
      "Epoch: 740/1000...  Training Step: 1480...  Training loss: 0.0063...  1.0097 sec/batch\n",
      "Epoch: 741/1000...  Training Step: 1481...  Training loss: 0.0163...  1.0097 sec/batch\n",
      "Epoch: 741/1000...  Training Step: 1482...  Training loss: 0.0063...  1.0086 sec/batch\n",
      "Epoch: 742/1000...  Training Step: 1483...  Training loss: 0.0162...  1.0101 sec/batch\n",
      "Epoch: 742/1000...  Training Step: 1484...  Training loss: 0.0062...  1.0106 sec/batch\n",
      "Epoch: 743/1000...  Training Step: 1485...  Training loss: 0.0162...  1.0099 sec/batch\n",
      "Epoch: 743/1000...  Training Step: 1486...  Training loss: 0.0062...  1.0099 sec/batch\n",
      "Epoch: 744/1000...  Training Step: 1487...  Training loss: 0.0161...  1.0098 sec/batch\n",
      "Epoch: 744/1000...  Training Step: 1488...  Training loss: 0.0061...  1.0102 sec/batch\n",
      "Epoch: 745/1000...  Training Step: 1489...  Training loss: 0.0160...  1.0096 sec/batch\n",
      "Epoch: 745/1000...  Training Step: 1490...  Training loss: 0.0061...  1.0088 sec/batch\n",
      "Epoch: 746/1000...  Training Step: 1491...  Training loss: 0.0160...  1.0099 sec/batch\n",
      "Epoch: 746/1000...  Training Step: 1492...  Training loss: 0.0060...  1.0098 sec/batch\n",
      "Epoch: 747/1000...  Training Step: 1493...  Training loss: 0.0159...  1.0097 sec/batch\n",
      "Epoch: 747/1000...  Training Step: 1494...  Training loss: 0.0060...  1.0107 sec/batch\n",
      "Epoch: 748/1000...  Training Step: 1495...  Training loss: 0.0159...  1.0106 sec/batch\n",
      "Epoch: 748/1000...  Training Step: 1496...  Training loss: 0.0059...  1.0099 sec/batch\n",
      "Epoch: 749/1000...  Training Step: 1497...  Training loss: 0.0158...  1.0097 sec/batch\n",
      "Epoch: 749/1000...  Training Step: 1498...  Training loss: 0.0059...  1.0084 sec/batch\n",
      "Epoch: 750/1000...  Training Step: 1499...  Training loss: 0.0157...  1.0095 sec/batch\n",
      "Epoch: 750/1000...  Training Step: 1500...  Training loss: 0.0058...  1.0103 sec/batch\n",
      "Epoch: 751/1000...  Training Step: 1501...  Training loss: 0.0157...  1.0093 sec/batch\n",
      "Epoch: 751/1000...  Training Step: 1502...  Training loss: 0.0058...  1.0108 sec/batch\n",
      "Epoch: 752/1000...  Training Step: 1503...  Training loss: 0.0156...  1.0098 sec/batch\n",
      "Epoch: 752/1000...  Training Step: 1504...  Training loss: 0.0058...  1.0101 sec/batch\n",
      "Epoch: 753/1000...  Training Step: 1505...  Training loss: 0.0155...  1.0101 sec/batch\n",
      "Epoch: 753/1000...  Training Step: 1506...  Training loss: 0.0057...  1.0101 sec/batch\n",
      "Epoch: 754/1000...  Training Step: 1507...  Training loss: 0.0155...  1.0097 sec/batch\n",
      "Epoch: 754/1000...  Training Step: 1508...  Training loss: 0.0057...  1.0102 sec/batch\n",
      "Epoch: 755/1000...  Training Step: 1509...  Training loss: 0.0154...  1.0087 sec/batch\n",
      "Epoch: 755/1000...  Training Step: 1510...  Training loss: 0.0056...  1.0104 sec/batch\n",
      "Epoch: 756/1000...  Training Step: 1511...  Training loss: 0.0154...  1.0101 sec/batch\n",
      "Epoch: 756/1000...  Training Step: 1512...  Training loss: 0.0056...  1.0100 sec/batch\n",
      "Epoch: 757/1000...  Training Step: 1513...  Training loss: 0.0153...  1.0105 sec/batch\n",
      "Epoch: 757/1000...  Training Step: 1514...  Training loss: 0.0055...  1.0086 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 758/1000...  Training Step: 1515...  Training loss: 0.0153...  1.0100 sec/batch\n",
      "Epoch: 758/1000...  Training Step: 1516...  Training loss: 0.0055...  1.0099 sec/batch\n",
      "Epoch: 759/1000...  Training Step: 1517...  Training loss: 0.0152...  1.0107 sec/batch\n",
      "Epoch: 759/1000...  Training Step: 1518...  Training loss: 0.0055...  1.0089 sec/batch\n",
      "Epoch: 760/1000...  Training Step: 1519...  Training loss: 0.0151...  1.0098 sec/batch\n",
      "Epoch: 760/1000...  Training Step: 1520...  Training loss: 0.0054...  1.0094 sec/batch\n",
      "Epoch: 761/1000...  Training Step: 1521...  Training loss: 0.0151...  1.0100 sec/batch\n",
      "Epoch: 761/1000...  Training Step: 1522...  Training loss: 0.0054...  1.0098 sec/batch\n",
      "Epoch: 762/1000...  Training Step: 1523...  Training loss: 0.0150...  1.0089 sec/batch\n",
      "Epoch: 762/1000...  Training Step: 1524...  Training loss: 0.0053...  1.0095 sec/batch\n",
      "Epoch: 763/1000...  Training Step: 1525...  Training loss: 0.0150...  1.0096 sec/batch\n",
      "Epoch: 763/1000...  Training Step: 1526...  Training loss: 0.0053...  1.0097 sec/batch\n",
      "Epoch: 764/1000...  Training Step: 1527...  Training loss: 0.0149...  1.0095 sec/batch\n",
      "Epoch: 764/1000...  Training Step: 1528...  Training loss: 0.0053...  1.0095 sec/batch\n",
      "Epoch: 765/1000...  Training Step: 1529...  Training loss: 0.0149...  1.0108 sec/batch\n",
      "Epoch: 765/1000...  Training Step: 1530...  Training loss: 0.0052...  1.0109 sec/batch\n",
      "Epoch: 766/1000...  Training Step: 1531...  Training loss: 0.0148...  1.0100 sec/batch\n",
      "Epoch: 766/1000...  Training Step: 1532...  Training loss: 0.0052...  1.0098 sec/batch\n",
      "Epoch: 767/1000...  Training Step: 1533...  Training loss: 0.0148...  1.0098 sec/batch\n",
      "Epoch: 767/1000...  Training Step: 1534...  Training loss: 0.0052...  1.0090 sec/batch\n",
      "Epoch: 768/1000...  Training Step: 1535...  Training loss: 0.0147...  1.0107 sec/batch\n",
      "Epoch: 768/1000...  Training Step: 1536...  Training loss: 0.0051...  1.0112 sec/batch\n",
      "Epoch: 769/1000...  Training Step: 1537...  Training loss: 0.0147...  1.0086 sec/batch\n",
      "Epoch: 769/1000...  Training Step: 1538...  Training loss: 0.0051...  1.0098 sec/batch\n",
      "Epoch: 770/1000...  Training Step: 1539...  Training loss: 0.0146...  1.0097 sec/batch\n",
      "Epoch: 770/1000...  Training Step: 1540...  Training loss: 0.0051...  1.0105 sec/batch\n",
      "Epoch: 771/1000...  Training Step: 1541...  Training loss: 0.0146...  1.0102 sec/batch\n",
      "Epoch: 771/1000...  Training Step: 1542...  Training loss: 0.0050...  1.0099 sec/batch\n",
      "Epoch: 772/1000...  Training Step: 1543...  Training loss: 0.0145...  1.0083 sec/batch\n",
      "Epoch: 772/1000...  Training Step: 1544...  Training loss: 0.0050...  1.0093 sec/batch\n",
      "Epoch: 773/1000...  Training Step: 1545...  Training loss: 0.0145...  1.0099 sec/batch\n",
      "Epoch: 773/1000...  Training Step: 1546...  Training loss: 0.0050...  1.0097 sec/batch\n",
      "Epoch: 774/1000...  Training Step: 1547...  Training loss: 0.0144...  1.0095 sec/batch\n",
      "Epoch: 774/1000...  Training Step: 1548...  Training loss: 0.0049...  1.0101 sec/batch\n",
      "Epoch: 775/1000...  Training Step: 1549...  Training loss: 0.0144...  1.0090 sec/batch\n",
      "Epoch: 775/1000...  Training Step: 1550...  Training loss: 0.0049...  1.0096 sec/batch\n",
      "Epoch: 776/1000...  Training Step: 1551...  Training loss: 0.0143...  1.0102 sec/batch\n",
      "Epoch: 776/1000...  Training Step: 1552...  Training loss: 0.0049...  1.0086 sec/batch\n",
      "Epoch: 777/1000...  Training Step: 1553...  Training loss: 0.0143...  1.0099 sec/batch\n",
      "Epoch: 777/1000...  Training Step: 1554...  Training loss: 0.0048...  1.0103 sec/batch\n",
      "Epoch: 778/1000...  Training Step: 1555...  Training loss: 0.0142...  1.0106 sec/batch\n",
      "Epoch: 778/1000...  Training Step: 1556...  Training loss: 0.0048...  1.0114 sec/batch\n",
      "Epoch: 779/1000...  Training Step: 1557...  Training loss: 0.0142...  1.0105 sec/batch\n",
      "Epoch: 779/1000...  Training Step: 1558...  Training loss: 0.0048...  1.0094 sec/batch\n",
      "Epoch: 780/1000...  Training Step: 1559...  Training loss: 0.0141...  1.0100 sec/batch\n",
      "Epoch: 780/1000...  Training Step: 1560...  Training loss: 0.0047...  1.0101 sec/batch\n",
      "Epoch: 781/1000...  Training Step: 1561...  Training loss: 0.0141...  1.0103 sec/batch\n",
      "Epoch: 781/1000...  Training Step: 1562...  Training loss: 0.0047...  1.0097 sec/batch\n",
      "Epoch: 782/1000...  Training Step: 1563...  Training loss: 0.0141...  1.0099 sec/batch\n",
      "Epoch: 782/1000...  Training Step: 1564...  Training loss: 0.0047...  1.0106 sec/batch\n",
      "Epoch: 783/1000...  Training Step: 1565...  Training loss: 0.0140...  1.0092 sec/batch\n",
      "Epoch: 783/1000...  Training Step: 1566...  Training loss: 0.0046...  1.0100 sec/batch\n",
      "Epoch: 784/1000...  Training Step: 1567...  Training loss: 0.0140...  1.0085 sec/batch\n",
      "Epoch: 784/1000...  Training Step: 1568...  Training loss: 0.0046...  1.0099 sec/batch\n",
      "Epoch: 785/1000...  Training Step: 1569...  Training loss: 0.0139...  1.0102 sec/batch\n",
      "Epoch: 785/1000...  Training Step: 1570...  Training loss: 0.0046...  1.0089 sec/batch\n",
      "Epoch: 786/1000...  Training Step: 1571...  Training loss: 0.0139...  1.0095 sec/batch\n",
      "Epoch: 786/1000...  Training Step: 1572...  Training loss: 0.0046...  1.0110 sec/batch\n",
      "Epoch: 787/1000...  Training Step: 1573...  Training loss: 0.0138...  1.0101 sec/batch\n",
      "Epoch: 787/1000...  Training Step: 1574...  Training loss: 0.0045...  1.0100 sec/batch\n",
      "Epoch: 788/1000...  Training Step: 1575...  Training loss: 0.0138...  1.0098 sec/batch\n",
      "Epoch: 788/1000...  Training Step: 1576...  Training loss: 0.0045...  1.0106 sec/batch\n",
      "Epoch: 789/1000...  Training Step: 1577...  Training loss: 0.0138...  1.0102 sec/batch\n",
      "Epoch: 789/1000...  Training Step: 1578...  Training loss: 0.0045...  1.0096 sec/batch\n",
      "Epoch: 790/1000...  Training Step: 1579...  Training loss: 0.0137...  1.0104 sec/batch\n",
      "Epoch: 790/1000...  Training Step: 1580...  Training loss: 0.0044...  1.0103 sec/batch\n",
      "Epoch: 791/1000...  Training Step: 1581...  Training loss: 0.0137...  1.0105 sec/batch\n",
      "Epoch: 791/1000...  Training Step: 1582...  Training loss: 0.0044...  1.0104 sec/batch\n",
      "Epoch: 792/1000...  Training Step: 1583...  Training loss: 0.0136...  1.0102 sec/batch\n",
      "Epoch: 792/1000...  Training Step: 1584...  Training loss: 0.0044...  1.0105 sec/batch\n",
      "Epoch: 793/1000...  Training Step: 1585...  Training loss: 0.0136...  1.0092 sec/batch\n",
      "Epoch: 793/1000...  Training Step: 1586...  Training loss: 0.0044...  1.0098 sec/batch\n",
      "Epoch: 794/1000...  Training Step: 1587...  Training loss: 0.0135...  1.0099 sec/batch\n",
      "Epoch: 794/1000...  Training Step: 1588...  Training loss: 0.0044...  1.0100 sec/batch\n",
      "Epoch: 795/1000...  Training Step: 1589...  Training loss: 0.0135...  1.0095 sec/batch\n",
      "Epoch: 795/1000...  Training Step: 1590...  Training loss: 0.0045...  1.0087 sec/batch\n",
      "Epoch: 796/1000...  Training Step: 1591...  Training loss: 0.0135...  1.0100 sec/batch\n",
      "Epoch: 796/1000...  Training Step: 1592...  Training loss: 0.0049...  1.0101 sec/batch\n",
      "Epoch: 797/1000...  Training Step: 1593...  Training loss: 0.0136...  1.0142 sec/batch\n",
      "Epoch: 797/1000...  Training Step: 1594...  Training loss: 0.0053...  1.0096 sec/batch\n",
      "Epoch: 798/1000...  Training Step: 1595...  Training loss: 0.0137...  1.0184 sec/batch\n",
      "Epoch: 798/1000...  Training Step: 1596...  Training loss: 0.0051...  1.0103 sec/batch\n",
      "Epoch: 799/1000...  Training Step: 1597...  Training loss: 0.0138...  1.0099 sec/batch\n",
      "Epoch: 799/1000...  Training Step: 1598...  Training loss: 0.0060...  1.0111 sec/batch\n",
      "Epoch: 800/1000...  Training Step: 1599...  Training loss: 0.0139...  1.0099 sec/batch\n",
      "Epoch: 800/1000...  Training Step: 1600...  Training loss: 0.0059...  1.0109 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 801/1000...  Training Step: 1601...  Training loss: 0.0140...  1.0105 sec/batch\n",
      "Epoch: 801/1000...  Training Step: 1602...  Training loss: 0.0065...  1.0087 sec/batch\n",
      "Epoch: 802/1000...  Training Step: 1603...  Training loss: 0.0143...  1.0098 sec/batch\n",
      "Epoch: 802/1000...  Training Step: 1604...  Training loss: 0.0063...  1.0109 sec/batch\n",
      "Epoch: 803/1000...  Training Step: 1605...  Training loss: 0.0144...  1.0104 sec/batch\n",
      "Epoch: 803/1000...  Training Step: 1606...  Training loss: 0.0059...  1.0105 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 804/1000...  Training Step: 1607...  Training loss: 0.0143...  1.0095 sec/batch\n",
      "Epoch: 804/1000...  Training Step: 1608...  Training loss: 0.0057...  1.0107 sec/batch\n",
      "Epoch: 805/1000...  Training Step: 1609...  Training loss: 0.0142...  1.0106 sec/batch\n",
      "Epoch: 805/1000...  Training Step: 1610...  Training loss: 0.0062...  1.0103 sec/batch\n",
      "Epoch: 806/1000...  Training Step: 1611...  Training loss: 0.0141...  1.0097 sec/batch\n",
      "Epoch: 806/1000...  Training Step: 1612...  Training loss: 0.0055...  1.0097 sec/batch\n",
      "Epoch: 807/1000...  Training Step: 1613...  Training loss: 0.0140...  1.0102 sec/batch\n",
      "Epoch: 807/1000...  Training Step: 1614...  Training loss: 0.0054...  1.0107 sec/batch\n",
      "Epoch: 808/1000...  Training Step: 1615...  Training loss: 0.0139...  1.0098 sec/batch\n",
      "Epoch: 808/1000...  Training Step: 1616...  Training loss: 0.0051...  1.0099 sec/batch\n",
      "Epoch: 809/1000...  Training Step: 1617...  Training loss: 0.0137...  1.0096 sec/batch\n",
      "Epoch: 809/1000...  Training Step: 1618...  Training loss: 0.0049...  1.0096 sec/batch\n",
      "Epoch: 810/1000...  Training Step: 1619...  Training loss: 0.0136...  1.0091 sec/batch\n",
      "Epoch: 810/1000...  Training Step: 1620...  Training loss: 0.0047...  1.0095 sec/batch\n",
      "Epoch: 811/1000...  Training Step: 1621...  Training loss: 0.0135...  1.0095 sec/batch\n",
      "Epoch: 811/1000...  Training Step: 1622...  Training loss: 0.0046...  1.0102 sec/batch\n",
      "Epoch: 812/1000...  Training Step: 1623...  Training loss: 0.0134...  1.0097 sec/batch\n",
      "Epoch: 812/1000...  Training Step: 1624...  Training loss: 0.0045...  1.0104 sec/batch\n",
      "Epoch: 813/1000...  Training Step: 1625...  Training loss: 0.0133...  1.0097 sec/batch\n",
      "Epoch: 813/1000...  Training Step: 1626...  Training loss: 0.0044...  1.0091 sec/batch\n",
      "Epoch: 814/1000...  Training Step: 1627...  Training loss: 0.0132...  1.0096 sec/batch\n",
      "Epoch: 814/1000...  Training Step: 1628...  Training loss: 0.0043...  1.0098 sec/batch\n",
      "Epoch: 815/1000...  Training Step: 1629...  Training loss: 0.0131...  1.0098 sec/batch\n",
      "Epoch: 815/1000...  Training Step: 1630...  Training loss: 0.0042...  1.0099 sec/batch\n",
      "Epoch: 816/1000...  Training Step: 1631...  Training loss: 0.0130...  1.0097 sec/batch\n",
      "Epoch: 816/1000...  Training Step: 1632...  Training loss: 0.0041...  1.0085 sec/batch\n",
      "Epoch: 817/1000...  Training Step: 1633...  Training loss: 0.0129...  1.0116 sec/batch\n",
      "Epoch: 817/1000...  Training Step: 1634...  Training loss: 0.0041...  1.0099 sec/batch\n",
      "Epoch: 818/1000...  Training Step: 1635...  Training loss: 0.0129...  1.0082 sec/batch\n",
      "Epoch: 818/1000...  Training Step: 1636...  Training loss: 0.0040...  1.0105 sec/batch\n",
      "Epoch: 819/1000...  Training Step: 1637...  Training loss: 0.0128...  1.0102 sec/batch\n",
      "Epoch: 819/1000...  Training Step: 1638...  Training loss: 0.0039...  1.0110 sec/batch\n",
      "Epoch: 820/1000...  Training Step: 1639...  Training loss: 0.0128...  1.0101 sec/batch\n",
      "Epoch: 820/1000...  Training Step: 1640...  Training loss: 0.0039...  1.0104 sec/batch\n",
      "Epoch: 821/1000...  Training Step: 1641...  Training loss: 0.0127...  1.0101 sec/batch\n",
      "Epoch: 821/1000...  Training Step: 1642...  Training loss: 0.0039...  1.0101 sec/batch\n",
      "Epoch: 822/1000...  Training Step: 1643...  Training loss: 0.0127...  1.0096 sec/batch\n",
      "Epoch: 822/1000...  Training Step: 1644...  Training loss: 0.0038...  1.0102 sec/batch\n",
      "Epoch: 823/1000...  Training Step: 1645...  Training loss: 0.0126...  1.0098 sec/batch\n",
      "Epoch: 823/1000...  Training Step: 1646...  Training loss: 0.0038...  1.0098 sec/batch\n",
      "Epoch: 824/1000...  Training Step: 1647...  Training loss: 0.0126...  1.0100 sec/batch\n",
      "Epoch: 824/1000...  Training Step: 1648...  Training loss: 0.0037...  1.0089 sec/batch\n",
      "Epoch: 825/1000...  Training Step: 1649...  Training loss: 0.0125...  1.0096 sec/batch\n",
      "Epoch: 825/1000...  Training Step: 1650...  Training loss: 0.0037...  1.0098 sec/batch\n",
      "Epoch: 826/1000...  Training Step: 1651...  Training loss: 0.0125...  1.0097 sec/batch\n",
      "Epoch: 826/1000...  Training Step: 1652...  Training loss: 0.0037...  1.0098 sec/batch\n",
      "Epoch: 827/1000...  Training Step: 1653...  Training loss: 0.0125...  1.0100 sec/batch\n",
      "Epoch: 827/1000...  Training Step: 1654...  Training loss: 0.0037...  1.0102 sec/batch\n",
      "Epoch: 828/1000...  Training Step: 1655...  Training loss: 0.0124...  1.0141 sec/batch\n",
      "Epoch: 828/1000...  Training Step: 1656...  Training loss: 0.0036...  1.0112 sec/batch\n",
      "Epoch: 829/1000...  Training Step: 1657...  Training loss: 0.0124...  1.0100 sec/batch\n",
      "Epoch: 829/1000...  Training Step: 1658...  Training loss: 0.0036...  1.0103 sec/batch\n",
      "Epoch: 830/1000...  Training Step: 1659...  Training loss: 0.0123...  1.0091 sec/batch\n",
      "Epoch: 830/1000...  Training Step: 1660...  Training loss: 0.0036...  1.0101 sec/batch\n",
      "Epoch: 831/1000...  Training Step: 1661...  Training loss: 0.0123...  1.0094 sec/batch\n",
      "Epoch: 831/1000...  Training Step: 1662...  Training loss: 0.0036...  1.0099 sec/batch\n",
      "Epoch: 832/1000...  Training Step: 1663...  Training loss: 0.0123...  1.0106 sec/batch\n",
      "Epoch: 832/1000...  Training Step: 1664...  Training loss: 0.0035...  1.0098 sec/batch\n",
      "Epoch: 833/1000...  Training Step: 1665...  Training loss: 0.0122...  1.0102 sec/batch\n",
      "Epoch: 833/1000...  Training Step: 1666...  Training loss: 0.0035...  1.0095 sec/batch\n",
      "Epoch: 834/1000...  Training Step: 1667...  Training loss: 0.0122...  1.0105 sec/batch\n",
      "Epoch: 834/1000...  Training Step: 1668...  Training loss: 0.0035...  1.0102 sec/batch\n",
      "Epoch: 835/1000...  Training Step: 1669...  Training loss: 0.0122...  1.0106 sec/batch\n",
      "Epoch: 835/1000...  Training Step: 1670...  Training loss: 0.0035...  1.0097 sec/batch\n",
      "Epoch: 836/1000...  Training Step: 1671...  Training loss: 0.0121...  1.0106 sec/batch\n",
      "Epoch: 836/1000...  Training Step: 1672...  Training loss: 0.0035...  1.0101 sec/batch\n",
      "Epoch: 837/1000...  Training Step: 1673...  Training loss: 0.0121...  1.0101 sec/batch\n",
      "Epoch: 837/1000...  Training Step: 1674...  Training loss: 0.0034...  1.0096 sec/batch\n",
      "Epoch: 838/1000...  Training Step: 1675...  Training loss: 0.0121...  1.0099 sec/batch\n",
      "Epoch: 838/1000...  Training Step: 1676...  Training loss: 0.0034...  1.0109 sec/batch\n",
      "Epoch: 839/1000...  Training Step: 1677...  Training loss: 0.0120...  1.0108 sec/batch\n",
      "Epoch: 839/1000...  Training Step: 1678...  Training loss: 0.0034...  1.0104 sec/batch\n",
      "Epoch: 840/1000...  Training Step: 1679...  Training loss: 0.0120...  1.0091 sec/batch\n",
      "Epoch: 840/1000...  Training Step: 1680...  Training loss: 0.0034...  1.0104 sec/batch\n",
      "Epoch: 841/1000...  Training Step: 1681...  Training loss: 0.0120...  1.0102 sec/batch\n",
      "Epoch: 841/1000...  Training Step: 1682...  Training loss: 0.0034...  1.0090 sec/batch\n",
      "Epoch: 842/1000...  Training Step: 1683...  Training loss: 0.0119...  1.0105 sec/batch\n",
      "Epoch: 842/1000...  Training Step: 1684...  Training loss: 0.0033...  1.0097 sec/batch\n",
      "Epoch: 843/1000...  Training Step: 1685...  Training loss: 0.0119...  1.0100 sec/batch\n",
      "Epoch: 843/1000...  Training Step: 1686...  Training loss: 0.0033...  1.0095 sec/batch\n",
      "Epoch: 844/1000...  Training Step: 1687...  Training loss: 0.0119...  1.0094 sec/batch\n",
      "Epoch: 844/1000...  Training Step: 1688...  Training loss: 0.0033...  1.0086 sec/batch\n",
      "Epoch: 845/1000...  Training Step: 1689...  Training loss: 0.0119...  1.0086 sec/batch\n",
      "Epoch: 845/1000...  Training Step: 1690...  Training loss: 0.0033...  1.0099 sec/batch\n",
      "Epoch: 846/1000...  Training Step: 1691...  Training loss: 0.0118...  1.0111 sec/batch\n",
      "Epoch: 846/1000...  Training Step: 1692...  Training loss: 0.0033...  1.0099 sec/batch\n",
      "Epoch: 847/1000...  Training Step: 1693...  Training loss: 0.0118...  1.0101 sec/batch\n",
      "Epoch: 847/1000...  Training Step: 1694...  Training loss: 0.0033...  1.0086 sec/batch\n",
      "Epoch: 848/1000...  Training Step: 1695...  Training loss: 0.0118...  1.0100 sec/batch\n",
      "Epoch: 848/1000...  Training Step: 1696...  Training loss: 0.0032...  1.0096 sec/batch\n",
      "Epoch: 849/1000...  Training Step: 1697...  Training loss: 0.0117...  1.0099 sec/batch\n",
      "Epoch: 849/1000...  Training Step: 1698...  Training loss: 0.0032...  1.0105 sec/batch\n",
      "Epoch: 850/1000...  Training Step: 1699...  Training loss: 0.0117...  1.0105 sec/batch\n",
      "Epoch: 850/1000...  Training Step: 1700...  Training loss: 0.0032...  1.0103 sec/batch\n",
      "Epoch: 851/1000...  Training Step: 1701...  Training loss: 0.0117...  1.0097 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 851/1000...  Training Step: 1702...  Training loss: 0.0032...  1.0101 sec/batch\n",
      "Epoch: 852/1000...  Training Step: 1703...  Training loss: 0.0117...  1.0107 sec/batch\n",
      "Epoch: 852/1000...  Training Step: 1704...  Training loss: 0.0032...  1.0103 sec/batch\n",
      "Epoch: 853/1000...  Training Step: 1705...  Training loss: 0.0116...  1.0095 sec/batch\n",
      "Epoch: 853/1000...  Training Step: 1706...  Training loss: 0.0032...  1.0098 sec/batch\n",
      "Epoch: 854/1000...  Training Step: 1707...  Training loss: 0.0116...  1.0099 sec/batch\n",
      "Epoch: 854/1000...  Training Step: 1708...  Training loss: 0.0031...  1.0095 sec/batch\n",
      "Epoch: 855/1000...  Training Step: 1709...  Training loss: 0.0116...  1.0096 sec/batch\n",
      "Epoch: 855/1000...  Training Step: 1710...  Training loss: 0.0031...  1.0102 sec/batch\n",
      "Epoch: 856/1000...  Training Step: 1711...  Training loss: 0.0116...  1.0147 sec/batch\n",
      "Epoch: 856/1000...  Training Step: 1712...  Training loss: 0.0031...  1.0118 sec/batch\n",
      "Epoch: 857/1000...  Training Step: 1713...  Training loss: 0.0115...  1.0135 sec/batch\n",
      "Epoch: 857/1000...  Training Step: 1714...  Training loss: 0.0031...  1.0109 sec/batch\n",
      "Epoch: 858/1000...  Training Step: 1715...  Training loss: 0.0115...  1.0101 sec/batch\n",
      "Epoch: 858/1000...  Training Step: 1716...  Training loss: 0.0031...  1.0099 sec/batch\n",
      "Epoch: 859/1000...  Training Step: 1717...  Training loss: 0.0115...  1.0101 sec/batch\n",
      "Epoch: 859/1000...  Training Step: 1718...  Training loss: 0.0031...  1.0104 sec/batch\n",
      "Epoch: 860/1000...  Training Step: 1719...  Training loss: 0.0114...  1.0104 sec/batch\n",
      "Epoch: 860/1000...  Training Step: 1720...  Training loss: 0.0031...  1.0102 sec/batch\n",
      "Epoch: 861/1000...  Training Step: 1721...  Training loss: 0.0114...  1.0095 sec/batch\n",
      "Epoch: 861/1000...  Training Step: 1722...  Training loss: 0.0030...  1.0102 sec/batch\n",
      "Epoch: 862/1000...  Training Step: 1723...  Training loss: 0.0114...  1.0105 sec/batch\n",
      "Epoch: 862/1000...  Training Step: 1724...  Training loss: 0.0030...  1.0103 sec/batch\n",
      "Epoch: 863/1000...  Training Step: 1725...  Training loss: 0.0114...  1.0099 sec/batch\n",
      "Epoch: 863/1000...  Training Step: 1726...  Training loss: 0.0030...  1.0103 sec/batch\n",
      "Epoch: 864/1000...  Training Step: 1727...  Training loss: 0.0113...  1.0100 sec/batch\n",
      "Epoch: 864/1000...  Training Step: 1728...  Training loss: 0.0030...  1.0101 sec/batch\n",
      "Epoch: 865/1000...  Training Step: 1729...  Training loss: 0.0113...  1.0096 sec/batch\n",
      "Epoch: 865/1000...  Training Step: 1730...  Training loss: 0.0030...  1.0097 sec/batch\n",
      "Epoch: 866/1000...  Training Step: 1731...  Training loss: 0.0113...  1.0104 sec/batch\n",
      "Epoch: 866/1000...  Training Step: 1732...  Training loss: 0.0030...  1.0101 sec/batch\n",
      "Epoch: 867/1000...  Training Step: 1733...  Training loss: 0.0113...  1.0099 sec/batch\n",
      "Epoch: 867/1000...  Training Step: 1734...  Training loss: 0.0030...  1.0095 sec/batch\n",
      "Epoch: 868/1000...  Training Step: 1735...  Training loss: 0.0112...  1.0096 sec/batch\n",
      "Epoch: 868/1000...  Training Step: 1736...  Training loss: 0.0029...  1.0099 sec/batch\n",
      "Epoch: 869/1000...  Training Step: 1737...  Training loss: 0.0112...  1.0094 sec/batch\n",
      "Epoch: 869/1000...  Training Step: 1738...  Training loss: 0.0029...  1.0090 sec/batch\n",
      "Epoch: 870/1000...  Training Step: 1739...  Training loss: 0.0112...  1.0098 sec/batch\n",
      "Epoch: 870/1000...  Training Step: 1740...  Training loss: 0.0029...  1.0106 sec/batch\n",
      "Epoch: 871/1000...  Training Step: 1741...  Training loss: 0.0112...  1.0084 sec/batch\n",
      "Epoch: 871/1000...  Training Step: 1742...  Training loss: 0.0029...  1.0104 sec/batch\n",
      "Epoch: 872/1000...  Training Step: 1743...  Training loss: 0.0111...  1.0105 sec/batch\n",
      "Epoch: 872/1000...  Training Step: 1744...  Training loss: 0.0029...  1.0103 sec/batch\n",
      "Epoch: 873/1000...  Training Step: 1745...  Training loss: 0.0111...  1.0103 sec/batch\n",
      "Epoch: 873/1000...  Training Step: 1746...  Training loss: 0.0029...  1.0100 sec/batch\n",
      "Epoch: 874/1000...  Training Step: 1747...  Training loss: 0.0111...  1.0101 sec/batch\n",
      "Epoch: 874/1000...  Training Step: 1748...  Training loss: 0.0029...  1.0095 sec/batch\n",
      "Epoch: 875/1000...  Training Step: 1749...  Training loss: 0.0111...  1.0102 sec/batch\n",
      "Epoch: 875/1000...  Training Step: 1750...  Training loss: 0.0029...  1.0102 sec/batch\n",
      "Epoch: 876/1000...  Training Step: 1751...  Training loss: 0.0110...  1.0102 sec/batch\n",
      "Epoch: 876/1000...  Training Step: 1752...  Training loss: 0.0028...  1.0103 sec/batch\n",
      "Epoch: 877/1000...  Training Step: 1753...  Training loss: 0.0110...  1.0096 sec/batch\n",
      "Epoch: 877/1000...  Training Step: 1754...  Training loss: 0.0028...  1.0099 sec/batch\n",
      "Epoch: 878/1000...  Training Step: 1755...  Training loss: 0.0110...  1.0097 sec/batch\n",
      "Epoch: 878/1000...  Training Step: 1756...  Training loss: 0.0028...  1.0096 sec/batch\n",
      "Epoch: 879/1000...  Training Step: 1757...  Training loss: 0.0110...  1.0095 sec/batch\n",
      "Epoch: 879/1000...  Training Step: 1758...  Training loss: 0.0028...  1.0099 sec/batch\n",
      "Epoch: 880/1000...  Training Step: 1759...  Training loss: 0.0110...  1.0091 sec/batch\n",
      "Epoch: 880/1000...  Training Step: 1760...  Training loss: 0.0028...  1.0095 sec/batch\n",
      "Epoch: 881/1000...  Training Step: 1761...  Training loss: 0.0109...  1.0111 sec/batch\n",
      "Epoch: 881/1000...  Training Step: 1762...  Training loss: 0.0028...  1.0097 sec/batch\n",
      "Epoch: 882/1000...  Training Step: 1763...  Training loss: 0.0109...  1.0096 sec/batch\n",
      "Epoch: 882/1000...  Training Step: 1764...  Training loss: 0.0028...  1.0100 sec/batch\n",
      "Epoch: 883/1000...  Training Step: 1765...  Training loss: 0.0109...  1.0103 sec/batch\n",
      "Epoch: 883/1000...  Training Step: 1766...  Training loss: 0.0028...  1.0104 sec/batch\n",
      "Epoch: 884/1000...  Training Step: 1767...  Training loss: 0.0109...  1.0106 sec/batch\n",
      "Epoch: 884/1000...  Training Step: 1768...  Training loss: 0.0027...  1.0103 sec/batch\n",
      "Epoch: 885/1000...  Training Step: 1769...  Training loss: 0.0108...  1.0087 sec/batch\n",
      "Epoch: 885/1000...  Training Step: 1770...  Training loss: 0.0027...  1.0101 sec/batch\n",
      "Epoch: 886/1000...  Training Step: 1771...  Training loss: 0.0108...  1.0098 sec/batch\n",
      "Epoch: 886/1000...  Training Step: 1772...  Training loss: 0.0027...  1.0099 sec/batch\n",
      "Epoch: 887/1000...  Training Step: 1773...  Training loss: 0.0108...  1.0095 sec/batch\n",
      "Epoch: 887/1000...  Training Step: 1774...  Training loss: 0.0027...  1.0097 sec/batch\n",
      "Epoch: 888/1000...  Training Step: 1775...  Training loss: 0.0108...  1.0098 sec/batch\n",
      "Epoch: 888/1000...  Training Step: 1776...  Training loss: 0.0027...  1.0101 sec/batch\n",
      "Epoch: 889/1000...  Training Step: 1777...  Training loss: 0.0107...  1.0096 sec/batch\n",
      "Epoch: 889/1000...  Training Step: 1778...  Training loss: 0.0027...  1.0112 sec/batch\n",
      "Epoch: 890/1000...  Training Step: 1779...  Training loss: 0.0107...  1.0102 sec/batch\n",
      "Epoch: 890/1000...  Training Step: 1780...  Training loss: 0.0027...  1.0101 sec/batch\n",
      "Epoch: 891/1000...  Training Step: 1781...  Training loss: 0.0107...  1.0101 sec/batch\n",
      "Epoch: 891/1000...  Training Step: 1782...  Training loss: 0.0027...  1.0095 sec/batch\n",
      "Epoch: 892/1000...  Training Step: 1783...  Training loss: 0.0107...  1.0098 sec/batch\n",
      "Epoch: 892/1000...  Training Step: 1784...  Training loss: 0.0027...  1.0094 sec/batch\n",
      "Epoch: 893/1000...  Training Step: 1785...  Training loss: 0.0107...  1.0093 sec/batch\n",
      "Epoch: 893/1000...  Training Step: 1786...  Training loss: 0.0026...  1.0099 sec/batch\n",
      "Epoch: 894/1000...  Training Step: 1787...  Training loss: 0.0106...  1.0101 sec/batch\n",
      "Epoch: 894/1000...  Training Step: 1788...  Training loss: 0.0026...  1.0112 sec/batch\n",
      "Epoch: 895/1000...  Training Step: 1789...  Training loss: 0.0106...  1.0088 sec/batch\n",
      "Epoch: 895/1000...  Training Step: 1790...  Training loss: 0.0026...  1.0102 sec/batch\n",
      "Epoch: 896/1000...  Training Step: 1791...  Training loss: 0.0106...  1.0098 sec/batch\n",
      "Epoch: 896/1000...  Training Step: 1792...  Training loss: 0.0026...  1.0096 sec/batch\n",
      "Epoch: 897/1000...  Training Step: 1793...  Training loss: 0.0106...  1.0099 sec/batch\n",
      "Epoch: 897/1000...  Training Step: 1794...  Training loss: 0.0026...  1.0100 sec/batch\n",
      "Epoch: 898/1000...  Training Step: 1795...  Training loss: 0.0106...  1.0096 sec/batch\n",
      "Epoch: 898/1000...  Training Step: 1796...  Training loss: 0.0026...  1.0101 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 899/1000...  Training Step: 1797...  Training loss: 0.0105...  1.0104 sec/batch\n",
      "Epoch: 899/1000...  Training Step: 1798...  Training loss: 0.0026...  1.0096 sec/batch\n",
      "Epoch: 900/1000...  Training Step: 1799...  Training loss: 0.0105...  1.0101 sec/batch\n",
      "Epoch: 900/1000...  Training Step: 1800...  Training loss: 0.0026...  1.0089 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Epoch: 901/1000...  Training Step: 1801...  Training loss: 0.0105...  1.0088 sec/batch\n",
      "Epoch: 901/1000...  Training Step: 1802...  Training loss: 0.0026...  1.0095 sec/batch\n",
      "Epoch: 902/1000...  Training Step: 1803...  Training loss: 0.0105...  1.0090 sec/batch\n",
      "Epoch: 902/1000...  Training Step: 1804...  Training loss: 0.0025...  1.0100 sec/batch\n",
      "Epoch: 903/1000...  Training Step: 1805...  Training loss: 0.0105...  1.0098 sec/batch\n",
      "Epoch: 903/1000...  Training Step: 1806...  Training loss: 0.0025...  1.0091 sec/batch\n",
      "Epoch: 904/1000...  Training Step: 1807...  Training loss: 0.0104...  1.0085 sec/batch\n",
      "Epoch: 904/1000...  Training Step: 1808...  Training loss: 0.0025...  1.0096 sec/batch\n",
      "Epoch: 905/1000...  Training Step: 1809...  Training loss: 0.0104...  1.0104 sec/batch\n",
      "Epoch: 905/1000...  Training Step: 1810...  Training loss: 0.0025...  1.0105 sec/batch\n",
      "Epoch: 906/1000...  Training Step: 1811...  Training loss: 0.0104...  1.0105 sec/batch\n",
      "Epoch: 906/1000...  Training Step: 1812...  Training loss: 0.0025...  1.0096 sec/batch\n",
      "Epoch: 907/1000...  Training Step: 1813...  Training loss: 0.0104...  1.0091 sec/batch\n",
      "Epoch: 907/1000...  Training Step: 1814...  Training loss: 0.0025...  1.0095 sec/batch\n",
      "Epoch: 908/1000...  Training Step: 1815...  Training loss: 0.0103...  1.0096 sec/batch\n",
      "Epoch: 908/1000...  Training Step: 1816...  Training loss: 0.0025...  1.0106 sec/batch\n",
      "Epoch: 909/1000...  Training Step: 1817...  Training loss: 0.0103...  1.0102 sec/batch\n",
      "Epoch: 909/1000...  Training Step: 1818...  Training loss: 0.0025...  1.0090 sec/batch\n",
      "Epoch: 910/1000...  Training Step: 1819...  Training loss: 0.0103...  1.0102 sec/batch\n",
      "Epoch: 910/1000...  Training Step: 1820...  Training loss: 0.0025...  1.0087 sec/batch\n",
      "Epoch: 911/1000...  Training Step: 1821...  Training loss: 0.0103...  1.0099 sec/batch\n",
      "Epoch: 911/1000...  Training Step: 1822...  Training loss: 0.0025...  1.0087 sec/batch\n",
      "Epoch: 912/1000...  Training Step: 1823...  Training loss: 0.0104...  1.0104 sec/batch\n",
      "Epoch: 912/1000...  Training Step: 1824...  Training loss: 0.0025...  1.0108 sec/batch\n",
      "Epoch: 913/1000...  Training Step: 1825...  Training loss: 0.0117...  1.0097 sec/batch\n",
      "Epoch: 913/1000...  Training Step: 1826...  Training loss: 0.0028...  1.0100 sec/batch\n",
      "Epoch: 914/1000...  Training Step: 1827...  Training loss: 0.0121...  1.0087 sec/batch\n",
      "Epoch: 914/1000...  Training Step: 1828...  Training loss: 0.0033...  1.0106 sec/batch\n",
      "Epoch: 915/1000...  Training Step: 1829...  Training loss: 0.0138...  1.0103 sec/batch\n",
      "Epoch: 915/1000...  Training Step: 1830...  Training loss: 0.0046...  1.0148 sec/batch\n",
      "Epoch: 916/1000...  Training Step: 1831...  Training loss: 0.0172...  1.0080 sec/batch\n",
      "Epoch: 916/1000...  Training Step: 1832...  Training loss: 0.0065...  1.0153 sec/batch\n",
      "Epoch: 917/1000...  Training Step: 1833...  Training loss: 0.0217...  1.0104 sec/batch\n",
      "Epoch: 917/1000...  Training Step: 1834...  Training loss: 0.0114...  1.0097 sec/batch\n",
      "Epoch: 918/1000...  Training Step: 1835...  Training loss: 0.0254...  1.0104 sec/batch\n",
      "Epoch: 918/1000...  Training Step: 1836...  Training loss: 0.0161...  1.0103 sec/batch\n",
      "Epoch: 919/1000...  Training Step: 1837...  Training loss: 0.0284...  1.0100 sec/batch\n",
      "Epoch: 919/1000...  Training Step: 1838...  Training loss: 0.0187...  1.0102 sec/batch\n",
      "Epoch: 920/1000...  Training Step: 1839...  Training loss: 0.0256...  1.0096 sec/batch\n",
      "Epoch: 920/1000...  Training Step: 1840...  Training loss: 0.0196...  1.0105 sec/batch\n",
      "Epoch: 921/1000...  Training Step: 1841...  Training loss: 0.0242...  1.0102 sec/batch\n",
      "Epoch: 921/1000...  Training Step: 1842...  Training loss: 0.0173...  1.0087 sec/batch\n",
      "Epoch: 922/1000...  Training Step: 1843...  Training loss: 0.0211...  1.0100 sec/batch\n",
      "Epoch: 922/1000...  Training Step: 1844...  Training loss: 0.0148...  1.0089 sec/batch\n",
      "Epoch: 923/1000...  Training Step: 1845...  Training loss: 0.0196...  1.0102 sec/batch\n",
      "Epoch: 923/1000...  Training Step: 1846...  Training loss: 0.0123...  1.0100 sec/batch\n",
      "Epoch: 924/1000...  Training Step: 1847...  Training loss: 0.0180...  1.0111 sec/batch\n",
      "Epoch: 924/1000...  Training Step: 1848...  Training loss: 0.0108...  1.0094 sec/batch\n",
      "Epoch: 925/1000...  Training Step: 1849...  Training loss: 0.0167...  1.0100 sec/batch\n",
      "Epoch: 925/1000...  Training Step: 1850...  Training loss: 0.0090...  1.0098 sec/batch\n",
      "Epoch: 926/1000...  Training Step: 1851...  Training loss: 0.0158...  1.0100 sec/batch\n",
      "Epoch: 926/1000...  Training Step: 1852...  Training loss: 0.0083...  1.0107 sec/batch\n",
      "Epoch: 927/1000...  Training Step: 1853...  Training loss: 0.0149...  1.0104 sec/batch\n",
      "Epoch: 927/1000...  Training Step: 1854...  Training loss: 0.0073...  1.0099 sec/batch\n",
      "Epoch: 928/1000...  Training Step: 1855...  Training loss: 0.0142...  1.0093 sec/batch\n",
      "Epoch: 928/1000...  Training Step: 1856...  Training loss: 0.0067...  1.0088 sec/batch\n",
      "Epoch: 929/1000...  Training Step: 1857...  Training loss: 0.0137...  1.0101 sec/batch\n",
      "Epoch: 929/1000...  Training Step: 1858...  Training loss: 0.0060...  1.0103 sec/batch\n",
      "Epoch: 930/1000...  Training Step: 1859...  Training loss: 0.0131...  1.0090 sec/batch\n",
      "Epoch: 930/1000...  Training Step: 1860...  Training loss: 0.0054...  1.0086 sec/batch\n",
      "Epoch: 931/1000...  Training Step: 1861...  Training loss: 0.0127...  1.0085 sec/batch\n",
      "Epoch: 931/1000...  Training Step: 1862...  Training loss: 0.0050...  1.0099 sec/batch\n",
      "Epoch: 932/1000...  Training Step: 1863...  Training loss: 0.0124...  1.0101 sec/batch\n",
      "Epoch: 932/1000...  Training Step: 1864...  Training loss: 0.0047...  1.0107 sec/batch\n",
      "Epoch: 933/1000...  Training Step: 1865...  Training loss: 0.0121...  1.0105 sec/batch\n",
      "Epoch: 933/1000...  Training Step: 1866...  Training loss: 0.0044...  1.0103 sec/batch\n",
      "Epoch: 934/1000...  Training Step: 1867...  Training loss: 0.0119...  1.0090 sec/batch\n",
      "Epoch: 934/1000...  Training Step: 1868...  Training loss: 0.0042...  1.0089 sec/batch\n",
      "Epoch: 935/1000...  Training Step: 1869...  Training loss: 0.0116...  1.0095 sec/batch\n",
      "Epoch: 935/1000...  Training Step: 1870...  Training loss: 0.0040...  1.0099 sec/batch\n",
      "Epoch: 936/1000...  Training Step: 1871...  Training loss: 0.0114...  1.0088 sec/batch\n",
      "Epoch: 936/1000...  Training Step: 1872...  Training loss: 0.0038...  1.0104 sec/batch\n",
      "Epoch: 937/1000...  Training Step: 1873...  Training loss: 0.0113...  1.0104 sec/batch\n",
      "Epoch: 937/1000...  Training Step: 1874...  Training loss: 0.0036...  1.0103 sec/batch\n",
      "Epoch: 938/1000...  Training Step: 1875...  Training loss: 0.0111...  1.0105 sec/batch\n",
      "Epoch: 938/1000...  Training Step: 1876...  Training loss: 0.0035...  1.0098 sec/batch\n",
      "Epoch: 939/1000...  Training Step: 1877...  Training loss: 0.0110...  1.0094 sec/batch\n",
      "Epoch: 939/1000...  Training Step: 1878...  Training loss: 0.0033...  1.0095 sec/batch\n",
      "Epoch: 940/1000...  Training Step: 1879...  Training loss: 0.0109...  1.0100 sec/batch\n",
      "Epoch: 940/1000...  Training Step: 1880...  Training loss: 0.0032...  1.0097 sec/batch\n",
      "Epoch: 941/1000...  Training Step: 1881...  Training loss: 0.0108...  1.0103 sec/batch\n",
      "Epoch: 941/1000...  Training Step: 1882...  Training loss: 0.0031...  1.0096 sec/batch\n",
      "Epoch: 942/1000...  Training Step: 1883...  Training loss: 0.0107...  1.0097 sec/batch\n",
      "Epoch: 942/1000...  Training Step: 1884...  Training loss: 0.0030...  1.0085 sec/batch\n",
      "Epoch: 943/1000...  Training Step: 1885...  Training loss: 0.0106...  1.0091 sec/batch\n",
      "Epoch: 943/1000...  Training Step: 1886...  Training loss: 0.0030...  1.0095 sec/batch\n",
      "Epoch: 944/1000...  Training Step: 1887...  Training loss: 0.0105...  1.0096 sec/batch\n",
      "Epoch: 944/1000...  Training Step: 1888...  Training loss: 0.0029...  1.0100 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 945/1000...  Training Step: 1889...  Training loss: 0.0105...  1.0095 sec/batch\n",
      "Epoch: 945/1000...  Training Step: 1890...  Training loss: 0.0028...  1.0101 sec/batch\n",
      "Epoch: 946/1000...  Training Step: 1891...  Training loss: 0.0104...  1.0104 sec/batch\n",
      "Epoch: 946/1000...  Training Step: 1892...  Training loss: 0.0028...  1.0108 sec/batch\n",
      "Epoch: 947/1000...  Training Step: 1893...  Training loss: 0.0103...  1.0088 sec/batch\n",
      "Epoch: 947/1000...  Training Step: 1894...  Training loss: 0.0027...  1.0091 sec/batch\n",
      "Epoch: 948/1000...  Training Step: 1895...  Training loss: 0.0103...  1.0103 sec/batch\n",
      "Epoch: 948/1000...  Training Step: 1896...  Training loss: 0.0027...  1.0102 sec/batch\n",
      "Epoch: 949/1000...  Training Step: 1897...  Training loss: 0.0102...  1.0106 sec/batch\n",
      "Epoch: 949/1000...  Training Step: 1898...  Training loss: 0.0026...  1.0097 sec/batch\n",
      "Epoch: 950/1000...  Training Step: 1899...  Training loss: 0.0102...  1.0101 sec/batch\n",
      "Epoch: 950/1000...  Training Step: 1900...  Training loss: 0.0026...  1.0091 sec/batch\n",
      "Epoch: 951/1000...  Training Step: 1901...  Training loss: 0.0101...  1.0096 sec/batch\n",
      "Epoch: 951/1000...  Training Step: 1902...  Training loss: 0.0026...  1.0102 sec/batch\n",
      "Epoch: 952/1000...  Training Step: 1903...  Training loss: 0.0101...  1.0101 sec/batch\n",
      "Epoch: 952/1000...  Training Step: 1904...  Training loss: 0.0025...  1.0099 sec/batch\n",
      "Epoch: 953/1000...  Training Step: 1905...  Training loss: 0.0100...  1.0096 sec/batch\n",
      "Epoch: 953/1000...  Training Step: 1906...  Training loss: 0.0025...  1.0102 sec/batch\n",
      "Epoch: 954/1000...  Training Step: 1907...  Training loss: 0.0100...  1.0095 sec/batch\n",
      "Epoch: 954/1000...  Training Step: 1908...  Training loss: 0.0025...  1.0093 sec/batch\n",
      "Epoch: 955/1000...  Training Step: 1909...  Training loss: 0.0100...  1.0085 sec/batch\n",
      "Epoch: 955/1000...  Training Step: 1910...  Training loss: 0.0024...  1.0104 sec/batch\n",
      "Epoch: 956/1000...  Training Step: 1911...  Training loss: 0.0099...  1.0096 sec/batch\n",
      "Epoch: 956/1000...  Training Step: 1912...  Training loss: 0.0024...  1.0099 sec/batch\n",
      "Epoch: 957/1000...  Training Step: 1913...  Training loss: 0.0099...  1.0094 sec/batch\n",
      "Epoch: 957/1000...  Training Step: 1914...  Training loss: 0.0024...  1.0094 sec/batch\n",
      "Epoch: 958/1000...  Training Step: 1915...  Training loss: 0.0099...  1.0113 sec/batch\n",
      "Epoch: 958/1000...  Training Step: 1916...  Training loss: 0.0024...  1.0102 sec/batch\n",
      "Epoch: 959/1000...  Training Step: 1917...  Training loss: 0.0098...  1.0097 sec/batch\n",
      "Epoch: 959/1000...  Training Step: 1918...  Training loss: 0.0023...  1.0105 sec/batch\n",
      "Epoch: 960/1000...  Training Step: 1919...  Training loss: 0.0098...  1.0113 sec/batch\n",
      "Epoch: 960/1000...  Training Step: 1920...  Training loss: 0.0023...  1.0096 sec/batch\n",
      "Epoch: 961/1000...  Training Step: 1921...  Training loss: 0.0098...  1.0103 sec/batch\n",
      "Epoch: 961/1000...  Training Step: 1922...  Training loss: 0.0023...  1.0099 sec/batch\n",
      "Epoch: 962/1000...  Training Step: 1923...  Training loss: 0.0097...  1.0095 sec/batch\n",
      "Epoch: 962/1000...  Training Step: 1924...  Training loss: 0.0023...  1.0099 sec/batch\n",
      "Epoch: 963/1000...  Training Step: 1925...  Training loss: 0.0097...  1.0087 sec/batch\n",
      "Epoch: 963/1000...  Training Step: 1926...  Training loss: 0.0023...  1.0099 sec/batch\n",
      "Epoch: 964/1000...  Training Step: 1927...  Training loss: 0.0097...  1.0096 sec/batch\n",
      "Epoch: 964/1000...  Training Step: 1928...  Training loss: 0.0022...  1.0098 sec/batch\n",
      "Epoch: 965/1000...  Training Step: 1929...  Training loss: 0.0097...  1.0096 sec/batch\n",
      "Epoch: 965/1000...  Training Step: 1930...  Training loss: 0.0022...  1.0107 sec/batch\n",
      "Epoch: 966/1000...  Training Step: 1931...  Training loss: 0.0096...  1.0099 sec/batch\n",
      "Epoch: 966/1000...  Training Step: 1932...  Training loss: 0.0022...  1.0095 sec/batch\n",
      "Epoch: 967/1000...  Training Step: 1933...  Training loss: 0.0096...  1.0096 sec/batch\n",
      "Epoch: 967/1000...  Training Step: 1934...  Training loss: 0.0022...  1.0095 sec/batch\n",
      "Epoch: 968/1000...  Training Step: 1935...  Training loss: 0.0096...  1.0097 sec/batch\n",
      "Epoch: 968/1000...  Training Step: 1936...  Training loss: 0.0022...  1.0095 sec/batch\n",
      "Epoch: 969/1000...  Training Step: 1937...  Training loss: 0.0096...  1.0098 sec/batch\n",
      "Epoch: 969/1000...  Training Step: 1938...  Training loss: 0.0022...  1.0108 sec/batch\n",
      "Epoch: 970/1000...  Training Step: 1939...  Training loss: 0.0095...  1.0100 sec/batch\n",
      "Epoch: 970/1000...  Training Step: 1940...  Training loss: 0.0022...  1.0106 sec/batch\n",
      "Epoch: 971/1000...  Training Step: 1941...  Training loss: 0.0095...  1.0112 sec/batch\n",
      "Epoch: 971/1000...  Training Step: 1942...  Training loss: 0.0021...  1.0105 sec/batch\n",
      "Epoch: 972/1000...  Training Step: 1943...  Training loss: 0.0095...  1.0088 sec/batch\n",
      "Epoch: 972/1000...  Training Step: 1944...  Training loss: 0.0021...  1.0095 sec/batch\n",
      "Epoch: 973/1000...  Training Step: 1945...  Training loss: 0.0095...  1.0098 sec/batch\n",
      "Epoch: 973/1000...  Training Step: 1946...  Training loss: 0.0021...  1.0101 sec/batch\n",
      "Epoch: 974/1000...  Training Step: 1947...  Training loss: 0.0094...  1.0104 sec/batch\n",
      "Epoch: 974/1000...  Training Step: 1948...  Training loss: 0.0021...  1.0111 sec/batch\n",
      "Epoch: 975/1000...  Training Step: 1949...  Training loss: 0.0094...  1.0157 sec/batch\n",
      "Epoch: 975/1000...  Training Step: 1950...  Training loss: 0.0021...  1.0089 sec/batch\n",
      "Epoch: 976/1000...  Training Step: 1951...  Training loss: 0.0094...  1.0146 sec/batch\n",
      "Epoch: 976/1000...  Training Step: 1952...  Training loss: 0.0021...  1.0085 sec/batch\n",
      "Epoch: 977/1000...  Training Step: 1953...  Training loss: 0.0094...  1.0104 sec/batch\n",
      "Epoch: 977/1000...  Training Step: 1954...  Training loss: 0.0021...  1.0083 sec/batch\n",
      "Epoch: 978/1000...  Training Step: 1955...  Training loss: 0.0093...  1.0090 sec/batch\n",
      "Epoch: 978/1000...  Training Step: 1956...  Training loss: 0.0020...  1.0098 sec/batch\n",
      "Epoch: 979/1000...  Training Step: 1957...  Training loss: 0.0093...  1.0095 sec/batch\n",
      "Epoch: 979/1000...  Training Step: 1958...  Training loss: 0.0020...  1.0088 sec/batch\n",
      "Epoch: 980/1000...  Training Step: 1959...  Training loss: 0.0093...  1.0107 sec/batch\n",
      "Epoch: 980/1000...  Training Step: 1960...  Training loss: 0.0020...  1.0096 sec/batch\n",
      "Epoch: 981/1000...  Training Step: 1961...  Training loss: 0.0093...  1.0089 sec/batch\n",
      "Epoch: 981/1000...  Training Step: 1962...  Training loss: 0.0020...  1.0090 sec/batch\n",
      "Epoch: 982/1000...  Training Step: 1963...  Training loss: 0.0093...  1.0097 sec/batch\n",
      "Epoch: 982/1000...  Training Step: 1964...  Training loss: 0.0020...  1.0100 sec/batch\n",
      "Epoch: 983/1000...  Training Step: 1965...  Training loss: 0.0092...  1.0100 sec/batch\n",
      "Epoch: 983/1000...  Training Step: 1966...  Training loss: 0.0020...  1.0099 sec/batch\n",
      "Epoch: 984/1000...  Training Step: 1967...  Training loss: 0.0092...  1.0097 sec/batch\n",
      "Epoch: 984/1000...  Training Step: 1968...  Training loss: 0.0020...  1.0100 sec/batch\n",
      "Epoch: 985/1000...  Training Step: 1969...  Training loss: 0.0092...  1.0098 sec/batch\n",
      "Epoch: 985/1000...  Training Step: 1970...  Training loss: 0.0020...  1.0094 sec/batch\n",
      "Epoch: 986/1000...  Training Step: 1971...  Training loss: 0.0092...  1.0098 sec/batch\n",
      "Epoch: 986/1000...  Training Step: 1972...  Training loss: 0.0020...  1.0100 sec/batch\n",
      "Epoch: 987/1000...  Training Step: 1973...  Training loss: 0.0092...  1.0094 sec/batch\n",
      "Epoch: 987/1000...  Training Step: 1974...  Training loss: 0.0019...  1.0099 sec/batch\n",
      "Epoch: 988/1000...  Training Step: 1975...  Training loss: 0.0091...  1.0095 sec/batch\n",
      "Epoch: 988/1000...  Training Step: 1976...  Training loss: 0.0019...  1.0096 sec/batch\n",
      "Epoch: 989/1000...  Training Step: 1977...  Training loss: 0.0091...  1.0089 sec/batch\n",
      "Epoch: 989/1000...  Training Step: 1978...  Training loss: 0.0019...  1.0084 sec/batch\n",
      "Epoch: 990/1000...  Training Step: 1979...  Training loss: 0.0091...  1.0096 sec/batch\n",
      "Epoch: 990/1000...  Training Step: 1980...  Training loss: 0.0019...  1.0088 sec/batch\n",
      "Epoch: 991/1000...  Training Step: 1981...  Training loss: 0.0091...  1.0099 sec/batch\n",
      "Epoch: 991/1000...  Training Step: 1982...  Training loss: 0.0019...  1.0121 sec/batch\n",
      "Epoch: 992/1000...  Training Step: 1983...  Training loss: 0.0091...  1.0103 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 992/1000...  Training Step: 1984...  Training loss: 0.0019...  1.0110 sec/batch\n",
      "Epoch: 993/1000...  Training Step: 1985...  Training loss: 0.0091...  1.0101 sec/batch\n",
      "Epoch: 993/1000...  Training Step: 1986...  Training loss: 0.0019...  1.0112 sec/batch\n",
      "Epoch: 994/1000...  Training Step: 1987...  Training loss: 0.0090...  1.0095 sec/batch\n",
      "Epoch: 994/1000...  Training Step: 1988...  Training loss: 0.0019...  1.0097 sec/batch\n",
      "Epoch: 995/1000...  Training Step: 1989...  Training loss: 0.0090...  1.0100 sec/batch\n",
      "Epoch: 995/1000...  Training Step: 1990...  Training loss: 0.0019...  1.0101 sec/batch\n",
      "Epoch: 996/1000...  Training Step: 1991...  Training loss: 0.0090...  1.0097 sec/batch\n",
      "Epoch: 996/1000...  Training Step: 1992...  Training loss: 0.0019...  1.0106 sec/batch\n",
      "Epoch: 997/1000...  Training Step: 1993...  Training loss: 0.0090...  1.0110 sec/batch\n",
      "Epoch: 997/1000...  Training Step: 1994...  Training loss: 0.0019...  1.0099 sec/batch\n",
      "Epoch: 998/1000...  Training Step: 1995...  Training loss: 0.0090...  1.0095 sec/batch\n",
      "Epoch: 998/1000...  Training Step: 1996...  Training loss: 0.0018...  1.0096 sec/batch\n",
      "Epoch: 999/1000...  Training Step: 1997...  Training loss: 0.0090...  1.0098 sec/batch\n",
      "Epoch: 999/1000...  Training Step: 1998...  Training loss: 0.0018...  1.0103 sec/batch\n",
      "Epoch: 1000/1000...  Training Step: 1999...  Training loss: 0.0089...  1.0094 sec/batch\n",
      "Epoch: 1000/1000...  Training Step: 2000...  Training loss: 0.0018...  1.0104 sec/batch\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i2000_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l600.ckpt\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i720_l600.ckpt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(1, 77), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l600.ckpt\n",
      "moutainst of felinaal ous monstoristir. Ind moner or had not stread bey a sound ig founted but the had stometh promped it whit dindianled skenising for that sty phill tementh was fomened chifed in had colour as a load or dark the pabelal torme. The nours whach inss doy, but promen astle the distunted and of ever the saveraly of enmalys an the nexters of the gadeable deally in a cunden agrimat of awayy. The lotser had beoon of the mey outhifif quitice on the oljorm was over Nahum ofadide abreas he roush and soulf the wald, coaling the fream of the stand was ead in in the water. The who scely hod ran the herr end that the felther woods menter and lames and the sainton was the imes thin near neadly s everyand moness the ed they of comenest the hoctous the seat coloul tofs ax reuled ascid. Nahum had leee but ghoun apon cormabey thit tromes out in fimearian to quite nough as ampasing froght on the yark, and the pans had rast weet mo to thite fir thatsh corours. The alaths of the olinto stamed on the fren thous wat vert Ammi and thencienty a croldgulas yound a dat, and issepeadyed in onc uprinai. shey meactering or mutinest, and it as nith night on the ghous. They camined doop grainly stanest of a fied in the ainat of lote and thatee wure mains. Thas had thals hige verimsed ppriffilged momied leffired the seate mentoo of wif. I dispatebrec to sibliait at had botn which the tale hameled on orihars tree so natroma to dundent whro whis ha thean mo the peres little aithing buld, lithly whe earngabled aboun this illded stilis soult be ane thes towers oby the mereols sevear to lother the kinkait it had mistled to best of the woods, frem theen senlited conised by the owred. The ded cale brien in the sttinged a miptest of thet the stuttered the witereloth of the hadden. The smain not alped to hele had been dealls in the mated afor hefesine centist rinighed encr pasing the rume of the fainor was apporsind to the moash of engatienar to the last. It ald bus whech had been the seath gresupod \n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"moutains\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(1, 77), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l550.ckpt\n",
      "Farres anl ons the andit tha the santhe thast on thend tired bothe coret on the satith wint thed thas and the thin sinle af ald bor orest of whand beringes and this woll tor al thered simid wimlad solist on ale the tee hhe the th minl then ond thare bote an thes, bas the se momed the thent ind tho se sithe tisto the cithint at thinge bed af and al the thes tas ther ont ord anterad, bortind. wol tore toad sant thase thiringe at thor wore whe sinte se the weth at tal chit tore tol wasthe angh tand, and on tha mat oftint talt the chetand. whe whis there tans an hhithare anterith sis af titil ce the sor an th ald women oudese and af the til oldind worins wath thas the wate and the the cathed whed ouss onthend af thareting thedith toled tin wanth the the the the whathed, wor an tire hare thon the thom thin thimed onthe bised, bassisd bethe then oud ar te the che the cislong wher ansenen wol thar sithe countedes tath and athit the wasetho te the womede to sherisd., thor wol tin to that onet on a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l550.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(1, 77), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l550.ckpt\n",
      "Farm of the wiles of the sudest to deport in the smork the spece becamed  folfrrd on the filites in monscriend whas was and const fexped incomelarch of this wat preemst and blank had secled in hir counds and bubling, and syomered agealse th sciencedi; and pheshec varieno had groued the reating thatien chawmy accinent desmitter and digan the proper of the conerung in the yorne partis afthrint whar an aghar at moush of the n arouthl and Nahum as om this  aster there, an and assed the neally sit of and south in the merfire yound sheat when the siomowing lafter theirs haid had been therred shomst-rigned inal the well. There were ablar. who stiend of stripking sied. That in sheal calten as he maying place-door sees times of the scrap, and which it was ronchented intruat. the the skneated yeach with the professors was to meated. He had now like their when con, but sky whal was, that wese from that descon's was evering to che mee rith from the ordiceana valien and hearishecthing long and some wit\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i2000_l550.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
